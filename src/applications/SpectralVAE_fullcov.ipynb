{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join('..')))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join('../core')))\n",
    "import numpy as np\n",
    "import pickle\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Lambda, Subtract, Dense\n",
    "from keras.layers.merge import _Merge\n",
    "from keras.activations import relu\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse\n",
    "\n",
    "import train\n",
    "import costs\n",
    "from data import predict_with_K_fn\n",
    "from layer import stack_layers\n",
    "from util import LearningHandler, make_layer_list, train_gen, get_scale\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET AND USEFUL FUNCTION DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_K_fn(K_fn, x, bs=1000):\n",
    "    '''\n",
    "    Convenience function: evaluates x by K_fn(x), where K_fn is\n",
    "    a Keras function, by batches of size 1000.\n",
    "    '''\n",
    "    if not isinstance(x, list):\n",
    "        x = [x]\n",
    "    num_outs = len(K_fn.outputs)\n",
    "    y = [np.empty((len(x[0]), output_.get_shape()[1])) for output_ in K_fn.outputs]\n",
    "    recon_means = []\n",
    "    for i in range(int((x[0].shape[0]-1)/bs + 1)):\n",
    "        x_batch = []\n",
    "        for x_ in x:\n",
    "            x_batch.append(x_[i*bs:(i+1)*bs])\n",
    "        temp = K_fn(x_batch)\n",
    "        for j in range(num_outs):\n",
    "            y[j][i*bs:(i+1)*bs] = temp[j]\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "from matplotlib.colors import ListedColormap\n",
    "# cmap1 = ListedColormap(sns.color_palette().as_hex())\n",
    "# cmap2 = ListedColormap(sns.color_palette('bright').as_hex())\n",
    "def plot(x, y=None, x2=None, y2=None, s=10, s2=None, alpha=0.5, label1=None, label2=None, cmap1=None, cmap2=None):\n",
    "    s2 = s if s2 is None else s2\n",
    "    n = x.shape[1]\n",
    "    if n == 1:\n",
    "        g = plt.figure()\n",
    "        plt.scatter(np.zeros((n,)), x[:,1], c=y, s=s, alpha=alpha, label=label1, cmap=cmap1)\n",
    "        if x2 is not None:\n",
    "            plt.scatter(np.zeros((n,)), x2[:,1], c=y2, s=s2, alpha=alpha, label=label2, cmap=cmap2)\n",
    "    if n == 3:\n",
    "        %matplotlib notebook\n",
    "        g = plt.figure()\n",
    "        ax = g.add_subplot(111, projection='3d')\n",
    "        ax.scatter(x[:,0], x[:,1], x[:,2], c=y, s=s, alpha=alpha, label=label1)\n",
    "        if x2 is not None:\n",
    "            ax.scatter(x2[:,0], x2[:,1], x2[:,2], c=y2, s=s2, alpha=alpha, label=label2)\n",
    "    elif n == 784:\n",
    "        %matplotlib inline\n",
    "        n_imgs = 10\n",
    "        # num = 7\n",
    "        # sub = y == num\n",
    "        sub = y == y\n",
    "        for i in range(n_imgs):\n",
    "            idx = np.random.randint(len(x[sub]))\n",
    "            if x2 is not None:\n",
    "                plt.subplot(1,2,1)\n",
    "                plt.imshow(x2[sub][idx].reshape(28, 28))\n",
    "            plt.subplot(1,2,2)\n",
    "            plt.imshow(x[sub][idx].reshape(28, 28))\n",
    "            g = plt.figure()\n",
    "    else:\n",
    "        g = plt.figure()\n",
    "        plt.scatter(x[:,0], x[:,1], c=y, s=s, alpha=alpha, label=label1, cmap=cmap1)\n",
    "        if x2 is not None:\n",
    "            plt.scatter(x2[:,0], x2[:,1], c=y2, s=s2, alpha=alpha, label=label2, cmap=cmap2)\n",
    "            \n",
    "    if label1 is not None or label2 is not None:\n",
    "        plt.legend()\n",
    "        \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_loop(n=1200, train_set_fraction=.8):\n",
    "    t = np.linspace(0, 2*np.pi, num=n)\n",
    "    \n",
    "    # generate all three coordinates\n",
    "    x = np.empty((n, 3))\n",
    "    x[:,0] = np.cos(t)\n",
    "    x[:,1] = np.sin(2*t)\n",
    "    x[:,2] = np.sin(3*t)\n",
    "    \n",
    "    # y is just t\n",
    "    y = t\n",
    "    \n",
    "    # shuffle\n",
    "    p = np.random.permutation(n)\n",
    "    y = y[p]\n",
    "    x = x[p]\n",
    "\n",
    "    # make train and test splits\n",
    "    n_train = int(n * train_set_fraction)\n",
    "    x_train, x_test = x[:n_train], x[n_train:]\n",
    "    y_train, y_test = y[:n_train].flatten(), y[n_train:].flatten()\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def generate_circle(n=1000, train_set_fraction=.8, alpha=4):\n",
    "    t = np.linspace(0, 2*np.pi, num=n)\n",
    "#     t = np.log(np.linspace(1, alpha, num=n))\n",
    "    t = t / np.max(t) * 2 * np.pi\n",
    "    \n",
    "    # generate all three coordinates\n",
    "    x = np.empty((n, 2))\n",
    "    x[:,0] = np.cos(t)\n",
    "    x[:,1] = np.sin(t)\n",
    "    \n",
    "    # y is just t\n",
    "    y = t\n",
    "    \n",
    "    # shuffle\n",
    "    p = np.random.permutation(n)\n",
    "    y = y[p]\n",
    "    x = x[p]\n",
    "\n",
    "    # make train and test splits\n",
    "    n_train = int(n * train_set_fraction)\n",
    "    x_train, x_test = x[:n_train], x[n_train:]\n",
    "    y_train, y_test = y[:n_train].flatten(), y[n_train:].flatten()\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def generate_line(n=1200, train_set_fraction=.8):\n",
    "    pts_per_cluster = int(n / 2)\n",
    "    x1 = np.linspace(0, 1, num=n).reshape((-1, 1))\n",
    "    x2 = np.linspace(0, 1, num=n).reshape((-1, 1))\n",
    "    x = np.concatenate([x1, x2], axis=1)\n",
    "    \n",
    "    # generate labels\n",
    "#     y = np.concatenate((np.zeros(shape=(pts_per_cluster, 1)), np.ones(shape=(pts_per_cluster, 1))), axis=0)\n",
    "    y = x1\n",
    "    \n",
    "    # shuffle\n",
    "    p = np.random.permutation(n)\n",
    "    y = y[p]\n",
    "    x = x[p]\n",
    "\n",
    "    # make train and test splits\n",
    "    n_train = int(n * train_set_fraction)\n",
    "    x_train, x_test = x[:n_train], x[n_train:]\n",
    "    y_train, y_test = y[:n_train].flatten(), y[n_train:].flatten()\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def generate_gaussians(n=1200, n_clusters=2, noise_sigma=0.1, train_set_fraction=1.):\n",
    "    '''\n",
    "    Generates and returns the nested 'C' example dataset (as seen in the leftmost\n",
    "    graph in Fig. 1)\n",
    "    '''\n",
    "    pts_per_cluster = int(n / n_clusters)\n",
    "    r = 1\n",
    "    \n",
    "    clusters = []\n",
    "    \n",
    "    for x in np.linspace(0, 1, num=n_clusters):\n",
    "        clusters.append(np.random.normal(x, noise_sigma, size=(pts_per_cluster, 2)))\n",
    "\n",
    "    # combine clusters\n",
    "    x = np.concatenate(clusters, axis=0)\n",
    "    print(np.max(x), np.min(x))\n",
    "    x /= (np.max(x) - np.min(x))\n",
    "    print(np.max(x), np.min(x))\n",
    "    x -= np.min(x)\n",
    "    print(np.max(x), np.min(x))\n",
    "\n",
    "    # generate labels\n",
    "    y = np.concatenate((np.zeros(shape=(pts_per_cluster, 1)), np.ones(shape=(pts_per_cluster, 1))), axis=0)\n",
    "\n",
    "    # shuffle\n",
    "    p = np.random.permutation(n)\n",
    "    y = y[p]\n",
    "    x = x[p]\n",
    "\n",
    "    # make train and test splits\n",
    "    n_train = int(n * train_set_fraction)\n",
    "    x_train, x_test = x[:n_train], x[n_train:]\n",
    "    y_train, y_test = y[:n_train].flatten(), y[n_train:].flatten()\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def generate_cc(n=1200, noise_sigma=0.1, train_set_fraction=1.):\n",
    "    '''\n",
    "    Generates and returns the nested 'C' example dataset (as seen in the leftmost\n",
    "    graph in Fig. 1)\n",
    "    '''\n",
    "    pts_per_cluster = int(n / 2)\n",
    "    r = 1\n",
    "\n",
    "    # generate clusters\n",
    "    theta1 = (np.random.uniform(0, 1, pts_per_cluster) * r * np.pi - np.pi / 2).reshape(pts_per_cluster, 1)\n",
    "    theta2 = (np.random.uniform(0, 1, pts_per_cluster) * r * np.pi - np.pi / 2).reshape(pts_per_cluster, 1)\n",
    "\n",
    "    cluster1 = np.concatenate((np.cos(theta1) * r, np.sin(theta1) * r), axis=1)\n",
    "    cluster2 = np.concatenate((np.cos(theta2) * r, np.sin(theta2) * r), axis=1)\n",
    "\n",
    "    # shift and reverse cluster 2\n",
    "    cluster2[:, 0] = -cluster2[:, 0] + 0.5\n",
    "    cluster2[:, 1] = -cluster2[:, 1] - 1\n",
    "\n",
    "    # combine clusters\n",
    "    x = np.concatenate((cluster1, cluster2), axis=0)\n",
    "\n",
    "    # add noise to x\n",
    "    x = x + np.random.randn(x.shape[0], 2) * noise_sigma\n",
    "    print(np.max(x), np.min(x))\n",
    "    x /= (np.max(x) - np.min(x))\n",
    "    print(np.max(x), np.min(x))\n",
    "    x -= np.min(x)\n",
    "    print(np.max(x), np.min(x))\n",
    "\n",
    "    # generate labels\n",
    "    y = np.concatenate((np.zeros(shape=(pts_per_cluster, 1)), np.ones(shape=(pts_per_cluster, 1))), axis=0)\n",
    "\n",
    "    # shuffle\n",
    "    p = np.random.permutation(n)\n",
    "    y = y[p]\n",
    "    x = x[p]\n",
    "\n",
    "    # make train and test splits\n",
    "    n_train = int(n * train_set_fraction)\n",
    "    x_train, x_test = x[:n_train], x[n_train:]\n",
    "    y_train, y_test = y[:n_train].flatten(), y[n_train:].flatten()\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: max 1.0, min -1.0\n"
     ]
    }
   ],
   "source": [
    "dataset = 'loop'\n",
    "\n",
    "if dataset == 'mnist':\n",
    "#     (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "#     x_train, x_test = x_train.reshape((-1, 784)), x_test.reshape((-1, 784))\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    # reshape and standardize x arrays\n",
    "    x_train = x_train.reshape(len(x_train), -1) / 255\n",
    "    x_test = x_test.reshape(len(x_test), -1) / 255\n",
    "    latent_dim = 9\n",
    "elif dataset == 'gaussians':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_gaussians(n=2000, n_clusters=1, train_set_fraction=0.85)\n",
    "    latent_dim = 6\n",
    "elif dataset == 'line':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_line(n=2000, train_set_fraction=0.85)\n",
    "    latent_dim = 1\n",
    "elif dataset == 'loop':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_loop(n=1000, train_set_fraction=0.85)\n",
    "    latent_dim = 2\n",
    "elif dataset == 'cc':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_cc(n=2000, noise_sigma=0.1, train_set_fraction=0.85)\n",
    "    latent_dim = 3\n",
    "elif dataset == 'circle':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_circle(n=1024, train_set_fraction=0.85, alpha=30)\n",
    "    latent_dim = 2\n",
    "\n",
    "x_all = np.concatenate([x_train, x_test], axis=0)\n",
    "    \n",
    "# normalize to between -1 and 1\n",
    "if dataset != 'mnist':\n",
    "    m, M = np.min(x_train), np.max(x_train)\n",
    "    a = (M + m) / 2\n",
    "    b = (M - m) / 2\n",
    "    x_train, x_test = (x_train - a) / b, (x_test - a) / b\n",
    "print('IMPORTANT: max {}, min {}'.format(np.max(x_train), np.min(x_train)))\n",
    "\n",
    "arch = [\n",
    "    {'type': 'relu', 'size': 128},\n",
    "    {'type': 'relu', 'size': 128},\n",
    "    {'type': 'relu', 'size': 512},\n",
    "    {'type': 'linear', 'size': 16},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralNet:\n",
    "    def __init__(self, inputs, arch, spec_reg, y_true, y_train_labeled_onehot,\n",
    "            n_clusters, affinity, scale_nbr, n_nbrs, batch_sizes, normalized=False,\n",
    "            siamese_net=None, x_train=None, have_labeled=False):\n",
    "        self.y_true = y_true\n",
    "        self.y_train_labeled_onehot = y_train_labeled_onehot\n",
    "        self.inputs = inputs\n",
    "        self.batch_sizes = batch_sizes\n",
    "        self.normalized = normalized\n",
    "        # generate layers\n",
    "        self.layers = make_layer_list(arch[:-1], 'spectral', spec_reg)\n",
    "        self.layers += [\n",
    "                  {'type': 'tanh',\n",
    "                   'size': n_clusters,\n",
    "                   'l2_reg': spec_reg,\n",
    "                   'name': 'spectral_{}'.format(len(arch)-1)},\n",
    "                  {'type': 'Orthonorm', 'name':'orthonorm'}\n",
    "                  ]\n",
    "\n",
    "        # create spectralnet\n",
    "        self.outputs = stack_layers(self.inputs, self.layers)\n",
    "        self.net = Model(inputs=self.inputs['Unlabeled'], outputs=self.outputs['Unlabeled'])\n",
    "\n",
    "        # DEFINE LOSS\n",
    "\n",
    "        # generate affinity matrix W according to params\n",
    "        if affinity == 'siamese':\n",
    "            input_affinity = tf.concat([siamese_net.outputs['A'], siamese_net.outputs['Labeled']], axis=0)\n",
    "            x_affinity = siamese_net.predict(x_train, batch_sizes)\n",
    "        elif affinity in ['knn', 'full']:\n",
    "            input_affinity = tf.concat([self.inputs['Unlabeled'], self.inputs['Labeled']], axis=0)\n",
    "            x_affinity = x_train\n",
    "\n",
    "        # calculate scale for affinity matrix\n",
    "        scale = get_scale(x_affinity, self.batch_sizes['Unlabeled'], scale_nbr)\n",
    "\n",
    "        # create affinity matrix\n",
    "        if affinity == 'full':\n",
    "            W = costs.full_affinity(input_affinity, scale=scale)\n",
    "        elif affinity in ['knn', 'siamese']:\n",
    "            W = costs.knn_affinity(input_affinity, n_nbrs, scale=scale, scale_nbr=scale_nbr)\n",
    "\n",
    "        # if we have labels, use them\n",
    "        if have_labeled:\n",
    "            # get true affinities (from labeled data)\n",
    "            W_true = tf.cast(tf.equal(costs.squared_distance(y_true), 0),dtype='float32')\n",
    "\n",
    "            # replace lower right corner of W with W_true\n",
    "            unlabeled_end = tf.shape(self.inputs['Unlabeled'])[0]\n",
    "            W_u = W[:unlabeled_end, :]                  # upper half\n",
    "            W_ll = W[unlabeled_end:, :unlabeled_end]    # lower left\n",
    "            W_l = tf.concat((W_ll, W_true), axis=1)      # lower half\n",
    "            W = tf.concat((W_u, W_l), axis=0)\n",
    "\n",
    "            # create pairwise batch distance matrix self.Dy\n",
    "            y_ = tf.concat([self.outputs['Unlabeled'], self.outputs['Labeled']], axis=0)\n",
    "        else:\n",
    "            y_ = self.outputs['Unlabeled']\n",
    "            \n",
    "        if self.normalized:\n",
    "            y_old = y_\n",
    "            y_ = y_ / tf.expand_dims(tf.reduce_sum(W, axis=1), axis=-1)\n",
    "        \n",
    "        self.Dy = costs.squared_distance(y_)\n",
    "\n",
    "        # define loss\n",
    "        self.loss = K.sum(W * self.Dy) / (2 * batch_sizes['Unlabeled'])\n",
    "\n",
    "        # create the train step update\n",
    "        self.learning_rate = tf.Variable(0., name='spectral_net_learning_rate')\n",
    "        self.train_step = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate).minimize(self.loss, var_list=self.net.trainable_weights)\n",
    "#         self.train_step = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss, var_list=self.net.trainable_weights)\n",
    "        \n",
    "        # initialize spectralnet variables\n",
    "        K.get_session().run(tf.variables_initializer(self.net.trainable_weights))\n",
    "\n",
    "    def train(self, x_train_unlabeled, x_train_labeled, x_val_unlabeled,\n",
    "            lr, drop, patience, num_epochs):\n",
    "        # create handler for early stopping and learning rate scheduling\n",
    "        self.lh = LearningHandler(\n",
    "                lr=lr,\n",
    "                drop=drop,\n",
    "                lr_tensor=self.learning_rate,\n",
    "                patience=patience)\n",
    "\n",
    "        losses = np.empty((num_epochs,))\n",
    "        val_losses = np.empty((num_epochs,))\n",
    "\n",
    "        # begin spectralnet training loop\n",
    "        self.lh.on_train_begin()\n",
    "        i = 0\n",
    "        for i in range(num_epochs):\n",
    "            # train spectralnet\n",
    "            losses[i] = train.train_step(\n",
    "                    return_var=[self.loss],\n",
    "                    updates=self.net.updates + [self.train_step],\n",
    "                    x_unlabeled=x_train_unlabeled,\n",
    "                    inputs=self.inputs,\n",
    "                    y_true=self.y_true,\n",
    "                    batch_sizes=self.batch_sizes,\n",
    "                    x_labeled=x_train_labeled,\n",
    "                    y_labeled=self.y_train_labeled_onehot,\n",
    "                    batches_per_epoch=100)[0]\n",
    "\n",
    "            # get validation loss\n",
    "            val_losses[i] = train.predict_sum(\n",
    "                    self.loss,\n",
    "                    x_unlabeled=x_val_unlabeled,\n",
    "                    inputs=self.inputs,\n",
    "                    y_true=self.y_true,\n",
    "                    x_labeled=x_train_unlabeled[0:0],\n",
    "                    y_labeled=self.y_train_labeled_onehot,\n",
    "                    batch_sizes=self.batch_sizes)\n",
    "\n",
    "            # do early stopping if necessary\n",
    "            if self.lh.on_epoch_end(i, val_losses[i]):\n",
    "                print('STOPPING EARLY')\n",
    "                break\n",
    "\n",
    "            # print training status\n",
    "            print(\"Epoch: {}, loss={:2f}, val_loss={:2f}\".format(i, losses[i], val_losses[i]))\n",
    "\n",
    "        return losses[:i+1], val_losses[:i+1]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # test inputs do not require the 'Labeled' input\n",
    "        inputs_test = {'Unlabeled': self.inputs['Unlabeled'], 'Orthonorm': self.inputs['Orthonorm']}\n",
    "        return train.predict(\n",
    "                    self.outputs['Unlabeled'],\n",
    "                    x_unlabeled=x,\n",
    "                    inputs=inputs_test,\n",
    "                    y_true=self.y_true,\n",
    "                    x_labeled=x[0:0],\n",
    "                    y_labeled=self.y_train_labeled_onehot[0:0],\n",
    "                    batch_sizes=self.batch_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWeightedAverage(_Merge):\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((32, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
    "\n",
    "class SVG:\n",
    "    def __init__(self, inputs, spectralnet, orig_dim):\n",
    "        optimizer = 'adam'\n",
    "#         optimizer = RMSprop(lr=0.00005)\n",
    "        self.input = inputs['Unlabeled']\n",
    "        self.orig_dim = orig_dim\n",
    "        \n",
    "        x = self.copy_spectralnet(spectralnet)\n",
    "        \n",
    "        #\n",
    "        # DEFINE ALL LOSSES\n",
    "        #\n",
    "        def mu_loss(_, __):\n",
    "            self.mu_loss = K.sum(mse(self.input, self.mu_recon), axis=-1) * self.orig_dim\n",
    "            return self.mu_loss\n",
    "        def kl_loss(_, __):\n",
    "            alpha = 1.0\n",
    "            kl_loss = -1 + K.log(alpha) - self.z_log_cov_values + K.exp(-self.z_log_cov_values)/alpha\n",
    "            self.kl_loss = K.sum(kl_loss, axis=-1) * 0.5\n",
    "            return self.kl_loss\n",
    "        def pca_loss(_, __):\n",
    "            self.pca_loss = K.sum(mse(self.pca_input, self.pca_recon), axis=-1) * self.orig_dim\n",
    "            return self.pca_loss\n",
    "        def neighbor_loss(_, __):\n",
    "            k = 3\n",
    "            # obtain pairwise distances (size(recon) x size(input))\n",
    "            D = self.pairwise_distances(self.x_enc, self.mu)\n",
    "\n",
    "            # get nearest (i.e., largest negative distance) neighbors of each point\n",
    "            vals, _ = tf.nn.top_k(-D, k=k)\n",
    "\n",
    "            # remove self as neighbor, negate to get positive distances again\n",
    "            vals = -vals[:, 1:]\n",
    "\n",
    "            # pick sigma\n",
    "            sigma = tf.reduce_max(vals[:, 0])\n",
    "\n",
    "            loss = vals * tf.exp(-vals/sigma)\n",
    "            self.neighbor_loss = K.sum(loss, axis=-1)\n",
    "\n",
    "            return self.neighbor_loss\n",
    "        # currently unused\n",
    "        def reconstruction_loss(_, __):\n",
    "            self.reconstruction_loss = K.sum(mse(self.input, self.x_recon), axis=-1) * self.orig_dim\n",
    "            return self.reconstruction_loss\n",
    "        \n",
    "        def vae_loss(_, __):\n",
    "            losses = [mu_loss, kl_loss, pca_loss, neighbor_loss]\n",
    "            loss_weights = [1, 1, 0, 1]\n",
    "            loss = sum([a(_, __) * b for a, b in zip(losses, loss_weights)])\n",
    "            return loss\n",
    "        \n",
    "        #\n",
    "        # DEFINE LAYERS\n",
    "        #\n",
    "\n",
    "        # create encoder\n",
    "        self.x_enc = x_enc = self.build_encoder(x)\n",
    "        self.encoder = Model(inputs=self.input, outputs=x_enc)\n",
    "\n",
    "        # create decoder\n",
    "        x_recon = self.build_decoder(x_enc)\n",
    "        self.decoder = Model(inputs=self.input, outputs=x_recon)\n",
    "        self.mu_recon = self.build_decoder(self.build_encoder(x, no_noise=True))\n",
    "        self.x_recon = x_recon\n",
    "        \n",
    "        # create normalized decoder\n",
    "        x_enc_norm = self.build_encoder(x, normalize_cov=True)\n",
    "        self.x_recon_norm = self.build_decoder(x_enc_norm)\n",
    "        \n",
    "        self.pcae = Model(inputs=self.input, outputs=self.pca_recon)\n",
    "        self.pc = Model(inputs=self.input, outputs=self.pc_embedding)\n",
    "        self.pcae.compile(optimizer=optimizer, loss=pca_loss)\n",
    "        \n",
    "        #\n",
    "        # ASSEMBLE NETWORK\n",
    "        #\n",
    "        self.vae = Model(inputs=self.input, outputs=self.x_recon)\n",
    "        \n",
    "        self.vae.compile(optimizer=optimizer, loss=vae_loss)\n",
    "        \n",
    "    def pairwise_distances(self, A, B):\n",
    "        r_A, r_B = tf.reduce_sum(A*A, 1), tf.reduce_sum(B*B, 1)\n",
    "\n",
    "        # turn r into column vector\n",
    "        r_A, r_B = tf.reshape(r_A, [-1, 1]), tf.reshape(r_B, [-1, 1])\n",
    "        D = r_A - 2*tf.matmul(A, B, transpose_b=True) + tf.transpose(r_B)\n",
    "\n",
    "        return D\n",
    "        \n",
    "    def build_decoder(self, x, arch=[1024, 256, 256]):\n",
    "        if not hasattr(self, 'decoder_layers'):\n",
    "            self.decoder_layers = [Dense(a, activation='relu') for a in arch]\n",
    "            # last layer of decoder / generator is always tanh (LINEAR because of WASSERSTEIN LOSS)\n",
    "            self.decoder_layers.append(Dense(self.orig_dim, activation='linear'))\n",
    "\n",
    "        for l in self.decoder_layers:\n",
    "            x = l(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def build_encoder(self, x, arch=[256, 256, 1024], normalize_cov=False, no_noise=False, qr=False, remove_dim=False):\n",
    "        if not hasattr(self, 'pca_layers'):\n",
    "            self.pca_layers = [Dense(self.latent_dim, activation='linear'), Dense(self.spectralnet_dim, activation='linear')]\n",
    "            \n",
    "        if not hasattr(self, 'encoder_layers'):\n",
    "            self.encoder_layers = [Dense(a, activation='relu') for a in arch]\n",
    "            self.encoder_vec_layer = Dense(self.latent_dim * self.latent_dim, activation='linear')\n",
    "            self.encoder_val_layer = Dense(self.latent_dim, activation='linear')\n",
    "            \n",
    "        # assemble pca layer (a linear autoencoder) and define mu (the latent embedding of this layer)\n",
    "        if not hasattr(self, 'pca_input'):\n",
    "            self.pca_input = x\n",
    "            \n",
    "        self.pc_embedding = x = self.pca_layers[0](x)\n",
    "        \n",
    "        if not hasattr(self, 'pca_recon'):\n",
    "            self.pca_recon = self.pca_layers[1](x)\n",
    "\n",
    "        # define mu (the latent embedding of the pca layer)\n",
    "        mu = x\n",
    "        if not hasattr(self, 'mu'):\n",
    "            self.mu = mu\n",
    "        \n",
    "        # now get sigma component\n",
    "        for l in self.encoder_layers:\n",
    "            x = l(x)\n",
    "            \n",
    "        # get eigenvalues and eigenvectors\n",
    "        v = self.encoder_vec_layer(x)\n",
    "        if qr:\n",
    "            lam = self.encoder_val_layer(x)\n",
    "            if not hasattr(self, 'z_log_cov_values'):\n",
    "                self.z_log_cov_values = lam\n",
    "        else:\n",
    "            # we don't use this, but we still need to pass a tensor so that tf won't complain\n",
    "            lam = mu\n",
    "            \n",
    "        dim = self.latent_dim\n",
    "        # remove one dimension if remove_dim is true (to enforce singular covariance matrix)\n",
    "        if remove_dim:\n",
    "            dim -= 1\n",
    "        \n",
    "        # sample latent space (and normalize covariances if we're trying to do random walks)\n",
    "        if not hasattr(self, 'encoder_sampling_layer'):\n",
    "            f = partial(self.sampling, normalize_cov=normalize_cov, qr=qr)\n",
    "            self.encoder_sampling_layer = Lambda(f, output_shape=(dim,), name='z')\n",
    "            \n",
    "        if no_noise:\n",
    "            cur_encoder_sampling_layer = Lambda(lambda x: x[0], output_shape=(dim,))\n",
    "            \n",
    "        # get encoder embedding\n",
    "        x_enc = self.encoder_sampling_layer([mu, v, lam])\n",
    "        \n",
    "        return x_enc\n",
    "        \n",
    "    def copy_spectralnet(self, spectralnet):\n",
    "        xs = [self.input]\n",
    "        layers = []\n",
    "        for l in spectralnet.net.layers[1:-1]:\n",
    "            w = l.get_weights()\n",
    "            n, m = w[0].shape\n",
    "            if hasattr(l, 'activation'):\n",
    "                act = l.activation\n",
    "            new_l = Dense(m, activation=act, input_shape=(n,), weights=w)\n",
    "            new_l.trainable = False\n",
    "            xs.append(new_l(xs[-1]))\n",
    "            layers.append(new_l)\n",
    "\n",
    "        pre_x = xs[-1]\n",
    "        # add orthonorm layer\n",
    "        sess = K.get_session()\n",
    "        with tf.variable_scope('', reuse=True):\n",
    "            v = tf.get_variable(\"ortho_weights_store\")\n",
    "        ows = sess.run(v)\n",
    "        t_ows = K.variable(ows)\n",
    "        l = Lambda(lambda x: K.dot(x, t_ows))\n",
    "        l.trainable = False\n",
    "        xs.append(l(xs[-1]))\n",
    "        layers.append(l)\n",
    "\n",
    "        x = xs[-1]\n",
    "\n",
    "        self.sn = Model(inputs=self.input, outputs=x)\n",
    "\n",
    "        self.spectralnet_dim = int(x.get_shape()[1])\n",
    "        self.latent_dim = self.spectralnet_dim - 1\n",
    "            \n",
    "        return x\n",
    "\n",
    "    def sampling(self, args, normalize_cov, qr, remove_dim=False):\n",
    "        # get args\n",
    "        z_mean, v, lam = args\n",
    "        # symmetrize z_log_var\n",
    "        v = tf.reshape(v, (-1, self.latent_dim, self.latent_dim))\n",
    "        \n",
    "        # if qr, do qr decomposition of v; else do eigendecomposition of a symmetrized v\n",
    "        if qr:\n",
    "            v, _ = tf.linalg.qr(v)\n",
    "        else:\n",
    "            # symmetrize v\n",
    "            lower_diagonal = tf.matrix_band_part(v, -1, 0)\n",
    "            symmetric = lower_diagonal + tf.einsum('ijk->ikj', lower_diagonal) - tf.matrix_band_part(v, 0, 0)\n",
    "            \n",
    "            # perform eigendecomposition\n",
    "            lam, v = tf.linalg.eigh(symmetric)\n",
    "            self.z_log_cov_values = lam\n",
    "            \n",
    "        dim = self.latent_dim\n",
    "        # remove one dimension if remove_dim is true (to enforce singular covariance matrix)\n",
    "        if remove_dim:\n",
    "            im -= 1\n",
    "            lam, v = lam[:, :-1], v[:, :-1, :-1]\n",
    "        \n",
    "        if not hasattr(self, 'z_cov_vectors'):\n",
    "            self.z_cov_vectors = tf.reshape(v, (-1, dim * dim))\n",
    "        \n",
    "        # get shapes\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "                \n",
    "        # sample from normal distribution\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        \n",
    "        # self.z_log_var_vectors.shape = (n_batches, n_dim, n_dim); epsilon.shape = (n_batches, n_dim)\n",
    "        exp_lam = K.exp(0.5 * lam)\n",
    "        if normalize_cov:\n",
    "            exp_lam = exp_lam * self.latent_dim / tf.reduce_sum(exp_lam)\n",
    "        \n",
    "        # get covariance matrix stack\n",
    "        left_mid_z_var = tf.einsum('ijk,ik->ijk', v, exp_lam)\n",
    "        z_var = tf.einsum('ijk,ilk->ijl', left_mid_z_var, v)\n",
    "        \n",
    "        # multiply covariance matrix stack with random normal vector\n",
    "        z_var_epsilon = tf.einsum('ijk,ik->ik', z_var, epsilon)\n",
    "        \n",
    "        if not hasattr(self, 'z_var'):\n",
    "            self.z_var = tf.reshape(z_var, (-1, dim * dim))\n",
    "        \n",
    "        # assembled output\n",
    "        output = z_mean + z_var_epsilon\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def generate_from_samples(self, x, return_mu_sigma=False, normalize_cov=False):\n",
    "        _x_recon = self.x_recon_norm if normalize_cov else self.x_recon\n",
    "        get_fn = K.function([self.input], [_x_recon, self.mu, self.z_cov_vectors, self.z_log_cov_values])\n",
    "        x_recon, x_mu, x_sigma_v, x_sigma_lam = predict_with_K_fn(get_fn, x)\n",
    "        if return_mu_sigma:\n",
    "            return x_recon, x_mu, x_sigma_v, x_sigma_lam\n",
    "        else:\n",
    "            return x_recon\n",
    "        \n",
    "    def train_pca(self, x_train, x_val=None, epochs=1, batch_size=128, patience=5):\n",
    "        if x_val is not None:\n",
    "            val_data = list((x_val, x_val))\n",
    "        else:\n",
    "            val_data = None\n",
    "        earlystop = EarlyStopping(monitor='val_loss', patience=patience, verbose=1, mode='auto')\n",
    "\n",
    "        self.pcae.fit(x=x_train,\n",
    "                y=x_train,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_data=val_data,\n",
    "                callbacks=[earlystop],\n",
    "                verbose=2)\n",
    "        \n",
    "    def train(self, X_train, batch_size=128, epochs=100):      \n",
    "        self.vae_loss = []\n",
    "        for epoch in range(epochs):\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            samples = [X_train[idx]]\n",
    "            vae_loss = self.vae.train_on_batch(samples, samples)\n",
    "            self.vae_loss.append(vae_loss)\n",
    "\n",
    "            # Plot the progress\n",
    "            print(\"{} [VAE loss: {}]\".format(epoch, vae_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: currently the train and test sets are combined\n",
    "\n",
    "# split = int(len(x_train)*0.8)\n",
    "# x_train, x_val = x_train[:split], x_train[split:]\n",
    "# y_train, y_val = y_train[:split], y_train[split:]\n",
    "\n",
    "x_train = np.concatenate([x_train, x_test], axis=0)\n",
    "y_train = np.concatenate([y_train, y_test], axis=0)\n",
    "\n",
    "# plt.scatter(x_test[:,0], x_test[:,1], c=y_test)\n",
    "g = plot(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = latent_dim + 1\n",
    "\n",
    "bsize = 512\n",
    "\n",
    "batch_sizes = {\n",
    "    'Unlabeled': bsize,\n",
    "    'Labeled': bsize,\n",
    "    'Orthonorm': bsize,\n",
    "    }\n",
    "\n",
    "input_shape = x_train.shape[1:]\n",
    "y_true = tf.placeholder(tf.float32, shape=(None, n_clusters), name='y_true')\n",
    "y_train_labeled_onehot = np.empty((0, len(np.unique(y_train))))\n",
    "inputs = {\n",
    "    'Unlabeled': Input(shape=input_shape, name='UnlabeledInput'),\n",
    "    'Labeled': Input(shape=input_shape, name='LabeledInput'),\n",
    "    'Orthonorm': Input(shape=input_shape, name='OrthonormInput'),\n",
    "    }\n",
    "k = 3\n",
    "spectral_net = SpectralNet(inputs, arch,\n",
    "            None, y_true, y_train_labeled_onehot,\n",
    "            n_clusters, affinity='full', scale_nbr=k, n_nbrs=k, \n",
    "            batch_sizes=batch_sizes, siamese_net=None, \n",
    "            x_train=x_train, have_labeled=len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_net.train(\n",
    "        x_train, np.zeros_like(x_train[0:0]), x_test,\n",
    "        lr=5e-5, drop=0.1, patience=30, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = spectral_net.predict(x_test)\n",
    "g = plot(y_pred[:,:3], y_test)\n",
    "print('range of y_pred values: {} - {}'.format(np.max(y_pred), np.min(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# now plot all the dimensions of spectralnet\n",
    "y_pred_embedded = TSNE().fit_transform(y_pred)\n",
    "g = plot(y_pred_embedded[:,:2], y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svg = SVG(inputs, spectralnet=spectral_net, orig_dim=x_train.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svg.train_pca(x_train, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svg.pca_layers[0].trainable = False\n",
    "svg.pca_layers[1].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svg.pc.predict(x_test)\n",
    "plt.axis = ('equal')\n",
    "g = plot(y_pred[:,:3], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svg.train(x_train, epochs=100, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot generated points\n",
    "x_gen = svg.generate_from_samples(x_train)\n",
    "g = plot(x_gen, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of neighbors within one standard deviation of each element in x_test\n",
    "_, _mu, _sigma_v, _sigma_lam = svg.generate_from_samples(x_test, return_mu_sigma=True)\n",
    "_sigma_v = _sigma_v.reshape(-1, latent_dim, latent_dim)\n",
    "\n",
    "num_close = []\n",
    "for i in range(len(_mu)):\n",
    "    l, v, m = np.exp(0.5 * -_sigma_lam[i,:]), _sigma_v[i,:], _mu[i,:]\n",
    "    left_cov = np.einsum('ij,j->ij', v, l)\n",
    "    cov = np.einsum('ij,kj->ik', left_cov, v)\n",
    "    scaled_dists = np.einsum('jk,ik->ij', cov, _mu - m)\n",
    "    # consider as neighbors all points within the variance of x_i\n",
    "    less_than_std = np.abs(scaled_dists) < 1\n",
    "    less_than_std = np.logical_and(less_than_std[:,0], less_than_std[:,1])\n",
    "    # split neighbors into those of the same class and those of a different class\n",
    "    same, diff = (y_test[less_than_std] == y_test[i]), (y_test[less_than_std] != y_test[i])\n",
    "    num_close.append((np.sum(same), np.sum(diff)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COVARIANCE TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fn1 = K.function([svg.input], [svg.z_var])\n",
    "get_fn2 = K.function([svg.input], [svg.x_enc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute sigmas\n",
    "_, _mu, _sigma_v, _sigma_lam = svg.generate_from_samples(x_test, return_mu_sigma=True)\n",
    "_sigma_v = _sigma_v.reshape(-1, latent_dim, latent_dim)\n",
    "# _sigma_v = np.einsum('ijk->ikj', _sigma_v)\n",
    "# _sigma_lam = np.flip(_sigma_lam, axis=1)\n",
    "_sigma = np.einsum('ijk,ilk->ijl', np.einsum('ijk,ik->ijk', _sigma_v, np.exp(0.5 * _sigma_lam)), _sigma_v)\n",
    "\n",
    "# verify sigmas\n",
    "_z_var = predict_with_K_fn(get_fn1, x_test)[0].reshape((-1, latent_dim, latent_dim))\n",
    "\n",
    "# verify encoding\n",
    "_x_enc = predict_with_K_fn(get_fn2, x_test)[0]\n",
    "\n",
    "print(\"ALSO\", _sigma.shape, _z_var.shape)\n",
    "print('ERROR', np.linalg.norm(_sigma - _z_var))\n",
    "\n",
    "epsilon = np.random.normal(size=_mu.shape)\n",
    "perturbations = np.einsum('ijk,ik->ik', _sigma, epsilon)\n",
    "\n",
    "single_perturbed_x = np.array([_mu[0,:]] * 154) + np.einsum('jk,ik->ij', _sigma[0,:], epsilon)\n",
    "perturbed_x = _mu + perturbations\n",
    "plt.axis=('equal')\n",
    "g = plot(_mu, x2=perturbed_x, s2=20)\n",
    "\n",
    "idxs = np.random.permutation(len(_mu))\n",
    "for i in idxs:\n",
    "    idx = np.argmin(_sigma_lam[i])\n",
    "    delta = _sigma_v[i, idx] * _sigma_lam[i, idx] / 4\n",
    "    start = _mu[i] + delta\n",
    "    end = _mu[i] - delta\n",
    "    plt.plot([start[0], end[0]], [start[1], end[1]], 'k-', lw=2, alpha=0.1)\n",
    "\n",
    "# plt.scatter(_mu[idxs, 0], _mu[idxs, 1], s=200)\n",
    "# plt.plot([0, 1], [1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute sigmas\n",
    "_, _mu, _sigma_v, _sigma_lam = svg.generate_from_samples(x_test, return_mu_sigma=True)\n",
    "_sigma_v = _sigma_v.reshape(-1, latent_dim, latent_dim)\n",
    "# _sigma_v = np.einsum('ijk->ikj', _sigma_v)\n",
    "# _sigma_lam = np.flip(_sigma_lam, axis=1)\n",
    "_sigma = np.einsum('ijk,ilk->ijl', np.einsum('ijk,ik->ijk', _sigma_v, np.exp(0.5 * _sigma_lam)), _sigma_v)\n",
    "\n",
    "# verify sigmas\n",
    "_z_var = predict_with_K_fn(get_fn1, x_test)[0].reshape((-1, latent_dim, latent_dim))\n",
    "\n",
    "# verify encoding\n",
    "_x_enc = predict_with_K_fn(get_fn2, x_test)[0]\n",
    "\n",
    "print(\"ALSO\", _sigma.shape, _z_var.shape)\n",
    "print('ERROR', np.linalg.norm(_sigma - _z_var))\n",
    "\n",
    "epsilon = np.random.normal(size=_mu.shape)\n",
    "perturbations = np.einsum('ijk,ik->ik', _sigma, epsilon)\n",
    "\n",
    "single_perturbed_x = np.array([_mu[0,:]] * 154) + np.einsum('jk,ik->ij', _sigma[0,:], epsilon)\n",
    "perturbed_x = _mu + perturbations\n",
    "g = plot(_mu, x2=perturbed_x, s2=20)\n",
    "plt.axis = 'equal'\n",
    "\n",
    "idxs = np.random.permutation(len(_mu))\n",
    "for i in idxs:\n",
    "    idx = np.argmax(_sigma_lam[i])\n",
    "    delta = _sigma_v[i, idx] * _sigma_lam[i, idx] / 8\n",
    "    start = _mu[i] + delta\n",
    "    end = _mu[i] - delta\n",
    "    plt.plot([start[0], end[0]], [start[1], end[1]], 'k-', lw=2, alpha=0.1)\n",
    "    \n",
    "for i in idxs:\n",
    "    idx = np.argmin(_sigma_lam[i])\n",
    "    delta = _sigma_v[i, idx] * _sigma_lam[i, idx] / 8\n",
    "    start = _mu[i] + delta\n",
    "    end = _mu[i] - delta\n",
    "    plt.plot([start[0], end[0]], [start[1], end[1]], 'k-', lw=2, alpha=0.1)\n",
    "#     plt.plot([_mu[i,0], _mu[i,0]], [_mu[i,1] + .1, _mu[i,1]])\n",
    "\n",
    "# plt.scatter(_mu[idxs, 0], _mu[idxs, 1], s=200)\n",
    "# plt.plot([0, 1], [1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BILIPSCHITZ TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differentiate decoder with respect to inputs to compute another jacobian, and then evaluate it on the same point\n",
    "_jacobian = [tf.expand_dims(tf.gradients(svg.x_recon[:,i], svg.x_enc)[0], 1) for i in range(svg.x_recon.shape[1])]\n",
    "jacobian = tf.reduce_sum(tf.concat(_jacobian, axis=1), axis=0)\n",
    "v = tf.reshape(svg.z_cov_vectors, (-1, latent_dim, latent_dim))\n",
    "B = tf.einsum('ijk,ilk->ijl', tf.einsum('ijk,ik->ijk', v, tf.exp(0.5 * svg.z_log_cov_values)), v)\n",
    "B = tf.reduce_mean(B, axis=0)\n",
    "cov = tf.matmul(jacobian, tf.matmul(B, jacobian, transpose_b=True))\n",
    "cov = tf.reshape(cov, (x_test[0].shape[0], x_test[0].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create burst from a point and compute covariance matrix\n",
    "burst_size = 1000\n",
    "rand_idx = np.random.randint(len(x_test))\n",
    "x_ = x_test[rand_idx]\n",
    "# x_ = np.array((np.cos(.25), np.sin(.25)))\n",
    "x_arr = np.array([x_] * burst_size)\n",
    "x_rec, x_mu, x_sigma_v, x_sigma_lam = svg.generate_from_samples(x_arr, return_mu_sigma=True)\n",
    "\n",
    "cov_burst = np.cov((x_rec - np.mean(x_rec, axis=0)).T)\n",
    "\n",
    "# run gradient burst\"\n",
    "# cov_grad = K.get_session().run([svg.x_recon, cov, B, jacobian], feed_dict={svg.input: np.array([x_]*1)})\n",
    "cov_grad = K.get_session().run([cov], feed_dict={svg.input: np.array([x_]*1)})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_burst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_burst, _ = np.linalg.eig(cov_burst)\n",
    "l_grad, _ = np.linalg.eig(cov_grad)\n",
    "l_burst = np.sort(l_burst)[::-1]\n",
    "l_grad = np.sort(l_grad)[::-1]\n",
    "print('l_burst:', l_burst, l_burst/l_burst[0])\n",
    "print('l_grad:', l_grad, l_grad/l_grad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plot(x_rec, x2=x_test, label1='true', label2='predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x__ = np.expand_dims(x_, axis=0)\n",
    "g = plot(x__, x2=x_test, alpha=.1, label1='true', label2='predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plot(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM WALK TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM WALK\n",
    "def walk(f, x_arr, branch_factor=5, n_steps=20, max_size=1000):\n",
    "    p = np.random.permutation(len(x_arr))[:1000]\n",
    "    x_arr = x_arr[p]\n",
    "    for i in range(n_steps):\n",
    "        x_arr = np.array([x_arr] * branch_factor).reshape([-1, x_.shape[0]])\n",
    "        (x_arr, x_mu, x_sigma) = f(x_arr)\n",
    "        p = np.random.permutation(len(x_arr))[:1000]\n",
    "        x_arr, x_mu, x_sigma = x_arr[p], x_mu[p], x_sigma[p]\n",
    "        \n",
    "    return x_arr, x_mu, x_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = partial(svg.generate_from_samples, return_mu_sigma=True, normalize_cov=False)\n",
    "y_test_sz = np.mean(f(x_all)[3], axis=1)\n",
    "sz_max = np.max(y_test_sz)\n",
    "sz_min = np.min(y_test_sz)\n",
    "y_test_sz = (y_test_sz - sz_min)/(sz_max - sz_min) * 5\n",
    "print(np.min(y_test_sz), np.max(y_test_sz))\n",
    "y_test_sz = np.exp(0.5 * y_test_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation\n",
    "\n",
    "# which space do we want to plot in?\n",
    "plot_latent = False\n",
    "plot_idx = 1 if plot_latent else 0\n",
    "\n",
    "x_arr = np.random.permutation(x_test)[:100]\n",
    "x__ = f(x_all)[plot_idx]\n",
    "x_ = f(x_arr)[plot_idx]\n",
    "x_tot = np.concatenate([x_, x__], axis=0)\n",
    "y_tot = np.concatenate([np.zeros(shape=(len(x_arr),)), np.ones(shape=(len(x__),))*2], axis=0)\n",
    "y_sz = np.concatenate([np.ones(shape=(len(x_arr),))*5, y_test_sz], axis=0)\n",
    "\n",
    "def update_graph(num):\n",
    "    global x_arr\n",
    "    global x__\n",
    "    global y_tot\n",
    "    x_arr, x_mu, x_sigma_v, x_sigma_lam = f(x_arr)\n",
    "    # plot in latent or original space\n",
    "    x_ = x_mu if plot_latent else x_arr\n",
    "    \n",
    "    x_ = np.concatenate([x_, x__], axis=0)\n",
    "    graph._offsets3d = (x_[:,0], x_[:,1], x_[:,2])\n",
    "    ax.view_init(elev=10, azim=num*4)\n",
    "    title.set_text('Walk, time={}'.format(num))\n",
    "\n",
    "fig = plt.figure(figsize=(12.8, 7.2))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "title = ax.set_title('Walk, time=0')\n",
    "\n",
    "# NOTE: ONLY RUNS WITH 3D DATASETS\n",
    "graph = ax.scatter(x_tot[:,0], x_tot[:,1], x_tot[:,2], c=y_tot, s=y_sz, alpha=0.4)\n",
    "\n",
    "ani = matplotlib.animation.FuncAnimation(fig, update_graph, 180, \n",
    "                               interval=50, blit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(ani.to_html5_video(embed_limit=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = partial(svg.generate_from_samples, return_mu_sigma=True, normalize_cov=False)\n",
    "def walk(f, x_arr, branch_factor=5, n_steps=20, max_size=1000):\n",
    "    p = np.random.permutation(len(x_arr))[:1000]\n",
    "    x_arr = x_arr[p]\n",
    "    for i in range(n_steps):\n",
    "        print(x_arr.shape, branch_factor)\n",
    "        x_arr = np.array([x_arr] * branch_factor).reshape([-1, x_.shape[1]])\n",
    "        (x_arr, x_mu, x_sigma_v, x_sigma_lam) = f(x_arr)\n",
    "        p = np.random.permutation(len(x_arr))[:1000]\n",
    "        x_arr, x_mu, x_sigma_v, x_sigma_lam = x_arr[p], x_mu[p], x_sigma_v[p], x_sigma_lam[p]\n",
    "        \n",
    "    return x_arr, x_mu, x_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = partial(svg.generate_from_samples, return_mu_sigma=True)\n",
    "\n",
    "x_test_sample = np.random.permutation(x_test)[:10]\n",
    "x_arr, x_sigma, x_mu = walk(f, x_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plot(x_arr, x2=x_test, label1='predicted', label2='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = plot(x_mu, x2=f(x_test_sample)[1])\n",
    "g = plot(x_mu, x2=f(x_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plot(x_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"samar was here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
