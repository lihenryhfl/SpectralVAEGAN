{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 30 23:51:32 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:1A:00.0  On |                  N/A |\n",
      "| 31%   52C    P2    94W / 250W |  10980MiB / 10988MiB |     36%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:1B:00.0 Off |                  N/A |\n",
      "| 30%   49C    P2    64W / 250W |  10652MiB / 10989MiB |     18%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:60:00.0 Off |                  N/A |\n",
      "| 30%   43C    P2    72W / 250W |  10634MiB / 10989MiB |     17%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:61:00.0 Off |                  N/A |\n",
      "| 29%   31C    P8    15W / 250W |  10652MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce RTX 208...  Off  | 00000000:B1:00.0 Off |                  N/A |\n",
      "| 29%   27C    P8     9W / 250W |  10654MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce RTX 208...  Off  | 00000000:B2:00.0 Off |                  N/A |\n",
      "| 29%   28C    P8    15W / 250W |  10652MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce RTX 208...  Off  | 00000000:DA:00.0 Off |                  N/A |\n",
      "| 29%   36C    P2    71W / 250W |  10650MiB / 10989MiB |      7%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce RTX 208...  Off  | 00000000:DB:00.0 Off |                  N/A |\n",
      "| 30%   27C    P8    17W / 250W |  10654MiB / 10989MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join('..')))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join('../core')))\n",
    "import numpy as np\n",
    "import pickle\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Lambda, Subtract, Dense, Conv2DTranspose, Dropout, Reshape, Flatten, UpSampling2D\n",
    "from keras.layers.merge import _Merge\n",
    "from keras.activations import relu\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import mnist, cifar10, fashion_mnist\n",
    "from keras.losses import mse\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import train, costs, pairs\n",
    "from data import predict_with_K_fn\n",
    "from layer import stack_layers\n",
    "from util import LearningHandler, make_layer_list, train_gen, get_scale\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "debug = False\n",
    "\n",
    "if debug:\n",
    "    from tensorflow.python import debug as tf_debug\n",
    "\n",
    "    K.set_session(tf_debug.LocalCLIDebugWrapperSession(K.get_session()))\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET AND USEFUL FUNCTION DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_get(input_tensors, output_tensors, input_data):\n",
    "    input_data = input_data if isinstance(input_data, list) else [input_data]\n",
    "    input_tensors, output_tensors = list(input_tensors), list(output_tensors)\n",
    "    sess = K.get_session()\n",
    "    return sess.run(output_tensors, dict(zip(input_tensors, input_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_K_fn(K_fn, x, bs=1000):\n",
    "    '''\n",
    "    Convenience function: evaluates x by K_fn(x), where K_fn is\n",
    "    a Keras function, by batches of size 1000.\n",
    "    '''\n",
    "    if not isinstance(x, list):\n",
    "        x = [x]\n",
    "    num_outs = len(K_fn.outputs)\n",
    "    shapes = [list(output_.get_shape()) for output_ in K_fn.outputs]\n",
    "    shapes = [[len(x[0])] + s[1:] for s in shapes]\n",
    "    y = [np.empty(s) for s in shapes]\n",
    "    recon_means = []\n",
    "    for i in range(int((x[0].shape[0]-1)/bs + 1)):\n",
    "        x_batch = []\n",
    "        for x_ in x:\n",
    "            x_batch.append(x_[i*bs:(i+1)*bs])\n",
    "        temp = K_fn(x_batch)\n",
    "        for j in range(num_outs):\n",
    "            y[j][i*bs:(i+1)*bs] = temp[j]\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from matplotlib.cbook import get_sample_data\n",
    "\n",
    "def imscatter(x, y, samples, shape, ax=None, zoom=1):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    x, y = np.atleast_1d(x, y)\n",
    "    artists = []\n",
    "    m=samples.shape[0]\n",
    "    i=0\n",
    "    for x0, y0  in zip(x, y):\n",
    "        im = OffsetImage(samples[i].reshape(shape), zoom=zoom)\n",
    "        ab = AnnotationBbox(im, (x0, y0), xycoords='data', frameon=False)\n",
    "        artists.append(ax.add_artist(ab))\n",
    "        i=i+1\n",
    "    ax.update_datalim(np.column_stack([x, y]))\n",
    "    ax.autoscale()\n",
    "    return artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "from matplotlib.colors import ListedColormap\n",
    "# cmap1 = ListedColormap(sns.color_palette().as_hex())\n",
    "# cmap2 = ListedColormap(sns.color_palette('bright').as_hex())\n",
    "def plot(x, y=None, x2=None, y2=None, s=10, s2=None, alpha=0.5, alpha2=None, label1=None, label2=None, cmap1=None, cmap2=None, n_imgs=4, shuffle=True):\n",
    "    s2 = s if s2 is None else s2\n",
    "    alpha2 = alpha if alpha2 is None else alpha2\n",
    "    \n",
    "    if x.shape[1:] == (28, 28, 1) or x.shape[1:] == (32, 32, 3):\n",
    "        x = x.reshape(len(x), -1)\n",
    "    \n",
    "    n = x.shape[1]\n",
    "    \n",
    "    if n == 1:\n",
    "        g = plt.figure()\n",
    "        plt.scatter(np.zeros((n,)), x[:,1], c=y, s=s, alpha=alpha, label=label1, cmap=cmap1)\n",
    "        if x2 is not None:\n",
    "            plt.scatter(np.zeros((n,)), x2[:,1], c=y2, s=s2, alpha=alpha2, label=label2, cmap=cmap2)\n",
    "    if n == 3:\n",
    "        %matplotlib notebook\n",
    "        g = plt.figure()\n",
    "        ax = g.add_subplot(111, projection='3d')\n",
    "        ax.set_axis_off()\n",
    "        ax.scatter(x[:,0], x[:,1], x[:,2], c=y, s=s, alpha=alpha, label=label1)\n",
    "        if x2 is not None:\n",
    "            ax.scatter(x2[:,0], x2[:,1], x2[:,2], c=y2, s=s2, alpha=alpha2, label=label2)\n",
    "        g = (g, ax)\n",
    "    elif n == 784 or n == 3072 or n == 24000:\n",
    "        %matplotlib inline\n",
    "        n_imgs = min(n_imgs, len(x))\n",
    "        if n == 784:\n",
    "            img_shape, wdist = (28, 28), -.7\n",
    "        elif n == 3072:\n",
    "            img_shape, wdist = (32, 32, 3), -.7\n",
    "        elif n == 24000:\n",
    "            img_shape, wdist = (100, 80, 3), -.785\n",
    "        \n",
    "        if x2 is None:\n",
    "            g, axarr = plt.subplots(n_imgs, n_imgs)\n",
    "            if shuffle:\n",
    "                p = np.random.permutation(len(x))\n",
    "            else:\n",
    "                p = np.arange(len(x))\n",
    "            n = 0\n",
    "            while n < n_imgs * n_imgs:\n",
    "                i, j = int(n / n_imgs), n % n_imgs\n",
    "                axarr[i,j].axis('off')\n",
    "                if n < len(x):\n",
    "                    axarr[i,j].imshow(x[p[n]].reshape(img_shape))\n",
    "                n += 1\n",
    "        elif x2 is not None:\n",
    "            p = np.random.permutation(len(x))[:n_imgs]\n",
    "            for i in range(n_imgs):\n",
    "                idx = p[i]\n",
    "                plt.subplot(1,2,1)\n",
    "                plt.imshow(x2[idx].reshape(img_shape))\n",
    "                plt.subplot(1,2,2)\n",
    "                plt.imshow(x[idx].reshape(img_shape))\n",
    "                g = plt.figure()\n",
    "        g.subplots_adjust(wspace=wdist, hspace=0)\n",
    "    else:\n",
    "        g = plt.figure()\n",
    "        plt.scatter(x[:,0], x[:,1], c=y, s=s, alpha=alpha, label=label1, cmap=cmap1)\n",
    "        if x2 is not None:\n",
    "            plt.scatter(x2[:,0], x2[:,1], c=y2, s=s2, alpha=alpha2, label=label2, cmap=cmap2)\n",
    "            \n",
    "    if label1 is not None or label2 is not None:\n",
    "        plt.legend()\n",
    "        \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bunny(n=2000, train_set_fraction=.8):\n",
    "    import bunny\n",
    "    a = [np.expand_dims(np.array(bunny.trace2[c]), axis=-1) for c in ['x', 'y', 'z']]\n",
    "    x = np.concatenate(a, axis=-1)\n",
    "    x = x.astype(np.float32)\n",
    "    x = x[np.logical_not(np.any(np.isnan(x), axis=1))]\n",
    "    y = np.arange(len(x))\n",
    "    \n",
    "    return shuffle_and_return_n(x, y, train_set_fraction, n=n)\n",
    "\n",
    "def generate_sphere(n=1200, train_set_fraction=.8):\n",
    "    r = 1\n",
    "    alpha = 4.0*np.pi*r*r/(n+1)\n",
    "    d = np.sqrt(alpha)\n",
    "    m_nu = int(np.round(np.pi/d))\n",
    "    d_nu = np.pi/m_nu\n",
    "    d_phi = alpha/d_nu\n",
    "    count = 0\n",
    "    coords = [[], [], []]\n",
    "    y = []\n",
    "    for i in range(0, m_nu):\n",
    "        nu = np.pi*(i+0.5)/m_nu\n",
    "        m_phi = int(np.round(2*np.pi*np.sin(nu)/d_phi))\n",
    "        for j in range(0, m_phi):\n",
    "            phi = 2*np.pi*j/m_phi\n",
    "            xp = r*np.sin(nu)*np.cos(phi)\n",
    "            yp = r*np.sin(nu)*np.sin(phi)\n",
    "            zp = r*np.cos(nu)\n",
    "            coords[0].append(xp)\n",
    "            coords[1].append(yp)\n",
    "            coords[2].append(zp)\n",
    "            y.append(i + j)\n",
    "            count = count +1\n",
    "            \n",
    "    x = np.array(coords).T\n",
    "    y = np.array(y).T\n",
    "        \n",
    "    return shuffle_and_return_n(x, y, train_set_fraction, n=n)\n",
    "\n",
    "def generate_plane(n=1200, train_set_fraction=.8):\n",
    "    # compute number of points in each dimension\n",
    "    n_i = np.int(np.sqrt(n))\n",
    "    n = n_i ** 2\n",
    "    \n",
    "    # compute points on this grid\n",
    "    t = np.mgrid[0:1:1/n_i, 0:1:1/n_i].reshape(2,-1).T\n",
    "    t = np.concatenate([t, np.zeros(shape=(len(t),1))], axis=1)\n",
    "    \n",
    "    # compute rotation\n",
    "    A = np.random.normal(size=(3, 3))\n",
    "    A, _ = np.linalg.qr(A)\n",
    "    \n",
    "    x = np.dot(A, t.T).T\n",
    "    \n",
    "    # y is the sum of the ts\n",
    "    y = t[:,0] + t[:,1]\n",
    "    \n",
    "    return shuffle_and_return_n(x, y, train_set_fraction)\n",
    "\n",
    "def generate_loop(n=1200, train_set_fraction=.8):\n",
    "    t = np.linspace(0, 2*np.pi, num=n+1)[:-1]\n",
    "    \n",
    "    # generate all three coordinates\n",
    "    x = np.empty((n, 3))\n",
    "    x[:,0] = np.cos(t)\n",
    "    x[:,1] = np.sin(2*t)\n",
    "    x[:,2] = np.sin(3*t)\n",
    "    \n",
    "    # y is just t\n",
    "    y = t\n",
    "    \n",
    "    return shuffle_and_return_n(x, y, train_set_fraction)\n",
    "\n",
    "def generate_circle(n=1000, train_set_fraction=.8, alpha=4):\n",
    "    t = np.linspace(0, 2*np.pi, num=n+1)[:-1]\n",
    "#     t = np.log(np.linspace(1, alpha, num=n))\n",
    "    t = t / np.max(t) * 2 * np.pi\n",
    "    \n",
    "    # generate all three coordinates\n",
    "    x = np.empty((n, 2))\n",
    "    x[:,0] = np.cos(t)\n",
    "    x[:,1] = np.sin(t)\n",
    "    \n",
    "    # y is just t\n",
    "    y = t\n",
    "    \n",
    "    return shuffle_and_return_n(x, y, train_set_fraction)\n",
    "\n",
    "def generate_line(n=1200, train_set_fraction=.8):\n",
    "    pts_per_cluster = int(n / 2)\n",
    "    x1 = np.linspace(0, 1, num=n).reshape((-1, 1))\n",
    "    x2 = np.linspace(0, 1, num=n).reshape((-1, 1))\n",
    "    x = np.concatenate([x1, x2], axis=1)\n",
    "    \n",
    "    # generate labels\n",
    "#     y = np.concatenate((np.zeros(shape=(pts_per_cluster, 1)), np.ones(shape=(pts_per_cluster, 1))), axis=0)\n",
    "    y = x1\n",
    "    \n",
    "    return shuffle_and_return_n(x, y, train_set_fraction)\n",
    "\n",
    "def generate_gaussians(n=1200, n_clusters=2, noise_sigma=0.1, train_set_fraction=1.):\n",
    "    '''\n",
    "    Generates and returns the nested 'C' example dataset (as seen in the leftmost\n",
    "    graph in Fig. 1)\n",
    "    '''\n",
    "    pts_per_cluster = int(n / n_clusters)\n",
    "    r = 1\n",
    "    \n",
    "    clusters = []\n",
    "    \n",
    "    for x in np.linspace(0, 1, num=n_clusters):\n",
    "        clusters.append(np.random.normal(x, noise_sigma, size=(pts_per_cluster, 2)))\n",
    "\n",
    "    # combine clusters\n",
    "    x = np.concatenate(clusters, axis=0)\n",
    "    print(np.max(x), np.min(x))\n",
    "    x /= (np.max(x) - np.min(x))\n",
    "    print(np.max(x), np.min(x))\n",
    "    x -= np.min(x)\n",
    "    print(np.max(x), np.min(x))\n",
    "\n",
    "    # generate labels\n",
    "    y = np.concatenate((np.zeros(shape=(pts_per_cluster, 1)), np.ones(shape=(pts_per_cluster, 1))), axis=0)\n",
    "\n",
    "    return shuffle_and_return_n(x, y, train_set_fraction)\n",
    "\n",
    "def generate_cc(n=1200, noise_sigma=0.1, train_set_fraction=1.):\n",
    "    '''\n",
    "    Generates and returns the nested 'C' example dataset (as seen in the leftmost\n",
    "    graph in Fig. 1)\n",
    "    '''\n",
    "    pts_per_cluster = int(n / 2)\n",
    "    r = 1\n",
    "\n",
    "    # generate clusters\n",
    "    theta1 = (np.random.uniform(0, 1, pts_per_cluster) * r * np.pi - np.pi / 2).reshape(pts_per_cluster, 1)\n",
    "    theta2 = (np.random.uniform(0, 1, pts_per_cluster) * r * np.pi - np.pi / 2).reshape(pts_per_cluster, 1)\n",
    "\n",
    "    cluster1 = np.concatenate((np.cos(theta1) * r, np.sin(theta1) * r), axis=1)\n",
    "    cluster2 = np.concatenate((np.cos(theta2) * r, np.sin(theta2) * r), axis=1)\n",
    "\n",
    "    # shift and reverse cluster 2\n",
    "    cluster2[:, 0] = -cluster2[:, 0] + 0.5\n",
    "    cluster2[:, 1] = -cluster2[:, 1] - 1\n",
    "\n",
    "    # combine clusters\n",
    "    x = np.concatenate((cluster1, cluster2), axis=0)\n",
    "\n",
    "    # add noise to x\n",
    "    x = x + np.random.randn(x.shape[0], 2) * noise_sigma\n",
    "    print(np.max(x), np.min(x))\n",
    "    x /= (np.max(x) - np.min(x))\n",
    "    print(np.max(x), np.min(x))\n",
    "    x -= np.min(x)\n",
    "    print(np.max(x), np.min(x))\n",
    "\n",
    "    # generate labels\n",
    "    y = np.concatenate((np.zeros(shape=(pts_per_cluster, 1)), np.ones(shape=(pts_per_cluster, 1))), axis=0)\n",
    "\n",
    "    return shuffle_and_return_n(x, y, train_set_fraction)\n",
    "\n",
    "def generate_faces(train_set_fraction=.8):\n",
    "    x = np.array(loadmat('frey_rawface.mat')['ff']).T\n",
    "    y = np.arange(len(x)) / len(x)\n",
    "    \n",
    "    return shuffle_and_return_n(x, y, train_set_fraction)\n",
    "\n",
    "def generate_bulldog(train_set_fraction=.8):\n",
    "    x = np.load('BuldogBig.npy').T\n",
    "    y = np.arange(len(x))\n",
    "    print(x.shape)\n",
    "    \n",
    "    x = x / np.max(x)\n",
    "    \n",
    "    return shuffle_and_return_n(x, y, train_set_fraction)\n",
    "\n",
    "def generate_gaussian_grid(train_set_fraction=.8):\n",
    "    dim = 2\n",
    "    n_per_gaussian = 300\n",
    "    scale = .1\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            loc = i - 2, j - 2\n",
    "            xs.append(np.random.normal(loc=loc, scale=scale, size=(n_per_gaussian, dim)))\n",
    "            ys.append([(i * 5) + j] * n_per_gaussian)\n",
    "    x = np.concatenate(xs, axis=0)\n",
    "    y = np.concatenate(ys, axis=0)\n",
    "    \n",
    "    return shuffle_and_return_n(x, y, train_set_fraction)\n",
    "\n",
    "def shuffle_and_return_n(x, y, train_set_fraction, n=None):\n",
    "    if n is None:\n",
    "        n = len(x)\n",
    "    # shuffle\n",
    "    p = np.random.permutation(len(x))[:n]\n",
    "    y = y[p]\n",
    "    x = x[p]\n",
    "    \n",
    "    # make train and test splits\n",
    "    n_train = int(n * train_set_fraction)\n",
    "    x_train, x_test = x[:n_train], x[n_train:]\n",
    "    y_train, y_test = y[:n_train].flatten(), y[n_train:].flatten()\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed data\n",
    "def embed_data(x, dset):\n",
    "    '''\n",
    "    Convenience function: embeds x into the code space using the corresponding\n",
    "    autoencoder (specified by dset).\n",
    "    '''\n",
    "    if not len(x):\n",
    "        return np.zeros(shape=(0, 10))\n",
    "    if dset == 'reuters':\n",
    "        dset = 'reuters10k'\n",
    "\n",
    "    json_path = '../pretrain_weights/ae_{}.json'.format(dset)\n",
    "    weights_path = '../pretrain_weights/ae_{}_weights.h5'.format(dset)\n",
    "\n",
    "    with open(json_path) as f:\n",
    "        pt_ae = model_from_json(f.read())\n",
    "    pt_ae.load_weights(weights_path)\n",
    "\n",
    "    x = x.reshape(-1, np.prod(x.shape[1:]))\n",
    "\n",
    "    get_embeddings = K.function([pt_ae.input],\n",
    "                                  [pt_ae.layers[3].output])\n",
    "\n",
    "    get_reconstruction = K.function([pt_ae.layers[4].input],\n",
    "                                  [pt_ae.output])\n",
    "    x_embedded = predict_with_K_fn(get_embeddings, x)[0]\n",
    "    x_recon = predict_with_K_fn(get_reconstruction, x_embedded)[0]\n",
    "    reconstruction_mse = np.mean(np.square(x - x_recon))\n",
    "    print(\"using pretrained embeddings; sanity check, total reconstruction error:\", np.mean(reconstruction_mse))\n",
    "\n",
    "    del pt_ae\n",
    "\n",
    "    return x_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'mnist'\n",
    "\n",
    "if dataset == 'mnist':\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    # reshape and standardize x arrays\n",
    "    x_train = x_train.reshape(len(x_train), -1) / 255\n",
    "    x_test = x_test.reshape(len(x_test), -1) / 255 \n",
    "    latent_dim = 10\n",
    "elif dataset == 'fmnist':\n",
    "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "    # reshape and standardize x arrays\n",
    "    x_train = x_train.reshape(len(x_train), -1) / 255\n",
    "    x_test = x_test.reshape(len(x_test), -1) / 255 \n",
    "    latent_dim = 10\n",
    "elif dataset == 'mnist_conv':\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    # reshape and standardize x arrays\n",
    "    x_train = np.expand_dims(x_train, -1) / 255\n",
    "    x_test = np.expand_dims(x_test, -1) / 255 \n",
    "    latent_dim = 10\n",
    "elif dataset == 'fmnist_conv':\n",
    "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "    # reshape and standardize x arrays\n",
    "    x_train = np.expand_dims(x_train, -1) / 255\n",
    "    x_test = np.expand_dims(x_test, -1) / 255 \n",
    "    latent_dim = 10\n",
    "if dataset == 'cifar10_conv':\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    x_train = x_train / 255\n",
    "    x_test = x_test / 255 \n",
    "    y_train, y_test = y_train.reshape(-1), y_test.reshape(-1)\n",
    "    latent_dim = 20\n",
    "elif dataset == 'bulldog':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_bulldog()\n",
    "    latent_dim = 3\n",
    "elif dataset == 'gaussians':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_gaussians(n=2000, n_clusters=2, train_set_fraction=0.85)\n",
    "    latent_dim = 5\n",
    "elif dataset == 'line':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_line(n=2000, train_set_fraction=0.85)\n",
    "    latent_dim = 2\n",
    "elif dataset == 'loop':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_loop(n=5000, train_set_fraction=0.85)\n",
    "    latent_dim = 2\n",
    "elif dataset == 'cc':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_cc(n=2000, noise_sigma=0.01, train_set_fraction=0.85)\n",
    "    latent_dim = 3\n",
    "elif dataset == 'circle':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_circle(n=1024, train_set_fraction=0.85, alpha=30)\n",
    "    latent_dim = 2\n",
    "elif dataset == 'plane':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_plane()\n",
    "    latent_dim = 3\n",
    "elif dataset == 'sphere':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_sphere(n=2000)\n",
    "    latent_dim = 4\n",
    "elif dataset == 'bunny':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_bunny(n=20000)\n",
    "    latent_dim = 3\n",
    "elif dataset == 'faces':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_faces()\n",
    "    latent_dim = 2\n",
    "elif dataset == 'gaussian_grid':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_gaussian_grid()\n",
    "    latent_dim = 30\n",
    "\n",
    "x_all = np.concatenate([x_train, x_test], axis=0)\n",
    "    \n",
    "# normalize to between -1 and 1\n",
    "if 'mnist' not in dataset and 'cifar' not in dataset and 'bulldog' not in dataset and 'gaussian_grid' not in dataset:\n",
    "    m, M = np.min(x_train), np.max(x_train)\n",
    "    a = (M + m) / 2\n",
    "    b = (M - m) / 2\n",
    "    x_train, x_test = (x_train - a) / b, (x_test - a) / b\n",
    "print('IMPORTANT: max {}, min {}, shape {} {}'.format(np.max(x_train), np.min(x_train), x_train.shape, x_test.shape))\n",
    "\n",
    "if 'conv' in dataset:\n",
    "    arch = [\n",
    "        {'type': 'Conv2D', 'channels': 64, 'strides':(1,1), 'kernel':4},\n",
    "        {'type': 'Conv2D', 'channels': 64, 'strides':(1,1), 'kernel':4},\n",
    "        {'type': 'MaxPooling2D', 'pool_size': 2},\n",
    "        {'type': 'Conv2D', 'channels': 64, 'strides':(1,1), 'kernel':4},\n",
    "        {'type': 'Conv2D', 'channels': 64, 'strides':(1,1), 'kernel':4},\n",
    "        {'type': 'MaxPooling2D', 'pool_size': 2},\n",
    "#         {'type': 'Dropout', 'rate': 0.8},\n",
    "        {'type': 'Flatten'},\n",
    "        {'type': 'relu', 'size': 1024},\n",
    "        {'type': 'relu', 'size': 1024},\n",
    "        {'type': 'relu', 'size': 512},\n",
    "        {'type': 'linear', 'size': latent_dim},\n",
    "        ]\n",
    "else:\n",
    "#     arch = [\n",
    "#         {'type': 'relu', 'size': 500},\n",
    "#         {'type': 'linear', 'size': latent_dim},\n",
    "#         ]\n",
    "    arch = [\n",
    "            {'type': 'relu', 'size': 1024},\n",
    "            {'type': 'relu', 'size': 1024},\n",
    "            {'type': 'relu', 'size': 512},\n",
    "            {'type': 'relu', 'size': 10},\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNet:\n",
    "    def __init__(self, inputs, arch, siam_reg, y_true):\n",
    "        self.orig_inputs = inputs\n",
    "        # set up inputs\n",
    "        self.inputs = {\n",
    "                'A': inputs['Unlabeled'],\n",
    "                'B': Input(shape=inputs['Unlabeled'].get_shape().as_list()[1:]),\n",
    "                'Labeled': inputs['Labeled'],\n",
    "                }\n",
    "\n",
    "        self.y_true = y_true\n",
    "\n",
    "        # generate layers\n",
    "        self.layers = []\n",
    "        self.layers += make_layer_list(arch, 'siamese', siam_reg)\n",
    "\n",
    "        # create the siamese net\n",
    "        self.outputs = stack_layers(self.inputs, self.layers)\n",
    "\n",
    "        # add the distance layer\n",
    "        self.distance = Lambda(costs.euclidean_distance, output_shape=costs.eucl_dist_output_shape)([self.outputs['A'], self.outputs['B']])\n",
    "\n",
    "        #create the distance model for training\n",
    "        self.net = Model([self.inputs['A'], self.inputs['B']], self.distance)\n",
    "\n",
    "        # compile the siamese network\n",
    "        self.net.compile(loss=costs.get_contrastive_loss(m_neg=1, m_pos=0.05), optimizer='rmsprop')\n",
    "\n",
    "    def train(self, pairs_train, dist_train, pairs_val, dist_val,\n",
    "            lr, drop, patience, num_epochs, batch_size):\n",
    "        # create handler for early stopping and learning rate scheduling\n",
    "        self.lh = LearningHandler(\n",
    "                lr=lr,\n",
    "                drop=drop,\n",
    "                lr_tensor=self.net.optimizer.lr,\n",
    "                patience=patience)\n",
    "\n",
    "        # initialize the training generator\n",
    "        train_gen_ = train_gen(pairs_train, dist_train, batch_size)\n",
    "\n",
    "        # format the validation data for keras\n",
    "        validation_data = ([pairs_val[:, 0], pairs_val[:, 1]], dist_val)\n",
    "\n",
    "        # compute the steps per epoch\n",
    "        steps_per_epoch = int(len(pairs_train) / batch_size)\n",
    "\n",
    "        # train the network\n",
    "        hist = self.net.fit_generator(train_gen_, epochs=num_epochs, validation_data=validation_data, steps_per_epoch=steps_per_epoch, callbacks=[self.lh])\n",
    "\n",
    "        return hist\n",
    "\n",
    "    def predict(self, x, batch_sizes):\n",
    "        # compute the siamese embeddings of the input data\n",
    "        return train.predict(self.outputs['A'], x_unlabeled=x, inputs=self.orig_inputs, y_true=self.y_true, batch_sizes=batch_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SiameseNet:\n",
    "#     def __init__(self, inputs, arch, siam_reg, y_true):\n",
    "#         self.orig_inputs = inputs\n",
    "        \n",
    "#         # set up inputs\n",
    "#         self.inputs = {\n",
    "#                 'A': inputs['Unlabeled'],\n",
    "#                 'B': Input(shape=inputs['Unlabeled'].get_shape().as_list()[1:]),\n",
    "#                 'Labeled': inputs['Labeled'],\n",
    "#                 }\n",
    "\n",
    "#         self.y_true = y_true\n",
    "\n",
    "#         # generate layers\n",
    "#         self.layers = [\n",
    "#             {'type': 'relu', 'size': 500},\n",
    "#             {'type': 'relu', 'size': 500},\n",
    "#             {'type': 'relu', 'size': 2000},\n",
    "#             {'type': 'linear', 'size': 10},\n",
    "#             ]\n",
    "#         self.layers += make_layer_list(arch, 'siamese', siam_reg)\n",
    "\n",
    "#         # create the siamese net\n",
    "#         self.outputs = stack_layers(self.inputs, self.layers)\n",
    "\n",
    "#         # add the distance layer\n",
    "#         self.distance = Lambda(costs.euclidean_distance, output_shape=costs.eucl_dist_output_shape)([self.outputs['A'], self.outputs['B']])\n",
    "\n",
    "#         #create the distance model for training\n",
    "#         self.net = Model([self.inputs['A'], self.inputs['B']], self.distance)\n",
    "        \n",
    "#         # load autoencoder weights from pt_ae and then freeze them\n",
    "#         json_path = '../pretrain_weights/ae_mnist.json'\n",
    "#         weights_path = '../pretrain_weights/ae_mnist_weights.h5'\n",
    "\n",
    "#         with open(json_path) as f:\n",
    "#             pt_ae = model_from_json(f.read())\n",
    "#         pt_ae.load_weights(weights_path)\n",
    "        \n",
    "#         print([l.weights for l in self.net.layers])\n",
    "        \n",
    "#         for l_from, l_to in zip(pt_ae.layers[:4], self.net.layers[2:6]):\n",
    "#             l_to.set_weights(l_from.get_weights())\n",
    "#             l_to.trainable = False\n",
    "\n",
    "#         # compile the siamese network\n",
    "#         self.net.compile(loss=costs.get_contrastive_loss(m_neg=1, m_pos=0.05), optimizer='rmsprop')\n",
    "\n",
    "#     def train(self, pairs_train, dist_train, pairs_val, dist_val,\n",
    "#             lr, drop, patience, num_epochs, batch_size):\n",
    "#         # create handler for early stopping and learning rate scheduling\n",
    "#         self.lh = LearningHandler(\n",
    "#                 lr=lr,\n",
    "#                 drop=drop,\n",
    "#                 lr_tensor=self.net.optimizer.lr,\n",
    "#                 patience=patience)\n",
    "\n",
    "#         # initialize the training generator\n",
    "#         train_gen_ = train_gen(pairs_train, dist_train, batch_size)\n",
    "\n",
    "#         # format the validation data for keras\n",
    "#         validation_data = ([pairs_val[:, 0], pairs_val[:, 1]], dist_val)\n",
    "\n",
    "#         # compute the steps per epoch\n",
    "#         steps_per_epoch = int(len(pairs_train) / batch_size)\n",
    "\n",
    "#         # train the network\n",
    "#         hist = self.net.fit_generator(train_gen_, epochs=num_epochs, validation_data=validation_data, steps_per_epoch=steps_per_epoch, callbacks=[self.lh])\n",
    "\n",
    "#         return hist\n",
    "\n",
    "#     def predict(self, x, batch_sizes):\n",
    "#         # compute the siamese embeddings of the input data\n",
    "#         return train.predict(self.outputs['A'], x_unlabeled=x, inputs=self.orig_inputs, y_true=self.y_true, batch_sizes=batch_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralNet:\n",
    "    def __init__(self, inputs, arch, spec_reg, y_true, y_train_labeled_onehot,\n",
    "            n_clusters, affinity, scale_nbr, n_nbrs, batch_sizes, normalized=False,\n",
    "            siamese_net=None, x_train=None, have_labeled=False):\n",
    "        self.y_true = y_true\n",
    "        self.y_train_labeled_onehot = y_train_labeled_onehot\n",
    "        self.inputs = inputs\n",
    "        self.batch_sizes = batch_sizes\n",
    "        self.normalized = normalized\n",
    "        # generate layers\n",
    "        self.layers = make_layer_list(arch[:-1], 'spectral', spec_reg)\n",
    "        self.layers += [\n",
    "                  {'type': 'tanh',\n",
    "                   'size': n_clusters,\n",
    "                   'l2_reg': spec_reg,\n",
    "                   'name': 'spectral_{}'.format(len(arch)-1)},\n",
    "                  {'type': 'Orthonorm', 'name':'orthonorm'}\n",
    "                  ]\n",
    "\n",
    "        # create spectralnet\n",
    "        self.outputs = stack_layers(self.inputs, self.layers)\n",
    "        self.net = Model(inputs=self.inputs['Unlabeled'], outputs=self.outputs['Unlabeled'])\n",
    "\n",
    "        # DEFINE LOSS\n",
    "\n",
    "        # generate affinity matrix W according to params\n",
    "        if affinity == 'siamese':\n",
    "            input_affinity = tf.concat([siamese_net.outputs['A'], siamese_net.outputs['Labeled']], axis=0)\n",
    "            x_affinity = siamese_net.predict(x_train, batch_sizes)\n",
    "        elif affinity in ['knn', 'full']:\n",
    "            input_affinity = tf.concat([self.inputs['Unlabeled'], self.inputs['Labeled']], axis=0)\n",
    "            x_affinity = x_train\n",
    "\n",
    "        # calculate scale for affinity matrix\n",
    "        scale = get_scale(x_affinity, self.batch_sizes['Unlabeled'], scale_nbr)\n",
    "\n",
    "        # create affinity matrix\n",
    "        if affinity == 'full':\n",
    "            W = costs.full_affinity(input_affinity, scale=scale)\n",
    "        elif affinity in ['knn', 'siamese']:\n",
    "            W = costs.knn_affinity(input_affinity, n_nbrs, scale=scale, scale_nbr=scale_nbr)\n",
    "\n",
    "        # if we have labels, use them\n",
    "        if have_labeled:\n",
    "            # get true affinities (from labeled data)\n",
    "            W_true = tf.cast(tf.equal(costs.squared_distance(y_true), 0),dtype='float32')\n",
    "\n",
    "            # replace lower right corner of W with W_true\n",
    "            unlabeled_end = tf.shape(self.inputs['Unlabeled'])[0]\n",
    "            W_u = W[:unlabeled_end, :]                  # upper half\n",
    "            W_ll = W[unlabeled_end:, :unlabeled_end]    # lower left\n",
    "            W_l = tf.concat((W_ll, W_true), axis=1)      # lower half\n",
    "            W = tf.concat((W_u, W_l), axis=0)\n",
    "\n",
    "            # create pairwise batch distance matrix self.Dy\n",
    "            y_ = tf.concat([self.outputs['Unlabeled'], self.outputs['Labeled']], axis=0)\n",
    "        else:\n",
    "            y_ = self.outputs['Unlabeled']\n",
    "\n",
    "        if self.normalized:\n",
    "            y_ = y_ / tf.reduce_sum(W, axis=1)\n",
    "\n",
    "        self.Dy = costs.squared_distance(y_)\n",
    "\n",
    "        # define loss\n",
    "        self.loss = K.sum(W * self.Dy) / (2 * batch_sizes['Unlabeled'])\n",
    "\n",
    "        # create the train step update\n",
    "        self.learning_rate = tf.Variable(0., name='spectral_net_learning_rate')\n",
    "        self.train_step = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate).minimize(self.loss, var_list=self.net.trainable_weights)\n",
    "\n",
    "        # initialize spectralnet variables\n",
    "        K.get_session().run(tf.variables_initializer(self.net.trainable_weights))\n",
    "\n",
    "    def train(self, x_train_unlabeled, x_train_labeled, x_val_unlabeled,\n",
    "            lr, drop, patience, num_epochs):\n",
    "        # create handler for early stopping and learning rate scheduling\n",
    "        self.lh = LearningHandler(\n",
    "                lr=lr,\n",
    "                drop=drop,\n",
    "                lr_tensor=self.learning_rate,\n",
    "                patience=patience)\n",
    "\n",
    "        losses = np.empty((num_epochs,))\n",
    "        val_losses = np.empty((num_epochs,))\n",
    "\n",
    "        # begin spectralnet training loop\n",
    "        self.lh.on_train_begin()\n",
    "        i = 0\n",
    "        for i in range(num_epochs):\n",
    "            # train spectralnet\n",
    "            losses[i] = train.train_step(\n",
    "                    return_var=[self.loss],\n",
    "                    updates=self.net.updates + [self.train_step],\n",
    "                    x_unlabeled=x_train_unlabeled,\n",
    "                    inputs=self.inputs,\n",
    "                    y_true=self.y_true,\n",
    "                    batch_sizes=self.batch_sizes,\n",
    "                    x_labeled=x_train_labeled,\n",
    "                    y_labeled=self.y_train_labeled_onehot,\n",
    "                    batches_per_epoch=100)[0]\n",
    "\n",
    "            # get validation loss\n",
    "            val_losses[i] = train.predict_sum(\n",
    "                    self.loss,\n",
    "                    x_unlabeled=x_val_unlabeled,\n",
    "                    inputs=self.inputs,\n",
    "                    y_true=self.y_true,\n",
    "                    x_labeled=x_train_unlabeled[0:0],\n",
    "                    y_labeled=self.y_train_labeled_onehot,\n",
    "                    batch_sizes=self.batch_sizes)\n",
    "\n",
    "            # do early stopping if necessary\n",
    "            if self.lh.on_epoch_end(i, val_losses[i]):\n",
    "                print('STOPPING EARLY')\n",
    "                break\n",
    "\n",
    "            # print training status\n",
    "            print(\"Epoch: {}, loss={:2f}, val_loss={:2f}\".format(i, losses[i], val_losses[i]))\n",
    "\n",
    "        return losses[:i+1], val_losses[:i+1]\n",
    "\n",
    "    def predict(self, x):\n",
    "        # test inputs do not require the 'Labeled' input\n",
    "        inputs_test = {'Unlabeled': self.inputs['Unlabeled'], 'Orthonorm': self.inputs['Orthonorm']}\n",
    "        return train.predict(\n",
    "                    self.outputs['Unlabeled'],\n",
    "                    x_unlabeled=x,\n",
    "                    inputs=inputs_test,\n",
    "                    y_true=self.y_true,\n",
    "                    x_labeled=x[0:0],\n",
    "                    y_labeled=self.y_train_labeled_onehot[0:0],\n",
    "                    batch_sizes=self.batch_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SpectralNet:\n",
    "#     def __init__(self, inputs, arch, spec_reg, y_true, y_train_labeled_onehot,\n",
    "#             n_clusters, affinity, scale_nbr, n_nbrs, batch_sizes, normalized=False,\n",
    "#             siamese_net=None, x_train=None, have_labeled=False, eps=1e-7):\n",
    "#         self.y_true = y_true\n",
    "#         self.y_train_labeled_onehot = y_train_labeled_onehot\n",
    "#         self.eps = eps\n",
    "#         self.inputs = inputs\n",
    "#         self.batch_sizes = batch_sizes\n",
    "#         self.normalized = normalized\n",
    "#         # generate layers\n",
    "#         self.layers = make_layer_list(arch[:-1], 'spectral', spec_reg)\n",
    "#         self.layers += [\n",
    "#                   {'type': 'linear',\n",
    "#                    'size': n_clusters,\n",
    "#                    'l2_reg': spec_reg,\n",
    "#                    'name': 'spectral_{}'.format(len(arch)-1)},\n",
    "#                   {'type': 'Orthonorm', 'name':'orthonorm'}\n",
    "#                   ]\n",
    "\n",
    "#         # create spectralnet\n",
    "#         self.outputs = stack_layers(self.inputs, self.layers)\n",
    "#         self.net = Model(inputs=self.inputs['Unlabeled'], outputs=self.outputs['Unlabeled'])\n",
    "\n",
    "#         # DEFINE LOSS\n",
    "\n",
    "#         # generate affinity matrix W according to params\n",
    "#         if siamese_net is not None:\n",
    "#             input_affinity = tf.concat([siamese_net.outputs['A'], siamese_net.outputs['Labeled']], axis=0)\n",
    "#             x_affinity = siamese_net.predict(x_train, batch_sizes)\n",
    "#         else:\n",
    "#             input_affinity = tf.concat([self.inputs['Unlabeled'], self.inputs['Labeled']], axis=0)\n",
    "#             x_affinity = x_train\n",
    "\n",
    "#         # calculate scale for affinity matrix\n",
    "#         scale = get_scale(x_affinity, self.batch_sizes['Unlabeled'], scale_nbr)\n",
    "\n",
    "#         # create affinity matrix\n",
    "#         W = costs.knn_affinity(input_affinity, n_nbrs, scale=scale, scale_nbr=scale_nbr)\n",
    "\n",
    "#         y_ = self.outputs['Unlabeled']\n",
    "            \n",
    "#         if self.normalized:\n",
    "#             # compute the row and column denominators\n",
    "#             row_norm = 1 / tf.reduce_sum(W, axis=1)\n",
    "#             col_norm = 1 / tf.reduce_sum(W, axis=0)\n",
    "            \n",
    "#             # apply them\n",
    "#             W = tf.einsum('j,ij->ij', col_norm, tf.einsum('i,ij->ij', row_norm, W))\n",
    "            \n",
    "#             # compute D^{-1/2} and apply it\n",
    "#             self.D_half_inv = dhv = tf.sqrt((1 / tf.reduce_sum(W, axis=1)) + self.eps)\n",
    "#             W = tf.einsum('j,ij->ij', dhv, tf.einsum('i,ij->ij', dhv, W))\n",
    "#             self.normalized_output = y_ * tf.expand_dims(dhv, -1)\n",
    "        \n",
    "#         self.Dy = costs.squared_distance(y_)\n",
    "\n",
    "#         # define loss\n",
    "#         self.loss = K.sum(W * self.Dy) / (2 * batch_sizes['Unlabeled'])\n",
    "\n",
    "#         # create the train step update\n",
    "#         self.learning_rate = tf.Variable(0., name='spectral_net_learning_rate')\n",
    "#         self.train_step = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate).minimize(self.loss, var_list=self.net.trainable_weights)\n",
    "# #         self.train_step = tf.train.AdamOptimizer().minimize(self.loss, var_list=self.net.trainable_weights)\n",
    "        \n",
    "#         # initialize spectralnet variables\n",
    "#         K.get_session().run(tf.variables_initializer(self.net.trainable_weights))\n",
    "\n",
    "#     def train(self, x_train_unlabeled, x_train_labeled, x_val_unlabeled,\n",
    "#             lr, drop, patience, num_epochs):\n",
    "#         # create handler for early stopping and learning rate scheduling\n",
    "#         self.lh = LearningHandler(\n",
    "#                 lr=lr,\n",
    "#                 drop=drop,\n",
    "#                 lr_tensor=self.learning_rate,\n",
    "#                 patience=patience)\n",
    "\n",
    "#         losses = np.empty((num_epochs,))\n",
    "#         val_losses = np.empty((num_epochs,))\n",
    "\n",
    "#         # begin spectralnet training loop\n",
    "#         self.lh.on_train_begin()\n",
    "#         i = 0\n",
    "#         for i in range(num_epochs):\n",
    "#             # train spectralnet\n",
    "#             losses[i] = train.train_step(\n",
    "#                     return_var=[self.loss],\n",
    "#                     updates=self.net.updates + [self.train_step],\n",
    "#                     x_unlabeled=x_train_unlabeled,\n",
    "#                     inputs=self.inputs,\n",
    "#                     y_true=self.y_true,\n",
    "#                     batch_sizes=self.batch_sizes,\n",
    "#                     x_labeled=x_train_labeled,\n",
    "#                     y_labeled=self.y_train_labeled_onehot,\n",
    "#                     batches_per_epoch=100)[0]\n",
    "\n",
    "#             # get validation loss\n",
    "#             val_losses[i] = train.predict_sum(\n",
    "#                     self.loss,\n",
    "#                     x_unlabeled=x_val_unlabeled,\n",
    "#                     inputs=self.inputs,\n",
    "#                     y_true=self.y_true,\n",
    "#                     x_labeled=x_train_unlabeled[0:0],\n",
    "#                     y_labeled=self.y_train_labeled_onehot,\n",
    "#                     batch_sizes=self.batch_sizes)\n",
    "\n",
    "#             # do early stopping if necessary\n",
    "#             if self.lh.on_epoch_end(i, val_losses[i]):\n",
    "#                 print('STOPPING EARLY')\n",
    "#                 break\n",
    "\n",
    "#             # print training status\n",
    "#             print(\"Epoch: {}, loss={:2f}, val_loss={:2f}\".format(i, losses[i], val_losses[i]))\n",
    "\n",
    "#         return losses[:i+1], val_losses[:i+1]\n",
    "    \n",
    "#     def predict(self, x):\n",
    "#         # test inputs do not require the 'Labeled' input\n",
    "#         inputs_test = {'Unlabeled': self.inputs['Unlabeled'], 'Orthonorm': self.inputs['Orthonorm'], 'Labeled':self.inputs['Labeled']}\n",
    "#         pred_tensor = self.normalized_output if self.normalized else self.outputs['Unlabeled']\n",
    "#         return train.predict(\n",
    "#                     pred_tensor,\n",
    "#                     x_unlabeled=x,\n",
    "#                     inputs=inputs_test,\n",
    "#                     y_true=self.y_true,\n",
    "#                     x_labeled=x[0:0],\n",
    "#                     y_labeled=self.y_train_labeled_onehot[0:0],\n",
    "#                     batch_sizes=self.batch_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_nearest_k(D, k, drop_self=True, randomize=False):\n",
    "            n_batch = tf.shape(D)[-1]\n",
    "            if drop_self:\n",
    "                _, idxs = tf.nn.top_k(-D, k=k+1)\n",
    "                idxs = idxs[:,1:]\n",
    "            else:\n",
    "                _, idxs = tf.nn.top_k(-D, k=k)\n",
    "            # create a random index \n",
    "            if randomize:\n",
    "                range_D = tf.expand_dims(tf.range(n_batch * k), -1)\n",
    "                p = tf.expand_dims(tf.random.uniform(shape=(n_batch * k,), maxval=k, dtype=tf.dtypes.int32), -1)\n",
    "                p = tf.concat([range_D, p], axis=1)\n",
    "                # draw from the top k values using this index\n",
    "                idx = tf.expand_dims(tf.gather_nd(idxs, p), -1)\n",
    "            else:\n",
    "                print(\"IMPORTANT\", idxs.shape)\n",
    "                idx = tf.reshape(idxs, (n_batch * k, 1))\n",
    "            \n",
    "            return idx\n",
    "        \n",
    "class SVG:\n",
    "    def __init__(self, inputs, spectralnet, orig_dim, remove_dim=0, \n",
    "                 pca=True, alpha=1., normalize_factor=.1, k=16, eps=1e-5, lam=1e-1,\n",
    "                 arch=[500], conv_decoder=False, siamesenet=None):\n",
    "        optimizer = 'adam'\n",
    "#         optimizer = RMSprop(lr=0.00005)\n",
    "        self.input = inputs['Unlabeled']\n",
    "        self.orig_dim = orig_dim\n",
    "        self.eps = eps\n",
    "        self.lam = lam\n",
    "        self.pca = pca\n",
    "        self.alpha = float(alpha)\n",
    "        self.k = k\n",
    "        self.remove_dim = remove_dim\n",
    "        self.conv_decoder = conv_decoder\n",
    "        self.arch = arch\n",
    "        self.siamesenet = siamesenet\n",
    "        \n",
    "        if conv_decoder:\n",
    "            print(\"USING CONVOLUTIONAL DECODER! CHECK OUTPUT SHAPE:\", self.orig_dim)\n",
    "        else:\n",
    "            print(\"USING DENSE DECODER! FLATTENING OUTPUT SHAPE IF NOT ALREADY FLAT:\", self.orig_dim, np.prod(self.orig_dim))\n",
    "            self.orig_dim = np.prod(self.orig_dim)\n",
    "        \n",
    "        self.x = x = self.copy_spectralnet(spectralnet, siamesenet=siamesenet)\n",
    "        \n",
    "        #\n",
    "        # DEFINE ALL LOSSES\n",
    "        #\n",
    "        def top_k_loss(D, k):\n",
    "            # get nearest (i.e., largest negative distance) neighbors of each point\n",
    "            vals, _ = tf.nn.top_k(-D, k=k)\n",
    "\n",
    "            # remove self as neighbor, negate to get positive distances again\n",
    "            vals = -vals\n",
    "                \n",
    "            return K.sum(vals)\n",
    "        def kl_loss(_, __):\n",
    "            log_e = 2 * self.half_log_e\n",
    "            v = tf.reshape(self.v, (-1, self.latent_dim, self.latent_dim))\n",
    "            # find local neighborhood of each point in batch\n",
    "            D = self.pairwise_distances(self.z, self.z_mu)\n",
    "            n_batch = tf.shape(D)[-1]\n",
    "            idx = pick_nearest_k(D, self.k, drop_self=True)\n",
    "            omn = z_mu_nb = tf.gather_nd(self.z_mu, idx)\n",
    "            z_mu_nb = tf.reshape(z_mu_nb, shape=(n_batch, self.k, self.latent_dim))\n",
    "            print(\"z_mu_nb\", z_mu_nb.shape)\n",
    "            \n",
    "            # obtain covariance of each local neighborhood\n",
    "            cov_nb = tf.einsum('ikj,ikl->ijl', z_mu_nb, z_mu_nb)\n",
    "            cov_nb = tf.Print(cov_nb, [tf.shape(omn), tf.shape(z_mu_nb), omn[:self.k], z_mu_nb[0,:,:]], summarize=10)\n",
    "            \n",
    "            # obtain eigendecomposition of local neighborhood\n",
    "            e_nb, v_nb = tf.linalg.eigh(cov_nb)\n",
    "            e_nb += self.lam\n",
    "            \n",
    "            # take log (used in final loss) BEFORE truncating\n",
    "            log_e_nb = tf.log(e_nb + self.eps)\n",
    "            \n",
    "            e_nb *= self.alpha\n",
    "            \n",
    "            # compute trace of Sigma_cov^{-1} Sigma_theta\n",
    "            inv_sigma_nb = tf.einsum('ijk,ilk->ijl', tf.einsum('ijk,ik->ijk', v_nb, 1 / e_nb), v_nb)\n",
    "            \n",
    "            sigma_vae = tf.einsum('ijk,ilk->ijl', tf.einsum('ijk,ik->ijk', v, self.e), v)\n",
    "            half_prod = tf.einsum('ikj,ikl->ijl', inv_sigma_nb, sigma_vae)\n",
    "            trace = tf.linalg.svd(half_prod, compute_uv=False)\n",
    "            \n",
    "            # compute KL divergence\n",
    "            self.kl_loss = tf.reduce_sum(log_e_nb - log_e - 1 + trace)\n",
    "            return self.kl_loss\n",
    "        def pca_loss(_, __):\n",
    "            if self.pca:\n",
    "                self.pca_loss = K.sum(mse(self.pca_input, self.pca_recon)) * np.prod(self.orig_dim)\n",
    "            else:\n",
    "                self.pca_loss = tf.constant(0.)\n",
    "            return self.pca_loss\n",
    "        def neighborhood_loss(_, __):\n",
    "            # involves two bursts, on-manifold burst and off-manifold burst\n",
    "            z = self.z\n",
    "            x_recon = self.x_recon\n",
    "            \n",
    "            # obtain pairwise distances (size(recon) x size(input))\n",
    "            D = self.pairwise_distances(z, self.z_mu)\n",
    "            n_batch = tf.shape(D)[-1]\n",
    "            idx = pick_nearest_k(D, self.k, drop_self=True)\n",
    "            \n",
    "            # now compute neighborhood\n",
    "            orig_dim = np.prod(self.orig_dim)\n",
    "            data_flat_shape = (-1, np.prod(self.orig_dim))\n",
    "            input_ = tf.reshape(self.input, data_flat_shape)\n",
    "            print(\"important input shape\", input_.shape)\n",
    "            input_neighborhood = tf.gather_nd(input_, idx)\n",
    "            x_recon_flattened_expanded = tf.reshape(x_recon, data_flat_shape + (1,))\n",
    "            x_recon = tf.reshape(tf.tile(x_recon_flattened_expanded, [1, self.k, 1]), data_flat_shape)\n",
    "            print(\"NEIGHBORHOOD AND RECON SHAPES:\", input_neighborhood.shape, x_recon.shape)\n",
    "            self.neighborhood_loss = tf.reduce_sum(mse(input_neighborhood, x_recon)) / self.k\n",
    "            \n",
    "            return self.neighborhood_loss\n",
    "        def vae_loss(_, __):\n",
    "            return self.loss\n",
    "        \n",
    "        #\n",
    "        # DEFINE LAYERS\n",
    "        #\n",
    "\n",
    "        # create encoder\n",
    "        self.x_enc = x_enc = self.build_encoder(x, arch=self.arch, remove_dim=remove_dim, pca=self.pca)\n",
    "        self.encoder = Model(inputs=self.input, outputs=x_enc)\n",
    "\n",
    "        # create decoder\n",
    "        self.x_recon = x_recon = self.build_decoder(x_enc, arch=self.arch)\n",
    "        self.decoder_input = Input(shape=(self.latent_dim,), name='UnlabeledInput')\n",
    "        self.decoder_output = self.build_decoder(self.decoder_input, arch=self.arch)\n",
    "        self.decoder = Model(inputs=self.decoder_input, outputs=self.decoder_output)\n",
    "        \n",
    "        # create normalized decoder\n",
    "        x_enc_norm = self.build_encoder(x, arch=self.arch, normalize_cov=normalize_factor, pca=self.pca)\n",
    "        self.x_recon_norm = self.build_decoder(x_enc_norm, arch=self.arch)\n",
    "        \n",
    "        if self.pca:\n",
    "            self.pcae = Model(inputs=self.input, outputs=self.pca_recon)\n",
    "            self.pc = Model(inputs=self.input, outputs=self.pc_embedding)\n",
    "            self.pcae.compile(optimizer=optimizer, loss=pca_loss)\n",
    "            \n",
    "        #\n",
    "        # COMPUTE LOSS\n",
    "        #\n",
    "        losses = [kl_loss, pca_loss, neighborhood_loss]\n",
    "        self.init_losses = [l(None, None) for l in losses]\n",
    "        loss_weights = [1, 0, 1]\n",
    "        # initialize losses\n",
    "        self.loss = sum([a * b if b != 0 else K.constant(0.) for a, b in zip(self.init_losses, loss_weights)])\n",
    "        \n",
    "        #\n",
    "        # ASSEMBLE NETWORK\n",
    "        #\n",
    "        self.vae = Model(inputs=self.input, outputs=self.x_recon)\n",
    "        self.vae.compile(optimizer=optimizer, loss=vae_loss)\n",
    "        \n",
    "    def pairwise_distances(self, A, B):\n",
    "        r_A, r_B = tf.reduce_sum(A*A, 1), tf.reduce_sum(B*B, 1)\n",
    "\n",
    "        # turn r into column vector\n",
    "        r_A, r_B = tf.reshape(r_A, [-1, 1]), tf.reshape(r_B, [-1, 1])\n",
    "        D = r_A - 2 * tf.matmul(A, B, transpose_b=True) + tf.transpose(r_B)\n",
    "\n",
    "        return D\n",
    "        \n",
    "    def build_decoder(self, x, arch):\n",
    "        if not hasattr(self, 'decoder_layers'):\n",
    "            if self.conv_decoder:\n",
    "                in_channels = 1\n",
    "                out_channels = 1 # int(self.input.shape[-1])\n",
    "                pix = 28 # int(self.input.shape[-2])\n",
    "                reshaped_dim = [int(pix / 4), int(pix / 4), in_channels]\n",
    "                inputs_decoder = int(np.prod(reshaped_dim))\n",
    "                keep_prob = 0.8\n",
    "                self.decoder_layers = [\n",
    "#                     Dense(inputs_decoder),\n",
    "#                     LeakyReLU(),\n",
    "#                     Dense(inputs_decoder * 2 + 1),\n",
    "#                     LeakyReLU(),\n",
    "#                     Reshape(reshaped_dim),\n",
    "#                     Conv2DTranspose(filters=64, kernel_size=4, strides=2, padding='same', activation='relu'),\n",
    "#                     Dropout(keep_prob),\n",
    "#                     Conv2DTranspose(filters=64, kernel_size=4, strides=1, padding='same', activation='relu'),\n",
    "#                     Dropout(keep_prob),\n",
    "#                     Conv2DTranspose(filters=64, kernel_size=4, strides=1, padding='same', activation='relu'),\n",
    "#                     Flatten(),\n",
    "#                     Dense(np.prod(self.orig_dim)),\n",
    "#                     LeakyReLU(),\n",
    "                    Dense(int(inputs_decoder / 2)),\n",
    "                    LeakyReLU(),\n",
    "                    Dense(inputs_decoder),\n",
    "                    LeakyReLU(),\n",
    "                    Reshape(reshaped_dim),\n",
    "                    Conv2DTranspose(filters=64, kernel_size=4, strides=1, padding='same', activation='relu'),\n",
    "                    Conv2DTranspose(filters=64, kernel_size=4, strides=1, padding='same', activation='relu'),\n",
    "                    UpSampling2D(),\n",
    "                    Conv2DTranspose(filters=64, kernel_size=4, strides=1, padding='same', activation='relu'),\n",
    "                    Conv2DTranspose(filters=out_channels, kernel_size=4, strides=1, padding='same', activation='relu'),\n",
    "                    UpSampling2D(),\n",
    "                    Reshape([pix * pix * out_channels]),\n",
    "#                     Dense(np.prod(self.orig_dim)),\n",
    "#                     LeakyReLU(),\n",
    "                    \n",
    "                    ]\n",
    "            else:\n",
    "                self.decoder_layers = [Dense(a, activation='relu') for a in arch]\n",
    "                self.decoder_layers.append(Dense(self.orig_dim, activation='linear'))\n",
    "\n",
    "        xs = [x]\n",
    "        for l in self.decoder_layers:\n",
    "            xs.append(l(xs[-1]))\n",
    "#             x = l(x)\n",
    "        x = xs[-1]\n",
    "        self.decoder_xs = xs\n",
    "        return x\n",
    "        \n",
    "    def build_encoder(self, x, arch, pca=False, normalize_cov=False, no_noise=False, remove_dim=0):\n",
    "        if pca and not hasattr(self, 'pca_layers'):\n",
    "            self.pca_layers = [Dense(self.latent_dim, activation='linear'), \n",
    "                               Dense(self.spectralnet_dim, activation='linear')]\n",
    "            \n",
    "        if not hasattr(self, 'encoder_layers'):\n",
    "            self.encoder_precov_layers = [Dense(a, activation='relu') for a in arch]\n",
    "            self.encoder_precov_layers.append(Dense(self.latent_dim * self.latent_dim, activation='linear'))\n",
    "            self.encoder_eig_layers = [Dense(a, activation='relu') for a in arch]\n",
    "            self.encoder_eig_layers.append(Dense(self.latent_dim, activation='linear'))\n",
    "            \n",
    "        # assemble pca layer (a linear autoencoder) and define mu (the latent embedding of this layer)\n",
    "        if pca:\n",
    "            if not hasattr(self, 'pca_input'):\n",
    "                self.pca_input = x\n",
    "            \n",
    "            self.pc_embedding = x = self.pca_layers[0](x)\n",
    "\n",
    "            if not hasattr(self, 'pca_recon'):\n",
    "                self.pca_recon = self.pca_layers[1](x)\n",
    "\n",
    "        # define mu (the latent embedding of the pca layer)\n",
    "        mu = x\n",
    "        if not hasattr(self, 'mu'):\n",
    "            self.z_mu = mu\n",
    "        \n",
    "        x_precov = x\n",
    "        # get covariance precursor\n",
    "        for l in self.encoder_precov_layers:\n",
    "            x_precov = l(x_precov)\n",
    "            \n",
    "        x_eig = x\n",
    "        # get eigenvalues\n",
    "        for l in self.encoder_eig_layers:\n",
    "            x_eig = l(x_eig)\n",
    "        \n",
    "        # sample latent space (and normalize covariances if we're trying to do random walks)\n",
    "        if not hasattr(self, 'encoder_sampling_layer'):\n",
    "            f = partial(self.sampling, normalize_cov=normalize_cov, remove_dim=remove_dim)\n",
    "            self.encoder_sampling_layer = Lambda(f, output_shape=(self.latent_dim,), name='z')\n",
    "            \n",
    "        if no_noise:\n",
    "            cur_encoder_sampling_layer = Lambda(lambda x_: x_[0], output_shape=(self.latent_dim,))\n",
    "            \n",
    "        # get encoder embedding\n",
    "        x_enc = self.encoder_sampling_layer([mu, x_precov, x_eig])\n",
    "        \n",
    "        return x_enc\n",
    "        \n",
    "    def copy_spectralnet(self, spectralnet, siamesenet):\n",
    "        xs = [self.input]\n",
    "        layers = []\n",
    "        \n",
    "        # load siamesenet if available\n",
    "        if siamesenet is not None:\n",
    "            # load autoencoder\n",
    "            json_path = '../pretrain_weights/ae_mnist.json'\n",
    "            weights_path = '../pretrain_weights/ae_mnist_weights.h5'\n",
    "\n",
    "            with open(json_path) as f:\n",
    "                pt_ae = model_from_json(f.read())\n",
    "            pt_ae.load_weights(weights_path)\n",
    "                \n",
    "            # copy autoencoder\n",
    "            for l in pt_ae.layers[:4]:\n",
    "                l.trainable = False\n",
    "                xs.append(l(xs[-1]))\n",
    "                layers.append(l)\n",
    "                \n",
    "#             # copy siamesenet\n",
    "#             for l in siamesenet.net.layers[2:-1]:\n",
    "#                 l.trainable = False\n",
    "#                 xs.append(l(xs[-1]))\n",
    "#                 layers.append(l)\n",
    "        \n",
    "        for l in spectralnet.net.layers[1:-1]:\n",
    "            l.trainable = False\n",
    "            xs.append(l(xs[-1]))\n",
    "            layers.append(l)\n",
    "        \n",
    "        pre_x = xs[-1]\n",
    "        # add orthonorm layer\n",
    "        sess = K.get_session()\n",
    "        with tf.variable_scope('', reuse=True):\n",
    "            v = tf.get_variable(\"ortho_weights_store\")\n",
    "        ows = sess.run(v)\n",
    "        t_ows = K.variable(ows)\n",
    "        l = Lambda(lambda x: K.dot(x, t_ows))\n",
    "        l.trainable = False\n",
    "        xs.append(l(xs[-1]))\n",
    "        layers.append(l)\n",
    "\n",
    "        x = xs[-1]\n",
    "        \n",
    "        self.xs = xs\n",
    "\n",
    "        self.sn = Model(inputs=self.input, outputs=x)\n",
    "\n",
    "        self.spectralnet_dim = int(x.get_shape()[1])\n",
    "        if self.pca:\n",
    "            print(\"PCA\")\n",
    "            self.latent_dim = self.spectralnet_dim - 1\n",
    "        else:\n",
    "            print(\"NO PCA\")\n",
    "            self.latent_dim = self.spectralnet_dim\n",
    "            \n",
    "        return x\n",
    "\n",
    "    def sampling(self, args, normalize_cov, remove_dim):\n",
    "        # get args\n",
    "        z_mean, precov, e = args\n",
    "        \n",
    "        # reshape precov and compute cov = precov x precov.T\n",
    "        cov = tf.reshape(precov, (-1, self.latent_dim, self.latent_dim))\n",
    "\n",
    "        # perform eigendecomposition\n",
    "        v, _ = tf.linalg.qr(cov)\n",
    "        \n",
    "        if not hasattr(self, 'e'):\n",
    "            self.half_log_e, self.e, self.v = e, tf.exp(2 * e), tf.reshape(v, (-1, self.latent_dim * self.latent_dim))\n",
    "            \n",
    "        dim = self.latent_dim\n",
    "        \n",
    "        # get shapes\n",
    "        batch = K.shape(z_mean)[0]\n",
    "                \n",
    "        # sample from normal distribution\n",
    "        epsilon = K.random_normal(stddev=self.alpha, shape=(batch, K.int_shape(z_mean)[1]))\n",
    "        \n",
    "        # get sqrt covariance matrix stack\n",
    "        sqrt_sigma = tf.einsum('ijk,ilk->ijl', tf.einsum('ijk,ik->ijk', v, tf.sqrt(self.e)), v)\n",
    "        \n",
    "        # multiply covariance matrix stack with random normal vector\n",
    "        sqrt_sigma_epsilon = tf.einsum('ijk,ik->ij', sqrt_sigma, epsilon)\n",
    "        \n",
    "        if not hasattr(self, 'sqrt_sigma'):\n",
    "            self.sqrt_sigma = tf.reshape(sqrt_sigma, (-1, self.latent_dim * self.latent_dim))\n",
    "        \n",
    "        # assembled output\n",
    "        z = z_mean + sqrt_sigma_epsilon\n",
    "        \n",
    "        if not hasattr(self, 'z'):\n",
    "            self.z = z\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def predict_from_spectralnet(self, x):\n",
    "        get_fn = K.function([self.input], [self.x])\n",
    "        return predict_with_K_fn(get_fn, x)\n",
    "\n",
    "    def generate_from_samples(self, x, return_mu_sigma=False, normalize_cov=False):\n",
    "        _x_recon = self.x_recon_norm if normalize_cov else self.x_recon\n",
    "        get_fn = K.function([self.input], [_x_recon, self.z_mu, self.v, self.e, self.x_enc])\n",
    "        x_recon, z_mu, z_sigma_v, z_sigma_lam, _x_enc = predict_with_K_fn(get_fn, x)\n",
    "        if return_mu_sigma:\n",
    "            return x_recon, z_mu, z_sigma_v, z_sigma_lam, _x_enc\n",
    "        else:\n",
    "            return x_recon\n",
    "        \n",
    "    def train_pca(self, x_train, x_val=None, epochs=1, batch_size=128, patience=5):\n",
    "        if x_val is not None:\n",
    "            val_data = list((x_val, x_val))\n",
    "        else:\n",
    "            val_data = None\n",
    "        earlystop = EarlyStopping(monitor='val_loss', patience=patience, verbose=1, mode='auto')\n",
    "        \n",
    "        self.pcae.fit(x=x_train, y=x_train, epochs=epochs, batch_size=batch_size, validation_data=val_data, callbacks=[earlystop], verbose=2)\n",
    "        \n",
    "    def train(self, X_train, batch_size=128, epochs=100):      \n",
    "        self.vae_loss = []\n",
    "        last_cov = np.zeros((self.latent_dim, self.latent_dim))\n",
    "        cov_x = X_train[np.random.randint(0, X_train.shape[0], 1)]\n",
    "        for epoch in range(epochs):\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            samples = X_train[idx]\n",
    "            vae_loss = self.vae.train_on_batch([samples], [samples])\n",
    "            self.vae_loss.append(vae_loss)\n",
    "\n",
    "            if epoch % 25 == 0:\n",
    "                # Plot the progress\n",
    "                loss_names = ['kl_loss', 'pca_loss','neighborhood_loss']\n",
    "                loss_string = \"{} [VAE loss: {}] [\" + \": {}] [\".join(loss_names) + \": {}]\"\n",
    "                losses = self.init_losses\n",
    "                loss_vals = K.get_session().run(losses, feed_dict={self.input: samples})\n",
    "                print(loss_string.format(epoch, vae_loss, *loss_vals))\n",
    "                \n",
    "                # now get variance of the covariance vectors with respect to some fixed vector\n",
    "                cov, val = K.get_session().run([self.v, self.e], feed_dict={self.input: cov_x})\n",
    "                cov = cov.reshape((self.latent_dim, self.latent_dim))\n",
    "                print('vector covariance:\\n', cov.dot(last_cov.T))\n",
    "                print(cov.T.dot(last_cov))\n",
    "                print(val)\n",
    "                last_cov = cov\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dim = False\n",
    "pca = False\n",
    "# latent_dim += 1\n",
    "k = 2\n",
    "bsize = min(1024, len(x_train))\n",
    "\n",
    "# NOTE: currently the train and test sets are combined\n",
    "\n",
    "# split = int(len(x_train)*0.8)\n",
    "# x_train, x_val = x_train[:split], x_train[split:]\n",
    "# y_train, y_val = y_train[:split], y_train[split:]\n",
    "\n",
    "x_train = np.concatenate([x_train, x_test], axis=0)\n",
    "y_train = np.concatenate([y_train, y_test], axis=0)\n",
    "\n",
    "# plt.scatter(x_test[:,0], x_test[:,1], c=y_test)\n",
    "# g = plot(x_train, y_train)\n",
    "# g = plot(x_test, y_test)\n",
    "def plot_dots(x):\n",
    "    sns.set()\n",
    "    sns.despine()\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.scatter(x[:,0], x[:,1], s=100, alpha=.1)\n",
    "    major_ticks = np.arange(-2, 3, 1)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_xticks(major_ticks)\n",
    "    ax.set_yticks(major_ticks)\n",
    "    plt.xlim((-2.5208333333, 2.5208333333))\n",
    "    plt.ylim((-2.5208333333, 2.5208333333))\n",
    "    \n",
    "if dataset == 'gaussian_grid':\n",
    "    g = plot_dots(x_train)\n",
    "else:\n",
    "    g = plot(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pca:\n",
    "    n_clusters = latent_dim + 1\n",
    "else:\n",
    "    n_clusters = latent_dim\n",
    "\n",
    "batch_sizes = {\n",
    "    'Unlabeled': bsize,\n",
    "    'Labeled': bsize,\n",
    "    'Orthonorm': bsize,\n",
    "    }\n",
    "\n",
    "input_shape = x_train.shape[1:]\n",
    "y_true = tf.placeholder(tf.float32, shape=(None, n_clusters), name='y_true')\n",
    "y_train_labeled_onehot = np.empty((0, len(np.unique(y_train))))\n",
    "inputs = {\n",
    "    'Unlabeled': Input(shape=input_shape, name='UnlabeledInput'),\n",
    "    'Labeled': Input(shape=input_shape, name='LabeledInput'),\n",
    "    'Orthonorm': Input(shape=input_shape, name='OrthonormInput'),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "if dataset == 'mnist':\n",
    "    # first embed data\n",
    "    x_train_ = embed_data(x_all, 'mnist')\n",
    "    x_test_ = embed_data(x_all, 'mnist')\n",
    "\n",
    "    s_input_shape = x_train_.shape[1:]\n",
    "    s_inputs = {\n",
    "        'Unlabeled': Input(shape=s_input_shape, name='UnlabeledInput'),\n",
    "        'Labeled': Input(shape=s_input_shape, name='LabeledInput'),\n",
    "        'Orthonorm': Input(shape=s_input_shape, name='OrthonormInput'),\n",
    "        }\n",
    "    \n",
    "    pairs_train, dist_train = pairs.create_pairs_from_unlabeled_data(\n",
    "        x1=x_train_,\n",
    "        p=None,\n",
    "        k=2,\n",
    "        tot_pairs=600000,\n",
    "        precomputed_knn_path='',\n",
    "        use_approx=True,\n",
    "        pre_shuffled=False,\n",
    "    )\n",
    "    pairs_val, dist_val = pairs.create_pairs_from_unlabeled_data(\n",
    "        x1=x_test_,\n",
    "        p=None,\n",
    "        k=2,\n",
    "        tot_pairs=600000,\n",
    "        precomputed_knn_path='',\n",
    "        use_approx=True,\n",
    "        pre_shuffled=False,\n",
    "    )\n",
    "    siamese_net = SiameseNet(inputs=s_inputs, arch=arch, siam_reg=None, y_true=y_true)\n",
    "    siamese_net.train(pairs_train, dist_train, pairs_val, dist_val,\n",
    "            lr=1e-3, drop=0.1, patience=10, num_epochs=400, batch_size=1024)\n",
    "else:\n",
    "    siamese_net = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'mnist':\n",
    "    spectral_net = SpectralNet(inputs=s_inputs, arch=arch,\n",
    "            spec_reg=None, y_true=y_true, y_train_labeled_onehot=y_train_labeled_onehot,\n",
    "            n_clusters=n_clusters, affinity='siamese', scale_nbr=3, n_nbrs=2, \n",
    "            batch_sizes=batch_sizes, siamese_net=siamese_net, \n",
    "            x_train=x_train_, have_labeled=False)\n",
    "else:\n",
    "    spectral_net = SpectralNet(inputs=inputs, arch=arch,\n",
    "                spec_reg=None, y_true=y_true, y_train_labeled_onehot=y_train_labeled_onehot,\n",
    "                n_clusters=n_clusters, affinity='siamese', scale_nbr=3, n_nbrs=2, \n",
    "                batch_sizes=batch_sizes, siamese_net=siamese_net, \n",
    "                x_train=x_train_, have_labeled=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if dataset == 'mnist':\n",
    "    spectral_net.train(\n",
    "            x_train_, np.zeros_like(x_train_[0:0]), x_test_,\n",
    "            lr=1e-3, drop=0.1, patience=20, num_epochs=500)\n",
    "else:\n",
    "    spectral_net.train(\n",
    "            x_train, np.zeros_like(x_train[0:0]), x_test,\n",
    "            lr=1e-3, drop=0.1, patience=20, num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if dataset == 'mnist':\n",
    "    y_pred = spectral_net.predict(x_train_)\n",
    "    g = plot(y_pred[:,:3], y_train.reshape(-1,))\n",
    "else:\n",
    "    y_pred = spectral_net.predict(x_train)\n",
    "    g = plot(y_pred[:,:3], y_train.reshape(-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'mnist' in dataset or 'cifar' in dataset:\n",
    "    from core.util import print_accuracy, get_cluster_sols, LearningHandler, make_layer_list, get_y_preds\n",
    "    from sklearn.cluster import KMeans\n",
    "    true_clusters = 10\n",
    "    # get accuracy and nmi\n",
    "    kmeans_assignments, km = get_cluster_sols(y_pred, ClusterClass=KMeans, n_clusters=true_clusters, init_args={'n_init':10})\n",
    "    y_spectralnet, _ = get_y_preds(kmeans_assignments, y_train, true_clusters)\n",
    "    print_accuracy(kmeans_assignments, y_train, true_clusters)\n",
    "    from sklearn.metrics import normalized_mutual_info_score as nmi\n",
    "    nmi_score = nmi(kmeans_assignments, y_train)\n",
    "    print('NMI: ' + str(np.round(nmi_score, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOW EMBEDDINGS WITH IM_SCATTER\n",
    "# _ = imscatter(v[:,1], v[:,2], x_all)\n",
    "if 'mnist' in dataset or 'cifar' in dataset:\n",
    "    if 'mnist' in dataset:\n",
    "        img_shape = (28, 28)\n",
    "    elif 'cifar' in dataset:\n",
    "        img_shape = (32, 32, 3)\n",
    "    %matplotlib inline\n",
    "    _ = imscatter(y_pred[:,1], y_pred[:,3], x_test, shape=img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now plot all the dimensions of spectralnet\n",
    "y_pred_embedded = TSNE().fit_transform(y_pred)\n",
    "g = plot(y_pred_embedded[:,:2], y=y_test)\n",
    "\n",
    "if 'mnist' in dataset or 'cifar' in dataset:\n",
    "    %matplotlib inline\n",
    "    if 'mnist' in dataset:\n",
    "        img_shape = (28, 28)\n",
    "    elif 'cifar' in dataset:\n",
    "        img_shape = (32, 32, 3)\n",
    "    _ = imscatter(y_pred_embedded[:,0], y_pred_embedded[:,1], x_test, shape=img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# divide k by the difference in batch size\n",
    "svg_bsize = 1024\n",
    "svg_k = 1\n",
    "# svg_k = int(latent_dim / 2)\n",
    "conv_decoder = False\n",
    "svg = SVG(inputs, spectralnet=spectral_net, orig_dim=x_train.shape[1:], remove_dim=remove_dim, pca=pca, k=svg_k, alpha=1., arch=[x['size'] for x in arch], conv_decoder=conv_decoder, siamesenet=siamese_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = '../pretrain_weights/ae_{}.json'.format('mnist')\n",
    "weights_path = '../pretrain_weights/ae_{}_weights.h5'.format('mnist')\n",
    "\n",
    "with open(json_path) as f:\n",
    "    pt_ae = model_from_json(f.read())\n",
    "pt_ae.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.permutation(len(x_test))[:100]\n",
    "y_pred_svg = tf_get([svg.input], [svg.x], x_test[p])[0]\n",
    "y_pred_svg_embedded = TSNE().fit_transform(y_pred_svg)\n",
    "\n",
    "if 'mnist' in dataset:\n",
    "    %matplotlib inline\n",
    "    _ = imscatter(y_pred_svg_embedded[:,0], y_pred_svg_embedded[:,1], x_test[p], shape=(28, 28))\n",
    "    \n",
    "    y_pred_svg = tf_get([svg.input], [svg.x], x_train)[0]\n",
    "    \n",
    "    true_clusters = 10\n",
    "    # get accuracy and nmi\n",
    "    kmeans_assignments, km = get_cluster_sols(y_pred_svg, ClusterClass=KMeans, n_clusters=true_clusters, init_args={'n_init':10})\n",
    "    y_spectralnet, _ = get_y_preds(kmeans_assignments, y_train, true_clusters)\n",
    "    print_accuracy(kmeans_assignments, y_train, true_clusters)\n",
    "    from sklearn.metrics import normalized_mutual_info_score as nmi\n",
    "    nmi_score = nmi(kmeans_assignments, y_train)\n",
    "    print('NMI: ' + str(np.round(nmi_score, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = spectral_net.predict(x_test[p])\n",
    "y_pred_embedded = TSNE().fit_transform(y_pred)\n",
    "\n",
    "if 'mnist' in dataset:\n",
    "    %matplotlib inline\n",
    "    _ = imscatter(y_pred_embedded[:,0], y_pred_embedded[:,1], x_test[p], shape=(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pca:\n",
    "    svg.train_pca(x_train, epochs=600)\n",
    "    svg.pca_layers[0].trainable = False\n",
    "    svg.pca_layers[1].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pca:\n",
    "    y_pred = svg.pc.predict(x_test)\n",
    "    plt.axis('equal')\n",
    "    g = plot(y_pred[:,:3], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tf_get([svg.input], [svg.loss], x_test[:10])\n",
    "# tf_get([svg.input], [svg.decoder_xs[-2]], x_test[:10])[0].shape\n",
    "svg.train(x_train, epochs=5000, batch_size=svg_bsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick subset size\n",
    "n_p = min(1000, len(x_test))\n",
    "p = np.random.permutation(len(x_test))[:n_p]\n",
    "x_test_p = x_test[p]\n",
    "y_test_p = y_test[p]\n",
    "\n",
    "# plot generated points\n",
    "x_gen = svg.generate_from_samples(x_train[:10000], normalize_cov=True)\n",
    "g = plot(x_gen, y_train, x2=x_train, s2=0)\n",
    "p_train = np.random.permutation(len(x_train))[:n_p]\n",
    "# if dataset == 'gaussian_grid':\n",
    "#     g = plot_dots(x_gen)\n",
    "# else:\n",
    "#     g = plot(x_gen[p], y_train[p_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "for i, image in enumerate(x_gen):\n",
    "    imageio.imwrite('/projects/mnist_imgs/vdae_test/{:05d}.png'.format(i), np.array(image).reshape(28,28).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.randint(len(x_gen))\n",
    "plt.imshow(x_gen[p].reshape(100,80,3))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM WALK TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = partial(svg.generate_from_samples, return_mu_sigma=True) #, normalize_cov=0.5)\n",
    "y_test_sz = np.mean(f(x_all)[3], axis=1)\n",
    "sz_max = np.max(y_test_sz)\n",
    "sz_min = np.min(y_test_sz)\n",
    "y_test_sz = (y_test_sz - sz_min)/(sz_max - sz_min) * 5\n",
    "print(np.min(y_test_sz), np.max(y_test_sz))\n",
    "y_test_sz = np.exp(0.5 * y_test_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation\n",
    "\n",
    "# which space do we want to plot in?\n",
    "plot_latent = False\n",
    "plot_idx = 1 if plot_latent else 0\n",
    "\n",
    "x_arr = np.random.permutation(x_test)[:100]\n",
    "x__ = f(x_all)[plot_idx]\n",
    "x_ = f(x_arr)[plot_idx]\n",
    "x_tot = np.concatenate([x_, x__], axis=0)\n",
    "y_tot = np.concatenate([np.zeros(shape=(len(x_arr),)), np.ones(shape=(len(x__),))*2], axis=0)\n",
    "y_sz = np.concatenate([np.ones(shape=(len(x_arr),))*5, y_test_sz], axis=0)\n",
    "\n",
    "def update_graph(num):\n",
    "    global x_arr\n",
    "    global x__\n",
    "    global y_tot\n",
    "    x_arr, z_mu, z_sigma_v, z_sigma_lam, _x_enc = f(x_arr)\n",
    "    # plot in latent or original space\n",
    "    x_ = z_mu if plot_latent else x_arr\n",
    "    \n",
    "    x_ = np.concatenate([x_, x__], axis=0)\n",
    "    \n",
    "    if x_.shape[1] == 3:\n",
    "        graph._offsets3d = (x_[:,0], x_[:,1], x_[:,2])\n",
    "        ax.view_init(elev=10, azim=num*4)\n",
    "    elif x_.shape[1] == 2:\n",
    "        graph.set_offsets(np.c_[x_[:,0], x_[:,1]])\n",
    "        \n",
    "    title.set_text('Walk, time={}'.format(num))\n",
    "\n",
    "fig = plt.figure(figsize=(12.8, 7.2))\n",
    "projection = '3d' if x_.shape[1] == 3 else None\n",
    "ax = fig.add_subplot(111, projection=projection)\n",
    "title = ax.set_title('Walk, time=0')\n",
    "\n",
    "if x_.shape[1] == 3:\n",
    "    graph = ax.scatter(x_tot[:,0], x_tot[:,1], x_tot[:,2], c=y_tot, s=y_sz, alpha=.1)\n",
    "elif x_.shape[1] == 2:\n",
    "    graph = ax.scatter(x_tot[:,0], x_tot[:,1], c=y_tot, s=y_sz, alpha=0.4)\n",
    "\n",
    "ani = matplotlib.animation.FuncAnimation(fig, update_graph, 180, \n",
    "                               interval=200, blit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "print(\"saving animation\")\n",
    "Writer = animation.writers['ffmpeg']\n",
    "writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n",
    "print(\"...\")\n",
    "ani.save('im_{}_inv.mp4'.format(dataset), writer=writer)\n",
    "print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = partial(svg.generate_from_samples, return_mu_sigma=True)#, normalize_cov=0.)\n",
    "def walk(f, x_arr, branch_factor=20, n_steps=10, max_size=5000):\n",
    "    p = np.random.permutation(len(x_arr))[:1000]\n",
    "    x_arr = x_arr[p]\n",
    "    orig_shape = (-1,) + (x_arr.shape[1:])\n",
    "    for i in range(n_steps):\n",
    "        x_arr = np.array([x_arr] * branch_factor).reshape(orig_shape)\n",
    "        (x_arr, z_mu, z_sigma_v, z_sigma_lam, _x_enc) = f(x_arr)\n",
    "        p = np.random.permutation(len(x_arr))[:1000]\n",
    "        x_arr, z_mu, z_sigma_v, z_sigma_lam = x_arr[p], z_mu[p], z_sigma_v[p], z_sigma_lam[p]\n",
    "        x_arr = x_arr.reshape(orig_shape)\n",
    "    \n",
    "    p = np.random.permutation(len(x_arr))[:1000]\n",
    "    x_arr = x_arr[p]\n",
    "        \n",
    "    return x_arr, z_mu, z_sigma_v, z_sigma_lam, _x_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.permutation(len(x_test))[:1]\n",
    "x_test_sample = x_test[p]\n",
    "print('random walking on {}'.format(y_test[p]))\n",
    "# x_test_sample = x_train[y_train == 5]\n",
    "x_arr, z_mu, z_sigma_v, z_sigma_lam, _x_enc = walk(f, x_test_sample, n_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.permutation(len(x_arr))\n",
    "x_true_n_gen = np.concatenate([x_test_sample, x_arr[p[:15]]], axis=0)\n",
    "g = plot(x_true_n_gen, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.permutation(len(p))[:1000]\n",
    "g, ax = plot(x_arr, x2=x_test[p], label1='predicted', label2='true', alpha2=0.2)\n",
    "# ax.scatter(x_test_sample[:,0], x_test_sample[:,1], x_test_sample[:,2], s=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = plot(z_mu, x2=f(x_test_sample)[1])\n",
    "g = plot(z_mu, x2=f(x_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plot(x_arr, x2=x_test_sample, s2=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE BILIP CONSTANT\n",
    "import annoy\n",
    "# for each point in x, determine neighborhood in x, and then compute upper and lower constants\n",
    "def bilip(x, y, k, eps=1e-7):\n",
    "    # find nearest neighbors\n",
    "    a = annoy.AnnoyIndex(x.shape[1], 'euclidean')\n",
    "    for i, x_ in enumerate(x):\n",
    "        a.add_item(i, x_)\n",
    "        \n",
    "    # build nn tree\n",
    "    a.build(-1)\n",
    "    \n",
    "    # compute K\n",
    "    maxs = []\n",
    "    for i in range(len(x)):\n",
    "        x_neighbs, dists = a.get_nns_by_item(i, k+1, include_distances=True)\n",
    "        x_neighbs, dists = x_neighbs[1:], dists[1:]\n",
    "        ratio1 = [np.linalg.norm(x[j] - x[i])/np.linalg.norm(y[j] - y[i]) for j in x_neighbs]\n",
    "        ratio2 = [np.linalg.norm(y[j] - y[i])/np.linalg.norm(x[j] - x[i]) for j in x_neighbs]\n",
    "        max1, max2 = np.max(ratio1), np.max(ratio2)\n",
    "        max_ = max(max1, max2) + eps\n",
    "#         print(ratio1 <= max_, ratio1, max_)\n",
    "#         print(ratio2 >= 1/max_, ratio2, 1/max_)\n",
    "        assert np.all(ratio1 <= max_)\n",
    "        assert np.all(ratio2 >= 1/max_)\n",
    "        maxs.append(max_)\n",
    "        \n",
    "    return maxs\n",
    "\n",
    "# p = np.random.permutation(len(x_train))[:100]\n",
    "x_recon, z_mu, z_sigma_v, z_sigma_lam, _x_enc = svg.generate_from_samples(x_train, return_mu_sigma=True)\n",
    "Ks = bilip(_x_enc, x_recon, 100)\n",
    "print(np.mean(Ks), np.std(Ks), np.max(Ks), np.min(Ks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plot(x_recon, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
