{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join('..')))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join('../core')))\n",
    "import numpy as np\n",
    "import pickle\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Lambda, Subtract, Dense\n",
    "from keras.layers.merge import _Merge\n",
    "from keras.activations import relu\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse\n",
    "\n",
    "import train\n",
    "import costs\n",
    "from data import predict_with_K_fn\n",
    "from layer import stack_layers\n",
    "from util import LearningHandler, make_layer_list, train_gen, get_scale\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET AND USEFUL FUNCTION DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_get(input_tensors, output_tensors, input_data):\n",
    "    input_data = input_data if isinstance(input_data, list) else [input_data]\n",
    "    input_tensors, output_tensors = list(input_tensors), list(output_tensors)\n",
    "    sess = K.get_session()\n",
    "    return sess.run(output_tensors, dict(zip(input_tensors, input_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_K_fn(K_fn, x, bs=1000):\n",
    "    '''\n",
    "    Convenience function: evaluates x by K_fn(x), where K_fn is\n",
    "    a Keras function, by batches of size 1000.\n",
    "    '''\n",
    "    if not isinstance(x, list):\n",
    "        x = [x]\n",
    "    num_outs = len(K_fn.outputs)\n",
    "    shapes = [list(output_.get_shape()) for output_ in K_fn.outputs]\n",
    "    shapes = [[len(x[0])] + s[1:] for s in shapes]\n",
    "    y = [np.empty(s) for s in shapes]\n",
    "    recon_means = []\n",
    "    for i in range(int((x[0].shape[0]-1)/bs + 1)):\n",
    "        x_batch = []\n",
    "        for x_ in x:\n",
    "            x_batch.append(x_[i*bs:(i+1)*bs])\n",
    "        temp = K_fn(x_batch)\n",
    "        for j in range(num_outs):\n",
    "            y[j][i*bs:(i+1)*bs] = temp[j]\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "from matplotlib.colors import ListedColormap\n",
    "# cmap1 = ListedColormap(sns.color_palette().as_hex())\n",
    "# cmap2 = ListedColormap(sns.color_palette('bright').as_hex())\n",
    "def plot(x, y=None, x2=None, y2=None, s=10, s2=None, alpha=0.5, alpha2=None, label1=None, label2=None, cmap1=None, cmap2=None):\n",
    "    s2 = s if s2 is None else s2\n",
    "    alpha2 = alpha if alpha2 is None else alpha2\n",
    "    n = x.shape[1]\n",
    "    if n == 1:\n",
    "        g = plt.figure()\n",
    "        plt.scatter(np.zeros((n,)), x[:,1], c=y, s=s, alpha=alpha, label=label1, cmap=cmap1)\n",
    "        if x2 is not None:\n",
    "            plt.scatter(np.zeros((n,)), x2[:,1], c=y2, s=s2, alpha=alpha2, label=label2, cmap=cmap2)\n",
    "    if n == 3:\n",
    "        %matplotlib notebook\n",
    "        g = plt.figure()\n",
    "        ax = g.add_subplot(111, projection='3d')\n",
    "        ax.scatter(x[:,0], x[:,1], x[:,2], c=y, s=s, alpha=alpha, label=label1)\n",
    "        if x2 is not None:\n",
    "            ax.scatter(x2[:,0], x2[:,1], x2[:,2], c=y2, s=s2, alpha=alpha2, label=label2)\n",
    "    elif n == 784:\n",
    "        %matplotlib inline\n",
    "        n_imgs = 10\n",
    "        # num = 7\n",
    "        # sub = y == num\n",
    "        sub = y == y\n",
    "        for i in range(n_imgs):\n",
    "            idx = np.random.randint(len(x[sub]))\n",
    "            if x2 is not None:\n",
    "                plt.subplot(1,2,1)\n",
    "                plt.imshow(x2[sub][idx].reshape(28, 28))\n",
    "            plt.subplot(1,2,2)\n",
    "            plt.imshow(x[sub][idx].reshape(28, 28))\n",
    "            g = plt.figure()\n",
    "    else:\n",
    "        g = plt.figure()\n",
    "        plt.scatter(x[:,0], x[:,1], c=y, s=s, alpha=alpha, label=label1, cmap=cmap1)\n",
    "        if x2 is not None:\n",
    "            plt.scatter(x2[:,0], x2[:,1], c=y2, s=s2, alpha=alpha2, label=label2, cmap=cmap2)\n",
    "            \n",
    "    if label1 is not None or label2 is not None:\n",
    "        plt.legend()\n",
    "        \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bunny(n=2000, train_set_fraction=.8):\n",
    "#     df = pd.read_csv('../../../bunny.csv')\n",
    "    # df.values[:,:3].shape\n",
    "#     x = df[['Bunny, x', 'Bunny, y', 'Bunny, z']].dropna().values\n",
    "    import bunny\n",
    "    a = [np.expand_dims(np.array(bunny.trace2[c]), axis=-1) for c in ['x', 'y', 'z']]\n",
    "    x = np.concatenate(a, axis=-1)\n",
    "    x = x.astype(np.float32)\n",
    "    x = x[np.logical_not(np.any(np.isnan(x), axis=1))]\n",
    "    y = np.arange(len(x))\n",
    "    \n",
    "    # shuffle\n",
    "    p = np.random.permutation(len(x))[:n]\n",
    "    y = y[p]\n",
    "    x = x[p]\n",
    "    \n",
    "    # make train and test splits\n",
    "    n_train = int(n * train_set_fraction)\n",
    "    x_train, x_test = x[:n_train], x[n_train:]\n",
    "    y_train, y_test = y[:n_train].flatten(), y[n_train:].flatten()\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def generate_sphere(n=1200, train_set_fraction=.8):\n",
    "    r = 1\n",
    "    alpha = 4.0*np.pi*r*r/(n+1)\n",
    "    d = np.sqrt(alpha)\n",
    "    m_nu = int(np.round(np.pi/d))\n",
    "    d_nu = np.pi/m_nu\n",
    "    d_phi = alpha/d_nu\n",
    "    count = 0\n",
    "    coords = [[], [], []]\n",
    "    y = []\n",
    "    for i in range(0, m_nu):\n",
    "        nu = np.pi*(i+0.5)/m_nu\n",
    "        m_phi = int(np.round(2*np.pi*np.sin(nu)/d_phi))\n",
    "        for j in range(0, m_phi):\n",
    "            phi = 2*np.pi*j/m_phi\n",
    "            xp = r*np.sin(nu)*np.cos(phi)\n",
    "            yp = r*np.sin(nu)*np.sin(phi)\n",
    "            zp = r*np.cos(nu)\n",
    "            coords[0].append(xp)\n",
    "            coords[1].append(yp)\n",
    "            coords[2].append(zp)\n",
    "            y.append(i + j)\n",
    "            count = count +1\n",
    "            \n",
    "    x = np.array(coords).T\n",
    "    y = np.array(y).T\n",
    "        \n",
    "    # shuffle\n",
    "    p = np.random.permutation(n)\n",
    "    y = y[p]\n",
    "    x = x[p]\n",
    "    \n",
    "    # make train and test splits\n",
    "    n_train = int(n * train_set_fraction)\n",
    "    x_train, x_test = x[:n_train], x[n_train:]\n",
    "    y_train, y_test = y[:n_train].flatten(), y[n_train:].flatten()\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def generate_plane(n=1200, train_set_fraction=.8):\n",
    "    # compute number of points in each dimension\n",
    "    n_i = np.int(np.sqrt(n))\n",
    "    n = n_i ** 2\n",
    "    \n",
    "    # compute points on this grid\n",
    "    t = np.mgrid[0:1:1/n_i, 0:1:1/n_i].reshape(2,-1).T\n",
    "    t = np.concatenate([t, np.zeros(shape=(len(t),1))], axis=1)\n",
    "    \n",
    "    # compute rotation\n",
    "    A = np.random.normal(size=(3, 3))\n",
    "    A, _ = np.linalg.qr(A)\n",
    "    \n",
    "    x = np.dot(A, t.T).T\n",
    "    \n",
    "    # y is the sum of the ts\n",
    "    y = t[:,0] + t[:,1]\n",
    "    \n",
    "    # shuffle\n",
    "    p = np.random.permutation(n)\n",
    "    y = y[p]\n",
    "    x = x[p]\n",
    "\n",
    "    # make train and test splits\n",
    "    n_train = int(n * train_set_fraction)\n",
    "    print(n_train)\n",
    "    x_train, x_test = x[:n_train], x[n_train:]\n",
    "    y_train, y_test = y[:n_train].flatten(), y[n_train:].flatten()\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def generate_loop(n=1200, train_set_fraction=.8):\n",
    "    t = np.linspace(0, 2*np.pi, num=n)\n",
    "    \n",
    "    # generate all three coordinates\n",
    "    x = np.empty((n, 3))\n",
    "    x[:,0] = np.cos(t)\n",
    "    x[:,1] = np.sin(2*t)\n",
    "    x[:,2] = np.sin(3*t)\n",
    "    \n",
    "    # y is just t\n",
    "    y = t\n",
    "    \n",
    "    # shuffle\n",
    "    p = np.random.permutation(n)\n",
    "    y = y[p]\n",
    "    x = x[p]\n",
    "\n",
    "    # make train and test splits\n",
    "    n_train = int(n * train_set_fraction)\n",
    "    x_train, x_test = x[:n_train], x[n_train:]\n",
    "    y_train, y_test = y[:n_train].flatten(), y[n_train:].flatten()\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def generate_circle(n=1000, train_set_fraction=.8, alpha=4):\n",
    "    t = np.linspace(0, 2*np.pi, num=n)\n",
    "#     t = np.log(np.linspace(1, alpha, num=n))\n",
    "    t = t / np.max(t) * 2 * np.pi\n",
    "    \n",
    "    # generate all three coordinates\n",
    "    x = np.empty((n, 2))\n",
    "    x[:,0] = np.cos(t)\n",
    "    x[:,1] = np.sin(t)\n",
    "    \n",
    "    # y is just t\n",
    "    y = t\n",
    "    \n",
    "    # shuffle\n",
    "    p = np.random.permutation(n)\n",
    "    y = y[p]\n",
    "    x = x[p]\n",
    "\n",
    "    # make train and test splits\n",
    "    n_train = int(n * train_set_fraction)\n",
    "    x_train, x_test = x[:n_train], x[n_train:]\n",
    "    y_train, y_test = y[:n_train].flatten(), y[n_train:].flatten()\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def generate_line(n=1200, train_set_fraction=.8):\n",
    "    pts_per_cluster = int(n / 2)\n",
    "    x1 = np.linspace(0, 1, num=n).reshape((-1, 1))\n",
    "    x2 = np.linspace(0, 1, num=n).reshape((-1, 1))\n",
    "    x = np.concatenate([x1, x2], axis=1)\n",
    "    \n",
    "    # generate labels\n",
    "#     y = np.concatenate((np.zeros(shape=(pts_per_cluster, 1)), np.ones(shape=(pts_per_cluster, 1))), axis=0)\n",
    "    y = x1\n",
    "    \n",
    "    # shuffle\n",
    "    p = np.random.permutation(n)\n",
    "    y = y[p]\n",
    "    x = x[p]\n",
    "\n",
    "    # make train and test splits\n",
    "    n_train = int(n * train_set_fraction)\n",
    "    x_train, x_test = x[:n_train], x[n_train:]\n",
    "    y_train, y_test = y[:n_train].flatten(), y[n_train:].flatten()\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def generate_gaussians(n=1200, n_clusters=2, noise_sigma=0.1, train_set_fraction=1.):\n",
    "    '''\n",
    "    Generates and returns the nested 'C' example dataset (as seen in the leftmost\n",
    "    graph in Fig. 1)\n",
    "    '''\n",
    "    pts_per_cluster = int(n / n_clusters)\n",
    "    r = 1\n",
    "    \n",
    "    clusters = []\n",
    "    \n",
    "    for x in np.linspace(0, 1, num=n_clusters):\n",
    "        clusters.append(np.random.normal(x, noise_sigma, size=(pts_per_cluster, 2)))\n",
    "\n",
    "    # combine clusters\n",
    "    x = np.concatenate(clusters, axis=0)\n",
    "    print(np.max(x), np.min(x))\n",
    "    x /= (np.max(x) - np.min(x))\n",
    "    print(np.max(x), np.min(x))\n",
    "    x -= np.min(x)\n",
    "    print(np.max(x), np.min(x))\n",
    "\n",
    "    # generate labels\n",
    "    y = np.concatenate((np.zeros(shape=(pts_per_cluster, 1)), np.ones(shape=(pts_per_cluster, 1))), axis=0)\n",
    "\n",
    "    # shuffle\n",
    "    p = np.random.permutation(n)\n",
    "    y = y[p]\n",
    "    x = x[p]\n",
    "\n",
    "    # make train and test splits\n",
    "    n_train = int(n * train_set_fraction)\n",
    "    x_train, x_test = x[:n_train], x[n_train:]\n",
    "    y_train, y_test = y[:n_train].flatten(), y[n_train:].flatten()\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def generate_cc(n=1200, noise_sigma=0.1, train_set_fraction=1.):\n",
    "    '''\n",
    "    Generates and returns the nested 'C' example dataset (as seen in the leftmost\n",
    "    graph in Fig. 1)\n",
    "    '''\n",
    "    pts_per_cluster = int(n / 2)\n",
    "    r = 1\n",
    "\n",
    "    # generate clusters\n",
    "    theta1 = (np.random.uniform(0, 1, pts_per_cluster) * r * np.pi - np.pi / 2).reshape(pts_per_cluster, 1)\n",
    "    theta2 = (np.random.uniform(0, 1, pts_per_cluster) * r * np.pi - np.pi / 2).reshape(pts_per_cluster, 1)\n",
    "\n",
    "    cluster1 = np.concatenate((np.cos(theta1) * r, np.sin(theta1) * r), axis=1)\n",
    "    cluster2 = np.concatenate((np.cos(theta2) * r, np.sin(theta2) * r), axis=1)\n",
    "\n",
    "    # shift and reverse cluster 2\n",
    "    cluster2[:, 0] = -cluster2[:, 0] + 0.5\n",
    "    cluster2[:, 1] = -cluster2[:, 1] - 1\n",
    "\n",
    "    # combine clusters\n",
    "    x = np.concatenate((cluster1, cluster2), axis=0)\n",
    "\n",
    "    # add noise to x\n",
    "    x = x + np.random.randn(x.shape[0], 2) * noise_sigma\n",
    "    print(np.max(x), np.min(x))\n",
    "    x /= (np.max(x) - np.min(x))\n",
    "    print(np.max(x), np.min(x))\n",
    "    x -= np.min(x)\n",
    "    print(np.max(x), np.min(x))\n",
    "\n",
    "    # generate labels\n",
    "    y = np.concatenate((np.zeros(shape=(pts_per_cluster, 1)), np.ones(shape=(pts_per_cluster, 1))), axis=0)\n",
    "\n",
    "    # shuffle\n",
    "    p = np.random.permutation(n)\n",
    "    y = y[p]\n",
    "    x = x[p]\n",
    "\n",
    "    # make train and test splits\n",
    "    n_train = int(n * train_set_fraction)\n",
    "    x_train, x_test = x[:n_train], x[n_train:]\n",
    "    y_train, y_test = y[:n_train].flatten(), y[n_train:].flatten()\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'loop'\n",
    "\n",
    "if dataset == 'mnist':\n",
    "#     (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "#     x_train, x_test = x_train.reshape((-1, 784)), x_test.reshape((-1, 784))\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    # reshape and standardize x arrays\n",
    "    x_train = x_train.reshape(len(x_train), -1) / 255\n",
    "    x_test = x_test.reshape(len(x_test), -1) / 255\n",
    "    latent_dim = 9\n",
    "elif dataset == 'gaussians':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_gaussians(n=2000, n_clusters=1, train_set_fraction=0.85)\n",
    "    latent_dim = 6\n",
    "elif dataset == 'line':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_line(n=2000, train_set_fraction=0.85)\n",
    "    latent_dim = 2\n",
    "elif dataset == 'loop':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_loop(n=5000, train_set_fraction=0.85)\n",
    "    latent_dim = 2\n",
    "elif dataset == 'cc':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_cc(n=2000, noise_sigma=0.1, train_set_fraction=0.85)\n",
    "    latent_dim = 3\n",
    "elif dataset == 'circle':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_circle(n=1024, train_set_fraction=0.85, alpha=30)\n",
    "    latent_dim = 2\n",
    "elif dataset == 'plane':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_plane()\n",
    "    latent_dim = 3\n",
    "elif dataset == 'sphere':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_sphere(n=2000)\n",
    "    latent_dim = 3\n",
    "elif dataset == 'bunny':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_bunny(n=20000)\n",
    "    latent_dim = 3\n",
    "\n",
    "x_all = np.concatenate([x_train, x_test], axis=0)\n",
    "    \n",
    "# normalize to between -1 and 1\n",
    "if dataset != 'mnist':\n",
    "    m, M = np.min(x_train), np.max(x_train)\n",
    "    a = (M + m) / 2\n",
    "    b = (M - m) / 2\n",
    "    x_train, x_test = (x_train - a) / b, (x_test - a) / b\n",
    "print('IMPORTANT: max {}, min {}'.format(np.max(x_train), np.min(x_train)))\n",
    "\n",
    "arch = [\n",
    "    {'type': 'relu', 'size': 128},\n",
    "    {'type': 'relu', 'size': 128},\n",
    "    {'type': 'relu', 'size': 512},\n",
    "    {'type': 'linear', 'size': 16},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralNet:\n",
    "    def __init__(self, inputs, arch, spec_reg, y_true, y_train_labeled_onehot,\n",
    "            n_clusters, affinity, scale_nbr, n_nbrs, batch_sizes, normalized=False,\n",
    "            siamese_net=None, x_train=None, have_labeled=False):\n",
    "        self.y_true = y_true\n",
    "        self.y_train_labeled_onehot = y_train_labeled_onehot\n",
    "        self.inputs = inputs\n",
    "        self.batch_sizes = batch_sizes\n",
    "        self.normalized = normalized\n",
    "        # generate layers\n",
    "        self.layers = make_layer_list(arch[:-1], 'spectral', spec_reg)\n",
    "        self.layers += [\n",
    "                  {'type': 'tanh',\n",
    "                   'size': n_clusters,\n",
    "                   'l2_reg': spec_reg,\n",
    "                   'name': 'spectral_{}'.format(len(arch)-1)},\n",
    "                  {'type': 'Orthonorm', 'name':'orthonorm'}\n",
    "                  ]\n",
    "\n",
    "        # create spectralnet\n",
    "        self.outputs = stack_layers(self.inputs, self.layers)\n",
    "        self.net = Model(inputs=self.inputs['Unlabeled'], outputs=self.outputs['Unlabeled'])\n",
    "\n",
    "        # DEFINE LOSS\n",
    "\n",
    "        # generate affinity matrix W according to params\n",
    "        if affinity == 'siamese':\n",
    "            input_affinity = tf.concat([siamese_net.outputs['A'], siamese_net.outputs['Labeled']], axis=0)\n",
    "            x_affinity = siamese_net.predict(x_train, batch_sizes)\n",
    "        elif affinity in ['knn', 'full']:\n",
    "            input_affinity = tf.concat([self.inputs['Unlabeled'], self.inputs['Labeled']], axis=0)\n",
    "            x_affinity = x_train\n",
    "\n",
    "        # calculate scale for affinity matrix\n",
    "        scale = get_scale(x_affinity, self.batch_sizes['Unlabeled'], scale_nbr)\n",
    "\n",
    "        # create affinity matrix\n",
    "        if affinity == 'full':\n",
    "            W = costs.full_affinity(input_affinity, scale=scale)\n",
    "        elif affinity in ['knn', 'siamese']:\n",
    "            W = costs.knn_affinity(input_affinity, n_nbrs, scale=scale, scale_nbr=scale_nbr)\n",
    "\n",
    "        # if we have labels, use them\n",
    "        if have_labeled:\n",
    "            # get true affinities (from labeled data)\n",
    "            W_true = tf.cast(tf.equal(costs.squared_distance(y_true), 0),dtype='float32')\n",
    "\n",
    "            # replace lower right corner of W with W_true\n",
    "            unlabeled_end = tf.shape(self.inputs['Unlabeled'])[0]\n",
    "            W_u = W[:unlabeled_end, :]                  # upper half\n",
    "            W_ll = W[unlabeled_end:, :unlabeled_end]    # lower left\n",
    "            W_l = tf.concat((W_ll, W_true), axis=1)      # lower half\n",
    "            W = tf.concat((W_u, W_l), axis=0)\n",
    "\n",
    "            # create pairwise batch distance matrix self.Dy\n",
    "            y_ = tf.concat([self.outputs['Unlabeled'], self.outputs['Labeled']], axis=0)\n",
    "        else:\n",
    "            y_ = self.outputs['Unlabeled']\n",
    "            \n",
    "        if self.normalized:\n",
    "            y_old = y_\n",
    "            y_ = y_ / tf.expand_dims(tf.reduce_sum(W, axis=1), axis=-1)\n",
    "        \n",
    "        self.Dy = costs.squared_distance(y_)\n",
    "\n",
    "        # define loss\n",
    "        self.loss = K.sum(W * self.Dy) / (2 * batch_sizes['Unlabeled'])\n",
    "\n",
    "        # create the train step update\n",
    "        self.learning_rate = tf.Variable(0., name='spectral_net_learning_rate')\n",
    "        self.train_step = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate).minimize(self.loss, var_list=self.net.trainable_weights)\n",
    "#         self.train_step = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss, var_list=self.net.trainable_weights)\n",
    "        \n",
    "        # initialize spectralnet variables\n",
    "        K.get_session().run(tf.variables_initializer(self.net.trainable_weights))\n",
    "\n",
    "    def train(self, x_train_unlabeled, x_train_labeled, x_val_unlabeled,\n",
    "            lr, drop, patience, num_epochs):\n",
    "        # create handler for early stopping and learning rate scheduling\n",
    "        self.lh = LearningHandler(\n",
    "                lr=lr,\n",
    "                drop=drop,\n",
    "                lr_tensor=self.learning_rate,\n",
    "                patience=patience)\n",
    "\n",
    "        losses = np.empty((num_epochs,))\n",
    "        val_losses = np.empty((num_epochs,))\n",
    "\n",
    "        # begin spectralnet training loop\n",
    "        self.lh.on_train_begin()\n",
    "        i = 0\n",
    "        for i in range(num_epochs):\n",
    "            # train spectralnet\n",
    "            losses[i] = train.train_step(\n",
    "                    return_var=[self.loss],\n",
    "                    updates=self.net.updates + [self.train_step],\n",
    "                    x_unlabeled=x_train_unlabeled,\n",
    "                    inputs=self.inputs,\n",
    "                    y_true=self.y_true,\n",
    "                    batch_sizes=self.batch_sizes,\n",
    "                    x_labeled=x_train_labeled,\n",
    "                    y_labeled=self.y_train_labeled_onehot,\n",
    "                    batches_per_epoch=100)[0]\n",
    "\n",
    "            # get validation loss\n",
    "            val_losses[i] = train.predict_sum(\n",
    "                    self.loss,\n",
    "                    x_unlabeled=x_val_unlabeled,\n",
    "                    inputs=self.inputs,\n",
    "                    y_true=self.y_true,\n",
    "                    x_labeled=x_train_unlabeled[0:0],\n",
    "                    y_labeled=self.y_train_labeled_onehot,\n",
    "                    batch_sizes=self.batch_sizes)\n",
    "\n",
    "            # do early stopping if necessary\n",
    "            if self.lh.on_epoch_end(i, val_losses[i]):\n",
    "                print('STOPPING EARLY')\n",
    "                break\n",
    "\n",
    "            # print training status\n",
    "            print(\"Epoch: {}, loss={:2f}, val_loss={:2f}\".format(i, losses[i], val_losses[i]))\n",
    "\n",
    "        return losses[:i+1], val_losses[:i+1]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # test inputs do not require the 'Labeled' input\n",
    "        inputs_test = {'Unlabeled': self.inputs['Unlabeled'], 'Orthonorm': self.inputs['Orthonorm']}\n",
    "        return train.predict(\n",
    "                    self.outputs['Unlabeled'],\n",
    "                    x_unlabeled=x,\n",
    "                    inputs=inputs_test,\n",
    "                    y_true=self.y_true,\n",
    "                    x_labeled=x[0:0],\n",
    "                    y_labeled=self.y_train_labeled_onehot[0:0],\n",
    "                    batch_sizes=self.batch_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVG:\n",
    "    def __init__(self, inputs, spectralnet, orig_dim, remove_dim=False, pca=True, alpha=0.1, normalize_factor=.1, k=16, eps=1e-6):\n",
    "        optimizer = 'adam'\n",
    "#         optimizer = RMSprop(lr=0.00005)\n",
    "        self.input = inputs['Unlabeled']\n",
    "        self.orig_dim = orig_dim\n",
    "        self.eps = eps\n",
    "        self.pca = pca\n",
    "        self.alpha = float(alpha)\n",
    "        self.k = k\n",
    "        \n",
    "        x = self.copy_spectralnet(spectralnet)\n",
    "        \n",
    "        #\n",
    "        # DEFINE ALL LOSSES\n",
    "        #\n",
    "        def pairwise_loss(D, k):\n",
    "            # get nearest (i.e., largest negative distance) neighbors of each point\n",
    "            vals, _ = tf.nn.top_k(-D, k=k)\n",
    "\n",
    "            # remove self as neighbor, negate to get positive distances again\n",
    "            vals = -vals[:, 1:]\n",
    "\n",
    "            # pick sigma\n",
    "            # sigma = tf.reduce_max(vals[:, 0])\n",
    "            sigma = vals[:, :1] + self.eps\n",
    "            sq_vals = vals ** 2\n",
    "            \n",
    "            #loss = sq_vals #* tf.exp(-sq_vals / sigma)\n",
    "            loss = vals\n",
    "            return K.sum(loss)\n",
    "        def mu_loss(_, __):\n",
    "            self.mu_loss = K.sum(mse(self.input, self.mu_recon)) * self.orig_dim\n",
    "            return self.mu_loss\n",
    "        def kl_loss(_, __):\n",
    "            e_log = K.log(self.z_cov_values + self.eps)\n",
    "            kl_loss = -1 + K.log(self.alpha) - e_log + self.z_cov_values/self.alpha\n",
    "            self.kl_loss = K.sum(kl_loss) * 0.5\n",
    "            return self.kl_loss\n",
    "        def pca_loss(_, __):\n",
    "            if self.pca:\n",
    "                self.pca_loss = K.sum(mse(self.pca_input, self.pca_recon)) * self.orig_dim\n",
    "            else:\n",
    "                self.pca_loss = tf.constant(0.)\n",
    "            return self.pca_loss\n",
    "        def neighbor_loss(_, __):\n",
    "            # obtain pairwise distances (size(recon) x size(input))\n",
    "            D = self.pairwise_distances(self.x_enc, self.mu)\n",
    "            self.neighbor_loss = pairwise_loss(D, self.k)\n",
    "            return self.neighbor_loss\n",
    "        def manifold_loss(_, __):\n",
    "            # involves two bursts, on-manifold burst and off-manifold burst\n",
    "            x_m = self.x_m\n",
    "            x_t = self.x_t_recon\n",
    "            \n",
    "            # compute on manifold loss first; obtain pairwise distances (size(recon) x size(input))\n",
    "            D = self.pairwise_distances(x_m, self.mu)\n",
    "            self.on_manifold_loss = pairwise_loss(D, self.k)\n",
    "            \n",
    "            # now compute off manifold loss\n",
    "            self.off_manifold_loss = K.sum(mse(self.input, x_t)) * self.orig_dim\n",
    "            \n",
    "            return self.on_manifold_loss + self.off_manifold_loss\n",
    "        # currently unused\n",
    "        def reconstruction_loss(_, __):\n",
    "            self.reconstruction_loss = K.sum(mse(self.input, self.x_recon)) * self.orig_dim\n",
    "            return self.reconstruction_loss\n",
    "        def vae_loss(_, __):\n",
    "            return self.loss\n",
    "        \n",
    "        #\n",
    "        # DEFINE LAYERS\n",
    "        #\n",
    "\n",
    "        # create encoder\n",
    "        self.x_enc = x_enc = self.build_encoder(x, remove_dim=remove_dim, pca=self.pca)\n",
    "        self.encoder = Model(inputs=self.input, outputs=x_enc)\n",
    "\n",
    "        # create decoder\n",
    "        self.x_recon = x_recon = self.build_decoder(x_enc)\n",
    "        self.decoder = Model(inputs=self.input, outputs=x_recon)\n",
    "        \n",
    "        # reconstruct other entities\n",
    "        self.mu_recon = self.build_decoder(self.build_encoder(x, no_noise=True))\n",
    "        self.x_t_recon = self.build_decoder(self.x_t)\n",
    "        \n",
    "        # create normalized decoder\n",
    "        x_enc_norm = self.build_encoder(x, normalize_cov=normalize_factor)\n",
    "        self.x_recon_norm = self.build_decoder(x_enc_norm)\n",
    "        \n",
    "        if self.pca:\n",
    "            self.pcae = Model(inputs=self.input, outputs=self.pca_recon)\n",
    "            self.pc = Model(inputs=self.input, outputs=self.pc_embedding)\n",
    "            self.pcae.compile(optimizer=optimizer, loss=pca_loss)\n",
    "            \n",
    "        #\n",
    "        # COMPUTE LOSS\n",
    "        #\n",
    "        losses = [reconstruction_loss, mu_loss, kl_loss, pca_loss, neighbor_loss, manifold_loss]\n",
    "        self.init_losses = [l(None, None) for l in losses]\n",
    "        loss_weights = [0, 0, 1, 0, 0, 1]\n",
    "        # initialize losses\n",
    "        self.loss = sum([a * b if b != 0 else K.constant(0.) for a, b in zip(self.init_losses, loss_weights)])\n",
    "        \n",
    "        #\n",
    "        # ASSEMBLE NETWORK\n",
    "        #\n",
    "        self.vae = Model(inputs=self.input, outputs=self.x_recon)\n",
    "        self.vae.compile(optimizer=optimizer, loss=vae_loss)\n",
    "        \n",
    "    def pairwise_distances(self, A, B):\n",
    "        r_A, r_B = tf.reduce_sum(A*A, 1), tf.reduce_sum(B*B, 1)\n",
    "\n",
    "        # turn r into column vector\n",
    "        r_A, r_B = tf.reshape(r_A, [-1, 1]), tf.reshape(r_B, [-1, 1])\n",
    "        D = r_A - 2 * tf.matmul(A, B, transpose_b=True) + tf.transpose(r_B)\n",
    "\n",
    "        return D\n",
    "        \n",
    "    def build_decoder(self, x, arch=[1024, 256, 256]):\n",
    "        if not hasattr(self, 'decoder_layers'):\n",
    "            self.decoder_layers = [Dense(a, activation='relu') for a in arch]\n",
    "            self.decoder_layers.append(Dense(self.orig_dim, activation='linear'))\n",
    "\n",
    "        for l in self.decoder_layers:\n",
    "            x = l(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def build_encoder(self, x, arch=[256, 256, 1024], pca=True, normalize_cov=False, no_noise=False, remove_dim=False):\n",
    "        if pca and not hasattr(self, 'pca_layers'):\n",
    "            self.pca_layers = [Dense(self.latent_dim, activation='linear'), Dense(self.spectralnet_dim, activation='linear')]\n",
    "            \n",
    "        if not hasattr(self, 'encoder_layers'):\n",
    "            self.encoder_precov_layers = [Dense(a, activation='relu') for a in arch]\n",
    "            self.encoder_precov_layers.append(Dense(self.latent_dim * self.latent_dim, activation='linear'))\n",
    "            self.encoder_eig_layers = [Dense(a, activation='relu') for a in arch]\n",
    "            # RELU because we're output exponentials\n",
    "            self.encoder_eig_layers.append(Dense(self.latent_dim, activation='relu'))\n",
    "            \n",
    "        # assemble pca layer (a linear autoencoder) and define mu (the latent embedding of this layer)\n",
    "        if pca:\n",
    "            if not hasattr(self, 'pca_input'):\n",
    "                self.pca_input = x\n",
    "\n",
    "            self.pc_embedding = x = self.pca_layers[0](x)\n",
    "\n",
    "            if not hasattr(self, 'pca_recon'):\n",
    "                self.pca_recon = self.pca_layers[1](x)\n",
    "\n",
    "        # define mu (the latent embedding of the pca layer)\n",
    "        mu = x\n",
    "        if not hasattr(self, 'mu'):\n",
    "            self.mu = mu\n",
    "        \n",
    "        x_precov = x\n",
    "        # get covariance precursor\n",
    "        for l in self.encoder_precov_layers:\n",
    "            x_precov = l(x_precov)\n",
    "            \n",
    "        x_eig = x\n",
    "        # get eigenvalues\n",
    "        for l in self.encoder_eig_layers:\n",
    "            x_eig = l(x_eig)\n",
    "        \n",
    "        # sample latent space (and normalize covariances if we're trying to do random walks)\n",
    "        if not hasattr(self, 'encoder_sampling_layer'):\n",
    "            f = partial(self.sampling, normalize_cov=normalize_cov, remove_dim=remove_dim)\n",
    "            self.encoder_sampling_layer = Lambda(f, output_shape=(self.latent_dim,), name='z')\n",
    "            \n",
    "        if no_noise:\n",
    "            cur_encoder_sampling_layer = Lambda(lambda x_: x_[0], output_shape=(self.latent_dim,))\n",
    "            \n",
    "        # get encoder embedding\n",
    "        x_enc = self.encoder_sampling_layer([mu, x_precov, x_eig])\n",
    "        \n",
    "        return x_enc\n",
    "        \n",
    "    def copy_spectralnet(self, spectralnet):\n",
    "        xs = [self.input]\n",
    "        layers = []\n",
    "        for l in spectralnet.net.layers[1:-1]:\n",
    "            w = l.get_weights()\n",
    "            n, m = w[0].shape\n",
    "            if hasattr(l, 'activation'):\n",
    "                act = l.activation\n",
    "            new_l = Dense(m, activation=act, input_shape=(n,), weights=w)\n",
    "            new_l.trainable = False\n",
    "            xs.append(new_l(xs[-1]))\n",
    "            layers.append(new_l)\n",
    "\n",
    "        pre_x = xs[-1]\n",
    "        # add orthonorm layer\n",
    "        sess = K.get_session()\n",
    "        with tf.variable_scope('', reuse=True):\n",
    "            v = tf.get_variable(\"ortho_weights_store\")\n",
    "        ows = sess.run(v)\n",
    "        t_ows = K.variable(ows)\n",
    "        l = Lambda(lambda x: K.dot(x, t_ows))\n",
    "        l.trainable = False\n",
    "        xs.append(l(xs[-1]))\n",
    "        layers.append(l)\n",
    "\n",
    "        x = xs[-1]\n",
    "\n",
    "        self.sn = Model(inputs=self.input, outputs=x)\n",
    "\n",
    "        self.spectralnet_dim = int(x.get_shape()[1])\n",
    "        if self.pca:\n",
    "            self.latent_dim = self.spectralnet_dim - 1\n",
    "        else:\n",
    "            self.latent_dim = self.spectralnet_dim\n",
    "            \n",
    "        return x\n",
    "\n",
    "    def sampling(self, args, normalize_cov, remove_dim=False):\n",
    "        # get args\n",
    "        z_mean, precov, e = args\n",
    "        \n",
    "        # reshape precov and compute cov = precov x precov.T\n",
    "#         precov = tf.reshape(precov, (-1, self.latent_dim, self.latent_dim))\n",
    "#         self.cov = cov = tf.einsum('ijk,ilk->ijl', precov, precov)\n",
    "        cov = tf.reshape(precov, (-1, self.latent_dim, self.latent_dim))\n",
    "\n",
    "        # perform eigendecomposition\n",
    "#         e, v = tf.linalg.eigh(cov)\n",
    "        v, _ = tf.linalg.qr(cov)\n",
    "#         e = tf.sort(e, axis=-1)\n",
    "\n",
    "        # eigenvectors/values are sorted in increasing order; let's reverse them\n",
    "#         e, v = e[:, ::-1], v[:, :, ::-1]\n",
    "        \n",
    "        if not hasattr(self, 'z_cov_vectors'):\n",
    "            self.z_cov_values, self.z_cov_vectors = e, tf.reshape(v, (-1, self.latent_dim * self.latent_dim))\n",
    "            \n",
    "        dim = self.latent_dim\n",
    "        # if remove_dim, split into two subspaces (to enforce disjoint manifold and off-manifold covariance matrices)\n",
    "        if remove_dim:\n",
    "            dim = self.latent_dim - 1\n",
    "            e_m, v_m = e[:, :dim], v[:, :, :dim]\n",
    "            e_t, v_t = e[:, dim:], v[:, :, dim:]\n",
    "        else:\n",
    "            raise NotImplementedError(\"Non- 'remove_dim' sampling for manifold_loss to be implemented.\")\n",
    "        \n",
    "        # get shapes\n",
    "        batch = K.shape(z_mean)[0]\n",
    "                \n",
    "        # sample from normal distribution\n",
    "        epsilon_m = K.random_normal(stddev=self.alpha, shape=(batch, K.int_shape(z_mean)[1]))\n",
    "        epsilon_t = K.random_normal(stddev=self.alpha, shape=(batch, K.int_shape(z_mean)[1]))\n",
    "        \n",
    "        # self.z_cov_vectors.shape = (n_batches, n_dim, n_dim); epsilon.shape = (n_batches, n_dim)\n",
    "        if normalize_cov:\n",
    "            e = e * normalize_cov\n",
    "        \n",
    "        # get sqrt covariance matrix stack\n",
    "        sqrt_var_m = tf.einsum('ijk,ilk->ijl', tf.einsum('ijk,ik->ijk', v_m, tf.sqrt(e_m + self.eps)), v_m)\n",
    "        sqrt_var_t = tf.einsum('ijk,ilk->ijl', tf.einsum('ijk,ik->ijk', v_t, tf.sqrt(e_t + self.eps)), v_t)\n",
    "        \n",
    "        # multiply covariance matrix stack with random normal vector\n",
    "        sqrt_var_epsilon_m = tf.einsum('ijk,ik->ij', sqrt_var_m, epsilon_m)\n",
    "        sqrt_var_epsilon_t = tf.einsum('ijk,ik->ij', sqrt_var_t, epsilon_m)\n",
    "        \n",
    "        if not hasattr(self, 'sqrt_var'):\n",
    "            self.sqrt_var = tf.reshape(sqrt_var_m, (-1, self.latent_dim * self.latent_dim))\n",
    "        \n",
    "        # assembled output\n",
    "        x_m = z_mean + sqrt_var_epsilon_m\n",
    "        x_t = z_mean + sqrt_var_epsilon_t\n",
    "        \n",
    "        if not hasattr(self, 'x_m'):\n",
    "            self.x_m, self.x_t = x_m, x_t\n",
    "        \n",
    "        return x_m\n",
    "\n",
    "    def generate_from_samples(self, x, return_mu_sigma=False, normalize_cov=False):\n",
    "        _x_recon = self.x_recon_norm if normalize_cov else self.x_recon\n",
    "        get_fn = K.function([self.input], [_x_recon, self.mu, self.z_cov_vectors, self.z_cov_values])\n",
    "        x_recon, x_mu, x_sigma_v, x_sigma_lam = predict_with_K_fn(get_fn, x)\n",
    "        if return_mu_sigma:\n",
    "            return x_recon, x_mu, x_sigma_v, x_sigma_lam\n",
    "        else:\n",
    "            return x_recon\n",
    "        \n",
    "    def train_pca(self, x_train, x_val=None, epochs=1, batch_size=128, patience=5):\n",
    "        if x_val is not None:\n",
    "            val_data = list((x_val, x_val))\n",
    "        else:\n",
    "            val_data = None\n",
    "        earlystop = EarlyStopping(monitor='val_loss', patience=patience, verbose=1, mode='auto')\n",
    "\n",
    "        self.pcae.fit(x=x_train,\n",
    "                y=x_train,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_data=val_data,\n",
    "                callbacks=[earlystop],\n",
    "                verbose=2)\n",
    "        \n",
    "    def train(self, X_train, batch_size=128, epochs=100):      \n",
    "        self.vae_loss = []\n",
    "        for epoch in range(epochs):\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            samples = [X_train[idx]]\n",
    "            vae_loss = self.vae.train_on_batch(samples, samples)\n",
    "            self.vae_loss.append(vae_loss)\n",
    "\n",
    "            if epoch % 25 == 0:\n",
    "                # Plot the progress\n",
    "                loss_names = ['reconstruction_loss', 'mu_loss', \n",
    "                              'kl_loss', 'pca_loss', 'neighbor_loss',\n",
    "                              'manifold_loss', 'on_manifold_loss', 'off_manifold_loss']\n",
    "                loss_string = \"{} [VAE loss: {}] [\" + \": {}] [\".join(loss_names) + \": {}]\"\n",
    "                losses = self.init_losses + [self.on_manifold_loss, self.off_manifold_loss]\n",
    "                loss_vals = K.get_session().run(losses, feed_dict={self.input: X_train})\n",
    "                print(loss_string.format(epoch, vae_loss, *loss_vals))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dim = True\n",
    "pca = True\n",
    "\n",
    "# NOTE: currently the train and test sets are combined\n",
    "\n",
    "# split = int(len(x_train)*0.8)\n",
    "# x_train, x_val = x_train[:split], x_train[split:]\n",
    "# y_train, y_val = y_train[:split], y_train[split:]\n",
    "\n",
    "x_train = np.concatenate([x_train, x_test], axis=0)\n",
    "y_train = np.concatenate([y_train, y_test], axis=0)\n",
    "\n",
    "# plt.scatter(x_test[:,0], x_test[:,1], c=y_test)\n",
    "# g = plot(x_train, y_train)\n",
    "g = plot(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pca:\n",
    "    n_clusters = latent_dim + 1\n",
    "else:\n",
    "    n_clusters = latent_dim\n",
    "\n",
    "bsize = 512\n",
    "\n",
    "batch_sizes = {\n",
    "    'Unlabeled': bsize,\n",
    "    'Labeled': bsize,\n",
    "    'Orthonorm': bsize,\n",
    "    }\n",
    "\n",
    "input_shape = x_train.shape[1:]\n",
    "y_true = tf.placeholder(tf.float32, shape=(None, n_clusters), name='y_true')\n",
    "y_train_labeled_onehot = np.empty((0, len(np.unique(y_train))))\n",
    "inputs = {\n",
    "    'Unlabeled': Input(shape=input_shape, name='UnlabeledInput'),\n",
    "    'Labeled': Input(shape=input_shape, name='LabeledInput'),\n",
    "    'Orthonorm': Input(shape=input_shape, name='OrthonormInput'),\n",
    "    }\n",
    "k = 3\n",
    "spectral_net = SpectralNet(inputs, arch,\n",
    "            None, y_true, y_train_labeled_onehot,\n",
    "            n_clusters, affinity='full', scale_nbr=k, n_nbrs=k, \n",
    "            batch_sizes=batch_sizes, siamese_net=None, \n",
    "            x_train=x_train, have_labeled=len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_net.train(\n",
    "        x_train, np.zeros_like(x_train[0:0]), x_test,\n",
    "        lr=5e-5, drop=0.1, patience=30, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = spectral_net.predict(x_test)\n",
    "g = plot(y_pred[:,:3], y_test)\n",
    "print('range of y_pred values: {} - {}'.format(np.max(y_pred), np.min(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# now plot all the dimensions of spectralnet\n",
    "y_pred_embedded = TSNE().fit_transform(y_pred)\n",
    "g = plot(y_pred_embedded[:,:2], y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svg = SVG(inputs, spectralnet=spectral_net, orig_dim=x_train.shape[-1], remove_dim=remove_dim, pca=pca, k=2, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pca:\n",
    "    svg.train_pca(x_train, epochs=400)\n",
    "    svg.pca_layers[0].trainable = False\n",
    "    svg.pca_layers[1].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pca:\n",
    "    y_pred = svg.pc.predict(x_test)\n",
    "    plt.axis('equal')\n",
    "    g = plot(y_pred[:,:3], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svg.train(x_train, epochs=1000, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick subset size\n",
    "n_p = min(len(x_test), 1000)\n",
    "p = np.random.permutation(len(x_test))[:n_p]\n",
    "x_test_p = x_test[p]\n",
    "y_test_p = y_test[p]\n",
    "\n",
    "# plot generated points\n",
    "x_gen = svg.generate_from_samples(x_train)\n",
    "# g = plot(x_gen, y_train, x2=x_train, s2=0)\n",
    "p_train = np.random.permutation(len(x_train))[:n_p]\n",
    "g = plot(x_gen[p], y_train[p_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_dim = latent_dim if pca else latent_dim + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of neighbors within one standard deviation of each element in x_test\n",
    "_, _mu, _sigma_v, _sigma_lam = svg.generate_from_samples(x_test_p, return_mu_sigma=True)\n",
    "_sigma_v = _sigma_v.reshape(-1, cov_dim, cov_dim)\n",
    "\n",
    "num_close = []\n",
    "for i in range(len(_mu)):\n",
    "    l, v, m = np.exp(0.5 * -_sigma_lam[i,:]), _sigma_v[i,:], _mu[i,:]\n",
    "    left_cov = np.einsum('ij,j->ij', v, l)\n",
    "    cov = np.einsum('ij,kj->ik', left_cov, v)\n",
    "    scaled_dists = np.einsum('jk,ik->ij', cov, _mu - m)\n",
    "    # consider as neighbors all points within the variance of x_i\n",
    "    less_than_std = np.abs(scaled_dists) < 1\n",
    "    less_than_std = np.logical_and(less_than_std[:,0], less_than_std[:,1])\n",
    "    # split neighbors into those of the same class and those of a different class\n",
    "    same, diff = (y_test_p[less_than_std] == y_test[i]), (y_test_p[less_than_std] != y_test_p[i])\n",
    "    num_close.append((np.sum(same), np.sum(diff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_flattened = _sigma_v.reshape((len(_sigma_v), -1))\n",
    "centered = (v_flattened - np.mean(v_flattened, axis=0))\n",
    "_cov = centered.T.dot(centered) / len(_sigma_v)\n",
    "print('MEAN VALUE\\n', np.mean(np.sqrt(_sigma_lam), axis=0))\n",
    "print('MEAN VECTOR\\n', np.mean(_sigma_v, axis=0))\n",
    "# print('COVARIANCE\\n', _cov)\n",
    "plt.imshow(_cov, cmap='hot', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COVARIANCE TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fn1 = K.function([svg.input], [svg.sqrt_var])\n",
    "get_fn2 = K.function([svg.input], [svg.x_enc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = np.random.normal(0, .5, size=_mu.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compute sigmas\n",
    "_, _mu, _sigma_v, _sigma_lam = svg.generate_from_samples(x_test_p, return_mu_sigma=True)\n",
    "_sigma_v = _sigma_v.reshape(-1, cov_dim, cov_dim)\n",
    "# _sigma_v = np.einsum('ijk->ikj', _sigma_v)\n",
    "# _sigma_lam = np.flip(_sigma_lam, axis=1)\n",
    "k = 1\n",
    "_sigma = np.einsum('ijk,ilk->ijl', np.einsum('ijk,ik->ijk', _sigma_v[:,:,:k], np.exp(0.5 * _sigma_lam[:,:k])), _sigma_v[:,:,:k])\n",
    "\n",
    "# verify sigmas\n",
    "_z_sqrt_var = predict_with_K_fn(get_fn1, x_test_p)[0].reshape((-1, cov_dim, cov_dim))\n",
    "\n",
    "# verify encoding\n",
    "_x_enc = predict_with_K_fn(get_fn2, x_test_p)[0]\n",
    "\n",
    "print(\"ALSO\", _sigma.shape, _z_sqrt_var.shape)\n",
    "print('ERROR', np.linalg.norm(_sigma - _z_sqrt_var))\n",
    "\n",
    "# epsilon = np.random.normal(size=_mu.shape)\n",
    "perturbations = np.einsum('ijk,ik->ij', _sigma, epsilon)\n",
    "# perturbations = epsilon\n",
    "\n",
    "single_perturbed_x = np.array([_mu[0,:]] * len(_mu)) + np.einsum('jk,ik->ij', _sigma[0,:], epsilon)\n",
    "perturbed_x = _mu + perturbations\n",
    "# g = plot(x=_mu, y=y_test)\n",
    "# g = plot(x=perturbed_x, y=y_test, x2=_mu, s2=100)\n",
    "# g = plot(_x_enc, y=y_test, x2=_mu, s2=100)\n",
    "g = plot(_x_enc, y=y_test_p)\n",
    "# plt.figure()\n",
    "\n",
    "idxs = np.random.permutation(len(_mu))\n",
    "for i in idxs:\n",
    "#     idx = np.argmax(_sigma_lam[i])\n",
    "    idx = 0\n",
    "    delta = _sigma_v[i,:,idx] * np.sqrt(_sigma_lam[i, idx])\n",
    "    start = _mu[i] + delta\n",
    "    end = _mu[i] - delta\n",
    "    coords = [[s, e] for s, e in zip(start, end)]\n",
    "    plt.plot(*coords, 'k-', lw=2, alpha=.1)\n",
    "    \n",
    "plt.axis('equal')\n",
    "\n",
    "# plt.scatter(_mu[idxs, 0], _mu[idxs, 1], s=200)\n",
    "# plt.plot([0, 1], [1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_perturbed_x = np.array([_mu[0,:]] * len(_mu)) + np.einsum('jk,ik->ij', _sigma[0,:], epsilon)\n",
    "perturbed_x = _mu + perturbations\n",
    "g = plot(_mu, x2=perturbed_x, s2=20)\n",
    "plt.axis('equal')\n",
    "\n",
    "idxs = np.random.permutation(len(_mu))[:100]\n",
    "for i in idxs:\n",
    "    idx = np.argmax(_sigma_lam[i])\n",
    "    delta = _sigma_v[i, idx] * np.sqrt(_sigma_lam[i, idx])\n",
    "    start = _mu[i] + delta\n",
    "    end = _mu[i] - delta\n",
    "    plt.plot([start[0], end[0]], [start[1], end[1]], 'k-', lw=2, alpha=0.1)\n",
    "    \n",
    "for i in idxs:\n",
    "    idx = np.argmin(_sigma_lam[i])\n",
    "    delta = _sigma_v[i,:,idx] * np.sqrt(_sigma_lam[i, idx])\n",
    "    start = _mu[i] + delta\n",
    "    end = _mu[i] - delta\n",
    "    plt.plot([start[0], end[0]], [start[1], end[1]], 'k-', lw=2, alpha=0.1)\n",
    "#     plt.plot([_mu[i,0], _mu[i,0]], [_mu[i,1] + .1, _mu[i,1]])\n",
    "\n",
    "# plt.scatter(_mu[idxs, 0], _mu[idxs, 1], s=200)\n",
    "# plt.plot([0, 1], [1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING MATRIX MATH\n",
    "\n",
    "# TEST MATRIX STACK x VECTOR STACK (REPRESENTING DIAGONALS)\n",
    "n_stacks = min(1500, len(_sigma_lam))\n",
    "n = 10\n",
    "\n",
    "# generate matrices\n",
    "matrices = []\n",
    "diagonals = []\n",
    "products = []\n",
    "for i in range(n_stacks):\n",
    "    matrix = _sigma_v[i,:,:]\n",
    "    diagonal = np.sqrt(_sigma_lam[i,:])\n",
    "    product = np.dot(np.dot(matrix, np.diag(diagonal)), matrix.T)\n",
    "    matrices.append(matrix)\n",
    "    diagonals.append(diagonal)\n",
    "    products.append(product)\n",
    "    \n",
    "matrix_stack = np.concatenate([np.expand_dims(m, axis=0) for m in matrices], axis=0)\n",
    "diagonals_stack = np.concatenate([np.expand_dims(d, axis=0) for d in diagonals], axis=0)\n",
    "product_stack = np.einsum('ijk,ilk->ijl', np.einsum('ijk,ik->ijk', matrix_stack, diagonals_stack), matrix_stack)\n",
    "\n",
    "diffs = []\n",
    "diffs2 = []\n",
    "diffs3 = []\n",
    "for i in range(n_stacks):\n",
    "    diff = np.sum(products[i] - product_stack[i,:,:])\n",
    "    diff2 = np.sum(products[i] - _sigma[i,:,:])\n",
    "    diff3 = np.sum(products[i] - _z_sqrt_var[i,:,:])\n",
    "    diffs.append(diff)\n",
    "    diffs2.append(diff2)\n",
    "    diffs3.append(diff3)\n",
    "    \n",
    "print(\"ERRORS\")\n",
    "print(sum(diffs))\n",
    "print(sum(diffs2))\n",
    "print(sum(diffs3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_ = np.random.randint(0, len(_mu))\n",
    "# i_ = 0\n",
    "print(_sigma[i_].dot(_sigma_v[i_]))\n",
    "print(np.linalg.norm(_sigma[i_].dot(_sigma_v[i_])))\n",
    "print(np.sqrt(_sigma_lam[i_]))\n",
    "print(_sigma_v[i_])\n",
    "print(epsilon[i_])\n",
    "print(_sigma[i_].dot(epsilon[i_]))\n",
    "print(perturbed_x[i_])\n",
    "print(_mu[i_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i_ = np.random.randint(0, len(_mu))\n",
    "# tmp = np.expand_dims(_sigma[i_].dot(epsilon[i_]), axis=0)\n",
    "# tmp2 = perturbed_x[i_-1:i_]\n",
    "# tmp3 = np.expand_dims(np.einsum('ij,j->j', _sigma[i_], epsilon[i_]), axis=0)\n",
    "# tmp4 = np.expand_dims(np.einsum('ij,j->i', _sigma[i_], epsilon[i_]), axis=0)\n",
    "# print('tmp', tmp)\n",
    "# print('tmp2', tmp2)\n",
    "# print('tmp3', tmp3, np.linalg.norm(tmp3/_mu[i_]))\n",
    "# print('tmp4', tmp4, np.linalg.norm(tmp4/_mu[i_]))\n",
    "# g = plot(_mu[i_:i_+1], x2=_mu[i_] + tmp, s=1000, s2=300)\n",
    "# plt.scatter(_mu[:,0], _mu[:,1])\n",
    "# idx = np.argmax(_sigma_lam[i])\n",
    "# delta = _sigma_v[i_,:,idx] * np.sqrt(_sigma_lam[i_, idx])\n",
    "# start = _mu[i_] + delta\n",
    "# end = _mu[i_] - delta\n",
    "# plt.plot([start[0], end[0]], [start[1], end[1]], 'k-', lw=2, alpha=1)\n",
    "# plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BILIPSCHITZ TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differentiate decoder with respect to inputs to compute another jacobian, and then evaluate it on the same point\n",
    "_jacobian = [tf.expand_dims(tf.gradients(svg.x_recon[:,i], svg.x_enc)[0], 1) for i in range(svg.x_recon.shape[1])]\n",
    "jacobian = tf.reduce_sum(tf.concat(_jacobian, axis=1), axis=0)\n",
    "v = tf.reshape(svg.z_cov_vectors, (-1, cov_dim, cov_dim))\n",
    "v = tf.Print(v, [tf.shape(v), tf.shape(svg.z_cov_values)], 'PRINT')\n",
    "temp = tf.einsum('ijk,ik->ijk', v, tf.sqrt(svg.z_cov_values[:,:1]))\n",
    "print(v.get_shape(), svg.z_cov_values.get_shape(), temp.get_shape())\n",
    "B = tf.einsum('ijk,ilk->ijl', temp, v)\n",
    "B = tf.reduce_mean(B, axis=0)\n",
    "cov = tf.matmul(jacobian, tf.matmul(B, jacobian, transpose_b=True))\n",
    "cov = tf.reshape(cov, (x_test[0].shape[0], x_test[0].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create burst from a point and compute covariance matrix\n",
    "burst_size = 1000\n",
    "rand_idx = np.random.randint(len(x_test))\n",
    "x_ = x_test[rand_idx]\n",
    "# x_ = np.array((np.cos(.25), np.sin(.25)))\n",
    "x_arr = np.array([x_] * burst_size)\n",
    "x_rec, x_mu, x_sigma_v, x_sigma_lam = svg.generate_from_samples(x_arr, return_mu_sigma=True)\n",
    "\n",
    "cov_burst = np.cov((x_rec - np.mean(x_rec, axis=0)).T)\n",
    "\n",
    "# run gradient burst\"\n",
    "# cov_grad = K.get_session().run([svg.x_recon, cov, B, jacobian], feed_dict={svg.input: np.array([x_]*1)})\n",
    "cov_grad = K.get_session().run([cov], feed_dict={svg.input: np.array([x_]*1)})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_burst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_burst, _ = np.linalg.eig(cov_burst)\n",
    "l_grad, _ = np.linalg.eig(cov_grad)\n",
    "l_burst = np.sort(l_burst)[::-1]\n",
    "l_grad = np.sort(l_grad)[::-1]\n",
    "print('l_burst:', l_burst, l_burst/l_burst[0])\n",
    "print('l_grad:', l_grad, l_grad/l_grad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plot(x_rec, x2=x_test_p, label1='true', label2='predicted', alpha2=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x__ = np.expand_dims(x_, axis=0)\n",
    "g = plot(x__, x2=x_test, alpha=.1, label1='true', label2='predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plot(x_test_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM WALK TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM WALK\n",
    "def walk(f, x_arr, branch_factor=5, n_steps=20, max_size=1000):\n",
    "    p = np.random.permutation(len(x_arr))[:1000]\n",
    "    x_arr = x_arr[p]\n",
    "    for i in range(n_steps):\n",
    "        x_arr = np.array([x_arr] * branch_factor).reshape([-1, x_arr.shape[0]])\n",
    "        (x_arr, x_mu, x_sigma) = f(x_arr)\n",
    "        p = np.random.permutation(len(x_arr))[:1000]\n",
    "        x_arr, x_mu, x_sigma = x_arr[p], x_mu[p], x_sigma[p]\n",
    "        \n",
    "    return x_arr, x_mu, x_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = partial(svg.generate_from_samples, return_mu_sigma=True) #, normalize_cov=0.5)\n",
    "y_test_sz = np.mean(f(x_all)[3], axis=1)\n",
    "sz_max = np.max(y_test_sz)\n",
    "sz_min = np.min(y_test_sz)\n",
    "y_test_sz = (y_test_sz - sz_min)/(sz_max - sz_min) * 5\n",
    "print(np.min(y_test_sz), np.max(y_test_sz))\n",
    "y_test_sz = np.exp(0.5 * y_test_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation\n",
    "\n",
    "# which space do we want to plot in?\n",
    "plot_latent = False\n",
    "plot_idx = 1 if plot_latent else 0\n",
    "\n",
    "x_arr = np.random.permutation(x_test)[:100]\n",
    "x__ = f(x_all)[plot_idx]\n",
    "x_ = f(x_arr)[plot_idx]\n",
    "x_tot = np.concatenate([x_, x__], axis=0)\n",
    "y_tot = np.concatenate([np.zeros(shape=(len(x_arr),)), np.ones(shape=(len(x__),))*2], axis=0)\n",
    "y_sz = np.concatenate([np.ones(shape=(len(x_arr),))*5, y_test_sz], axis=0)\n",
    "\n",
    "def update_graph(num):\n",
    "    global x_arr\n",
    "    global x__\n",
    "    global y_tot\n",
    "    x_arr, x_mu, x_sigma_v, x_sigma_lam = f(x_arr)\n",
    "    # plot in latent or original space\n",
    "    x_ = x_mu if plot_latent else x_arr\n",
    "    \n",
    "    x_ = np.concatenate([x_, x__], axis=0)\n",
    "    \n",
    "    if x_.shape[1] == 3:\n",
    "        graph._offsets3d = (x_[:,0], x_[:,1], x_[:,2])\n",
    "        ax.view_init(elev=10, azim=num*4)\n",
    "    elif x_.shape[1] == 2:\n",
    "        graph.set_offsets(np.c_[x_[:,0], x_[:,1]])\n",
    "        \n",
    "    title.set_text('Walk, time={}'.format(num))\n",
    "\n",
    "fig = plt.figure(figsize=(12.8, 7.2))\n",
    "projection = '3d' if x_.shape[1] == 3 else None\n",
    "ax = fig.add_subplot(111, projection=projection)\n",
    "title = ax.set_title('Walk, time=0')\n",
    "\n",
    "if x_.shape[1] == 3:\n",
    "    graph = ax.scatter(x_tot[:,0], x_tot[:,1], x_tot[:,2], c=y_tot, s=y_sz, alpha=.1)\n",
    "elif x_.shape[1] == 2:\n",
    "    graph = ax.scatter(x_tot[:,0], x_tot[:,1], c=y_tot, s=y_sz, alpha=0.4)\n",
    "\n",
    "ani = matplotlib.animation.FuncAnimation(fig, update_graph, 180, \n",
    "                               interval=200, blit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "print(\"saving animation\")\n",
    "Writer = animation.writers['ffmpeg']\n",
    "writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n",
    "print(\"...\")\n",
    "ani.save('im_{}.mp4'.format(dataset), writer=writer)\n",
    "print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import HTML\n",
    "# HTML(ani.to_html5_video(embed_limit=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = partial(svg.generate_from_samples, return_mu_sigma=True, normalize_cov=False)\n",
    "def walk(f, x_arr, branch_factor=5, n_steps=200, max_size=10000):\n",
    "    p = np.random.permutation(len(x_arr))[:1000]\n",
    "    x_arr = x_arr[p]\n",
    "    for i in range(n_steps):\n",
    "        x_arr = np.array([x_arr] * branch_factor).reshape([-1, x_arr.shape[1]])\n",
    "        (x_arr, x_mu, x_sigma_v, x_sigma_lam) = f(x_arr)\n",
    "        p = np.random.permutation(len(x_arr))[:1000]\n",
    "        x_arr, x_mu, x_sigma_v, x_sigma_lam = x_arr[p], x_mu[p], x_sigma_v[p], x_sigma_lam[p]\n",
    "        \n",
    "    return x_arr, x_mu, x_sigma_v, x_sigma_lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = partial(svg.generate_from_samples, return_mu_sigma=True)\n",
    "\n",
    "x_test_sample = np.random.permutation(x_test)[:1]\n",
    "x_arr, x_mu, x_sigma_v, x_sigma_lam = walk(f, x_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plot(x_arr, x2=x_test_sample, s2=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plot(x_arr, x2=x_test_p, label1='predicted', label2='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = plot(x_mu, x2=f(x_test_sample)[1])\n",
    "g = plot(x_mu, x2=f(x_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plot(x_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"samar was here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
