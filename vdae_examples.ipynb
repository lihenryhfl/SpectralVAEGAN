{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from keras.layers import Input\n",
    "\n",
    "from vdae.networks import VDAE, SpectralNet\n",
    "from vdae.vdae_util import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'loop'\n",
    "\n",
    "if dataset == 'mnist':\n",
    "    from keras.datsets import mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    # reshape and standardize x arrays\n",
    "    x_train = x_train.reshape(len(x_train), -1) / 255\n",
    "    x_test = x_test.reshape(len(x_test), -1) / 255 \n",
    "    latent_dim = 10\n",
    "    bsize = 512\n",
    "elif dataset == 'coil20hr':\n",
    "    d_in = scipy.io.loadmat('data/COIL20_UNPROC_128.mat')\n",
    "    X = d_in['X'] / np.max(d_in['X'])\n",
    "    Y = d_in['Y']-1\n",
    "    Y = Y.reshape(-1)\n",
    "    x_train, x_test = X.reshape((len(X), -1)), X.reshape((len(X), -1))\n",
    "    y_train, y_test = Y, Y\n",
    "    latent_dim = 10\n",
    "    bsize = 256\n",
    "elif dataset == 'bulldog':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_bulldog()\n",
    "    latent_dim = 3\n",
    "    bsize = len(x_train)\n",
    "elif dataset == 'loop':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_loop(n=5000, train_set_fraction=0.85)\n",
    "    latent_dim = 3\n",
    "    bsize = len(x_train)\n",
    "elif dataset == 'faces':\n",
    "    (x_train, y_train), (x_test, y_test) = generate_faces()\n",
    "    latent_dim = 5\n",
    "    bsize = len(x_train)\n",
    "\n",
    "x_all = np.concatenate([x_train, x_test], axis=0)\n",
    "    \n",
    "# normalize to between -1 and 1\n",
    "if 'mnist' not in dataset and 'cifar' not in dataset and 'bulldog' not in dataset and 'gaussian_grid' not in dataset:\n",
    "    m, M = np.min(x_train), np.max(x_train)\n",
    "    a = (M + m) / 2\n",
    "    b = (M - m) / 2\n",
    "    x_train, x_test = (x_train - a) / b, (x_test - a) / b\n",
    "\n",
    "arch = [\n",
    "    {'type': 'relu', 'size': 500},\n",
    "    {'type': 'linear', 'size': latent_dim},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN SPECTRALNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plot(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = latent_dim\n",
    "\n",
    "batch_sizes = {\n",
    "    'Unlabeled': bsize,\n",
    "    'Labeled': bsize,\n",
    "    'Orthonorm': bsize,\n",
    "    }\n",
    "\n",
    "input_shape = x_train.shape[1:]\n",
    "y_true = tf.placeholder(tf.float32, shape=(None, n_clusters), name='y_true')\n",
    "y_train_labeled_onehot = np.empty((0, len(np.unique(y_train))))\n",
    "inputs = {\n",
    "    'Unlabeled': Input(shape=input_shape, name='UnlabeledInput'),\n",
    "    'Labeled': Input(shape=input_shape, name='LabeledInput'),\n",
    "    'Orthonorm': Input(shape=input_shape, name='OrthonormInput'),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_net = SpectralNet(inputs=inputs, arch=arch,\n",
    "                spec_reg=None, y_true=y_true, y_train_labeled_onehot=y_train_labeled_onehot,\n",
    "                n_clusters=n_clusters, affinity='knn', scale_nbr=8, n_nbrs=8, \n",
    "                batch_sizes=batch_sizes, siamese_net=None, \n",
    "                x_train=x_train, have_labeled=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spectral_net.train(\n",
    "            x_train, np.zeros_like(x_train[0:0]), x_test,\n",
    "            lr=1e-5, drop=0.1, patience=20, num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = spectral_net.predict(x_train)\n",
    "g = plot(y_pred[:,:3], y_train.reshape(-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot images over embeddings\n",
    "if 'mnist' in dataset or 'cifar' in dataset or 'coil' in dataset or 'faces' in dataset:\n",
    "    zoom = 1\n",
    "    if 'mnist' in dataset:\n",
    "        img_shape = (28, 28)\n",
    "    elif 'cifar' in dataset:\n",
    "        img_shape = (32, 32, 3)\n",
    "    elif 'coil' in dataset and 'hr' in dataset:\n",
    "        img_shape = (128, 128)\n",
    "        zoom = 0.2\n",
    "    elif 'coil' in dataset:\n",
    "        img_shape = (32, 32)\n",
    "    elif 'faces' in dataset:\n",
    "        img_shape = (28, 20)\n",
    "        zoom = 2\n",
    "    _ = imscatter(y_pred[p,0], y_pred[p,1], x_train[p], shape=img_shape, zoom=zoom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN VDAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svg_bsize = 288\n",
    "svg_k = 2\n",
    "svg_arch = [500]\n",
    "\n",
    "svg = VDAE(inputs, spectralnet=spectral_net, orig_dim=x_train.shape[1:], k=svg_k, alpha=1., arch=svg_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svg.train(x_train, epochs=10000, batch_size=svg_bsize, full_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a random walk step of a point. then plot its nearest neighbor\n",
    "x_recon, z_mu, z_sigma_v, z_sigma_lam, _x_enc = svg.generate_from_samples(x_train, return_mu_sigma=True, normalize_cov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x_recon[:1000], y_train[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = partial(svg.generate_from_samples, return_mu_sigma=True)#, normalize_cov=0.)\n",
    "def walk(f, x_arr, branch_factor=20, n_steps=10, max_size=5000):\n",
    "    p = np.random.permutation(len(x_arr))[:max_size]\n",
    "    x_arr = x_arr[p]\n",
    "    orig_shape = (-1,) + (x_arr.shape[1:])\n",
    "    for i in range(n_steps):\n",
    "        x_arr = np.array([x_arr] * branch_factor).reshape(orig_shape)\n",
    "        (x_arr, z_mu, z_sigma_v, z_sigma_lam, _x_enc) = f(x_arr)\n",
    "        p = np.random.permutation(len(x_arr))[:max_size]\n",
    "        x_arr, z_mu, z_sigma_v, z_sigma_lam, _x_enc = x_arr[p], z_mu[p], z_sigma_v[p], z_sigma_lam[p], _x_enc[p]\n",
    "        x_arr = x_arr.reshape(orig_shape)\n",
    "    \n",
    "    p = np.random.permutation(len(x_arr))[:max_size]\n",
    "    x_arr = x_arr[p]\n",
    "        \n",
    "    return x_arr, z_mu, z_sigma_v, z_sigma_lam, _x_enc\n",
    "\n",
    "# NOW GENERATE all data points\n",
    "max_size = min(1000, len(x_train))\n",
    "x_arrs = []\n",
    "test_size = len(x_train)\n",
    "n_ = 0\n",
    "while n_ < test_size:\n",
    "    p = np.random.permutation(len(x_test))[:200]\n",
    "    x_test_sample = x_test[p]\n",
    "    x_arr, z_mu, z_sigma_v, z_sigma_lam, _x_enc = walk(f, x_test_sample, n_steps=4, branch_factor=2, max_size=max_size)\n",
    "    x_arrs.append(x_arr)\n",
    "    n_ += len(x_arr)\n",
    "    print(\"generated {}/{} points\".format(n_, test_size), _x_enc.shape, x_arr.shape)\n",
    "\n",
    "x_gens = np.concatenate(x_arrs, axis=0)[:len(x_train)]\n",
    "\n",
    "# COMPUTE GROMOV-WASSERSTEIN\n",
    "gw = gromorov_wasserstein_d(x_train, x_gens)\n",
    "\n",
    "# COMPUTE BILIP CONSTANT\n",
    "Ks = bilip(_x_enc, x_arr, 100)\n",
    "print('Mean bi-Lipschitz constant: {:.4f} +- {:.4f}'.format(np.mean(Ks), np.std(Ks)))\n",
    "print('Gromov-wasserstein distance: {:.4f}'.format(gw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick subset size\n",
    "n_p = min(1000, len(x_train))\n",
    "p = np.random.permutation(len(x_train))[:n_p]\n",
    "x_train_p = x_train[p]\n",
    "y_train_p = y_train[p]\n",
    "\n",
    "# plot generated points\n",
    "x_gen = svg.generate_from_samples(x_train_p, normalize_cov=False)\n",
    "g = plot(x_gen, y_train_p, x2=x_train_p, s2=0)\n",
    "p_train = np.random.permutation(len(x_train))[:n_p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM WALK TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.permutation(len(x_test))[:1]\n",
    "x_test_sample = x_test[p]\n",
    "print('random walking on {}'.format(y_test[p]))\n",
    "x_arr, z_mu, z_sigma_v, z_sigma_lam, _x_enc = walk(f, x_test_sample, n_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.permutation(len(x_arr))\n",
    "x_true_n_gen = np.concatenate([x_test_sample, x_arr[p]], axis=0)\n",
    "g = plot(x_true_n_gen, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g, ax = plot(x_arr, x2=x_test, label1='predicted', label2='true', alpha2=0.2)\n",
    "plt.title('predicted vs true points in data space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plot(z_mu, x2=f(x_test)[1], label1='predicted', label2='true')\n",
    "plt.title('predicted vs true points in latent space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plot(x_arr, x2=x_test_sample, label1='predicted', label2='seed point', s2=1000)\n",
    "plt.title('predicted vs initial seed point in data space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plot(z_mu, x2=f(x_test_sample)[1], label1='predicted', label2='seed point', s2=1000)\n",
    "plt.title('predicted vs initial seed point in latent space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vdae_env",
   "language": "python",
   "name": "vdae_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
